{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geological Interpretor Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for testing and developping some of the basic code in this package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial as spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patheffects\n",
    "import pandas as pd\n",
    "import owlready2 as owl\n",
    "class MalissiaBaseError(Exception):\n",
    "    \"\"\"This is the base class for all exceptions of this package\"\"\"\n",
    "    \n",
    "class MalissiaNotImplementedYet(MalissiaBaseError):\n",
    "    \"\"\"This is an exception for features that are not yet implemented\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geological Knowledge Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GeologicalKnowledgeManager** may know different instances of **GeologicalKnowledgeFramework**,<br>\n",
    "for example to allow differenciating scenarios or for allowing customisation of knowledge and its formalisation.\n",
    "\n",
    "**GeologicalKnowledgeFramework** provides access to concept definitions for providing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "class GeologicalKnowledgeManager(object):\n",
    "    \"\"\"GeologicalKnowledgeManager is managing one or several GeologicalKnowledgeFramework.\n",
    "    \n",
    "    The GeologicalKnowledgeManager is typically a singleton, so there is always one and only one instance of it.\n",
    "    \n",
    "    The GeologicalKnowledgeManager may know different instances of GeologicalKnowledgeFramework,\n",
    "    for example to allow different interpretation scenarios or for allowing user-specific customisation\n",
    "    of knowledge and its formalisation.\n",
    "    \n",
    "    GeologicalKnowledgeFramework are typically ontologies and extensions defined in this package or elsewhere.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __new__(cls):\n",
    "        \"\"\"Method to access (and create if needed) the only allowed instance of this class.\n",
    "        \n",
    "        Returns:\n",
    "        - an instance of GeologicalKnowledgeManager\"\"\"\n",
    "        if not hasattr(cls, 'instance'):\n",
    "            cls.instance = super(GeologicalKnowledgeManager, cls).__new__(cls)\n",
    "            cls.initialised= False\n",
    "        return cls.instance\n",
    "        \n",
    "    def __init__(self, default= \"mogi\", default_source_directory= \"../ontologies/\", default_source_file= \"mogi.owl\", default_ontology_backend= \"owlready2\"):\n",
    "        \"\"\"Initializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        if not self.initialised:\n",
    "            self._initialise(default= default, default_source_directory= default_source_directory, default_source_file= default_source_file, default_ontology_backend= default_ontology_backend)\n",
    "            \n",
    "    def _initialise(self, default, default_source_directory, default_source_file, default_ontology_backend):\n",
    "        \"\"\"Initializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        self.default= default\n",
    "        self.default_source_directory= default_source_directory\n",
    "        self.default_source_file= default_source_file\n",
    "        self.default_ontology_backend= default_ontology_backend\n",
    "        \n",
    "        self.knowledge_framework_dict = {}\n",
    "        \n",
    "        self.initialised= True\n",
    "        \n",
    "    def reset(self, default= \"mogi\", default_source_directory= \"../ontologies/\", default_source_file= \"mogi.owl\", default_ontology_backend= \"owlready2\"):\n",
    "        \"\"\"Reinitializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        self._initialise(default= default, default_source_directory= default_source_directory, default_source_file= default_source_file, default_ontology_backend= default_ontology_backend)\n",
    "             \n",
    "    def load_knowledge_framework(self, name=None, source= None, source_directory= None, backend= None):\n",
    "        \"\"\"Gets and initilises the ontology from the specified source.\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name to be given to the knowledge framework. If None (default) the file name will be used.\n",
    "        - source: filename to the ontology source. If None(default) the default ontology is used.\n",
    "        - source_directory: where the system should look for ontology definition files. If None, the `GeologicalKnowledgeFramework` will decide.\n",
    "        - backend: the ontology backend to be used. If None, the `GeologicalKnowledgeFramework` will decide.\"\"\"\n",
    "        source = source if source is not None else self.default_source_file\n",
    "        name = name if name is not None else os.path.basename(source).split(os.path.extsep)[0]\n",
    "        self.knowledge_framework_dict[name] = GeologicalKnowledgeFramework(name= name, source= source, source_directory= source_directory, backend= backend)\n",
    "    \n",
    "    def get_knowledge_framework(self,name= \"default\"):\n",
    "        \"\"\"Accessor to knowledge frameworks.\"\"\"\n",
    "        name = self.default if name == \"default\" else name\n",
    "        assert len(self.knowledge_framework_dict) > 0, \"No ontology has been loaded yet. Please use GeologicalKnowledgeManager().load_knowledge_framework() first\"\n",
    "        assert name in self.knowledge_framework_dict.keys(), \"The specified ontology hasn't been loaded: \"+name+\\\n",
    "            \"\\navailable ontology names are: \"+\"\\n\".join(self.knowledge_framework_dict.keys())\n",
    "        return self.knowledge_framework_dict[name]\n",
    "    \n",
    "class GeologicalKnowledgeFramework(object):\n",
    "    \"\"\"A GeologicalKnowledgeFramework holds the definition of concepts and relationships describing knowledge.\n",
    "    \n",
    "    This is typically an overlay around a formal ontology definition, which also brings additional capabilities,\n",
    "    such as algorithms and factories to achieve specific tasks and create objects.\n",
    "    \n",
    "    This knowledge framework also holds \n",
    "\n",
    "    - a registry of available constructors for making new objects:\n",
    "     -- registered_constructors: a dictionnary holding the constructors for a given class, sorted in order of preference,\n",
    "         structured as {class_object or key: [constructor1, constructor2,...]}\n",
    "     -- constructor_conditions: conditions for the application of a constructor,\n",
    "       e.g., regarding InterpretationSituation, interpretor status, and parameters.\n",
    "        Structured as dictionnaries {constructor: function_to_evaluate_conditions}\n",
    "\n",
    "    - a registry of available evaluators for evaluating objects:\n",
    "     -- registered_constructors: a dictionnary holding the evaluators for a given class, sorted in order of preference,\n",
    "         structured as {class_object or key: [evaluator1, evaluator2,...]}\n",
    "\n",
    "    - a registry of available updaters for making new objects:\n",
    "     -- registered_constructors: a dictionnary holding the updaters for a given class, sorted in order of preference,\n",
    "         structured as {class_object or key: [updater1, updater2,...]}\n",
    "     -- updater_conditions: conditions for the application of an updater,\n",
    "       e.g., regarding InterpretationSituation, interpretor status, and parameters.\n",
    "        Structured as dictionnaries {updater: function_to_evaluate_updaters}\n",
    "\n",
    "    \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    # registry of object constructors and conditions\n",
    "    registered_constructors = {}\n",
    "    constructor_conditions = {}\n",
    "    \n",
    "    # registry of evaluators\n",
    "    registered_internal_consistency_evaluation = {}\n",
    "    registered_explanation_consistency_evaluation = {}\n",
    "\n",
    "    # registry of updaters and conditions\n",
    "    registered_updaters = {}\n",
    "    updater_conditions = {}\n",
    "\n",
    "    \n",
    "    def __init__(self, name, source, source_directory= None, backend= None):\n",
    "        \"\"\"Initialise a KnowledgeFramework form a given ontology file (source).\n",
    "        \n",
    "        Parameters:\n",
    "        - name: should be the name under which this KnowledgeFramework is known in the manager\n",
    "        - source: the source file for the ontology definition\n",
    "        - source_directory: the directory where the source files for the ontology definition are looked for.\n",
    "        If None (default) the default path provided by the `KnowledgeManager` is used.\n",
    "        - backend: the ontology backend to be used for this knwoledge framework.\n",
    "        If None (default) the default ontology backend provided by the `KnowledgeManager` is used.\"\"\"\n",
    "        self.name= name\n",
    "        \n",
    "        self._source_directory= None\n",
    "        self.init_source_directory(source_directory)\n",
    "        self.initialise_ontology_backend(backend)\n",
    "        \n",
    "        self.load_ontology(source)\n",
    "            \n",
    "    \n",
    "    def init_source_directory(self, source_directory):\n",
    "        \"\"\"Initialises the folder where source files are searched.\n",
    "        \n",
    "        Parameters:\n",
    "        - source_directory: if None, the previous value is used if it wasn't None, else the `GeologicalKnowledgeManager`default is used.\"\"\"\n",
    "        if source_directory is not None:\n",
    "            self._source_directory= source_directory\n",
    "        elif self._source_directory is None:\n",
    "            self._source_directory= GeologicalKnowledgeManager().default_source_directory\n",
    "    \n",
    "    def initialise_ontology_backend(self, backend_name:str= None):\n",
    "        \"\"\"Initializes the ontology package used as a backend to access ontologies.\n",
    "        \n",
    "        This will:\n",
    "        - try to import the backend as onto\n",
    "        - set the default path for ontologies\"\"\"\n",
    "                \n",
    "        self._ontology_backend = None\n",
    "        backend_name= GeologicalKnowledgeManager().default_ontology_backend if backend_name is None else backend_name\n",
    "        if backend_name == \"owlready2\":\n",
    "            try:\n",
    "                import owlready2 as owl2 \n",
    "                self._ontology_backend = owl2\n",
    "                if self._source_directory not in self._ontology_backend.onto_path:\n",
    "                    self._ontology_backend.onto_path.append(self._source_directory)\n",
    "            except ImportError:\n",
    "                raise ImportError(\"Your are trying to use Owlready2 as a backend for ontology management, but it doesn't appear to be installed.\"\\\n",
    "                \"This is either because OwlReady2 is given as default option or because you asked for it.\"\\\n",
    "                \"Please install the OwlReady2 package from https://owlready2.readthedocs.io\"\\\n",
    "                \"or give another backend through GeologicalKnowledgeManager().initialise_ontology_backend()\")\n",
    "                \n",
    "            # also test if java is correctly installed & accessible, as it is used by owlready2 for reasoning\n",
    "            try:\n",
    "                os.system(\"java -version\")\n",
    "            except:\n",
    "                raise ImportError(\"Java doesn't appear to be installed properly as the command `java -version` returned an error.\"\\\n",
    "                    \"This error occured while loading owlready2 package as an ontology backend, because java is used for the reasoning engine.\")\n",
    "        else:\n",
    "            raise Exception(\"The specified backed for ontology is not supported: \"+backend_name)\n",
    "          \n",
    "        \n",
    "    def load_ontology(self, source):\n",
    "        \"\"\"Loads the ontology specified by source.\n",
    "        \n",
    "        Parameters:\n",
    "        - source: the source file for the ontology definition\n",
    "        - source_directory: the directory where the source files for the ontology definition are looked for.\n",
    "        If None (default) the default path provided by the `KnowledgeManager` is used.\"\"\"\n",
    "        self._source= source\n",
    "        try:\n",
    "            self._onto = self._ontology_backend.get_ontology(self._source).load()\n",
    "        except Exception as err:\n",
    "            raise Exception(\"Unexpected exception received while loading ontology:\\n - source: {}\\n - onto_path: {}\".format(self._source, self._ontology_backend.onto_path))\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self._onto\n",
    "        \n",
    "    def get_ontology_backend(self):\n",
    "        \"\"\"Gets the ontology backend\"\"\"\n",
    "        assert self._ontology_backend is not None, \"Trying to access the ontology backend without initialising it.\"\n",
    "        return self._ontology_backend\n",
    "    \n",
    "    def search(self, name= None, type= None, qualities= None, prepend_star=True) -> list:\n",
    "        \"\"\"Search function to interface the serach capabilities of the internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the search object (you can use * to replace any set of characters and ? to replace any single character)\n",
    "        Note: if `prepend_star` a * is always prepended to allows the search to work because of the internal prefix names\n",
    "        - type: the type of the searched observations as defined by the internal ontology\n",
    "        - qualities: qualities to filter the observations.\n",
    "        If `qualities` is a :\n",
    "         * `str`: a single quality will be searched for with any value (\"*\"),\n",
    "         * `list`: a list of qualities will be searched for with any values (\"*\")\n",
    "         * `dict`: a list of qualities defined by the keys and with the associated values will be searched for\"\"\"\n",
    "        if name is None:\n",
    "            name = \"*\"\n",
    "        elif prepend_star:\n",
    "            name = \"*\"+name\n",
    "        \n",
    "        if isinstance(qualities,list):\n",
    "            kargs = {quality_i: \"*\" for quality_i in qualities} \n",
    "        elif isinstance(qualities,str):\n",
    "            kargs = {qualities: \"*\"}\n",
    "        elif isinstance(qualities,dict):\n",
    "            kargs = qualities\n",
    "        else:\n",
    "            kargs = {}\n",
    "            assert (qualities is None) or isinstance(qualities,dict), \"qualities should be given as either None, a str, a list, or a dict\"\n",
    "        if type is not None: kargs[\"type\"] = type\n",
    "        return self._onto.search(iri= name, **kargs)\n",
    "    \n",
    "    def format_qualities(self, **qualities):\n",
    "        \"\"\"Formats the qualities for setting an instance\n",
    "\n",
    "        It will :\n",
    "         - remove qualities that are not defined in the ontology\n",
    "         - transform scalar values into vectors (for non functional properties)\n",
    "         - filter out None values\n",
    "        \"\"\"\n",
    "        \n",
    "        formated_qualities = {key:val for key, val in qualities.items() \n",
    "                 if (val is not None) and (getattr(self._onto,key) is not None)\n",
    "                 }\n",
    "        for key, val in formated_qualities.items():\n",
    "            if not getattr(self._onto,key).is_functional_for(None):\n",
    "                formated_qualities[key] = val if isinstance(val,list) else [val]\n",
    "        return formated_qualities\n",
    "\n",
    "    def create_instance(self, class_type, name = None, physical_space= None, warning= False, **qualities):\n",
    "        \"\"\"Creates an instance in the ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - class_type: the type of the class of the object to be created (as in the ontology, e.g., mogi().Surface).\n",
    "        Note: the given class_type must be in the internal ontology stored in this manager.\n",
    "        - name: a string giving the name of the object. If None (default) this name is set automatically\n",
    "        - physical_space: if defined, a PhysicalSpaceRepresentation that handles the spatial qualities given in qualities\n",
    "        - qualities: a series of possible qualities to be added to the object\"\"\"\n",
    "        if class_type not in self._onto.classes():\n",
    "            raise MalissiaBaseError(\"The proposed class type ({}) does not belong to this ontology ({})\".format(class_type, self._onto))\n",
    "            \n",
    "        # checking name\n",
    "        if name == \"\":\n",
    "            if warning: logging.warning(\"Trying to create a '{}' with a blank name, passing None instead.\".format(class_type))\n",
    "            name = None\n",
    "        if not isinstance(name,str):\n",
    "            if warning: logging.warning(\"Trying to create a '{}' with a name that is not a string, using default naming instead.\".format(class_type))\n",
    "            name = None\n",
    "\n",
    "        #reformating the qualities\n",
    "        \n",
    "        formated_qualities = self.format_qualities(**qualities)\n",
    "        if physical_space:\n",
    "            non_coordinate_qualities = physical_space.filter_qualities(**formated_qualities)\n",
    "            new_individual = class_type(name= name, **non_coordinate_qualities)\n",
    "            physical_space.set_object_coordinates(new_individual, **qualities)\n",
    "        else:\n",
    "            new_individual = class_type(name= name, **formated_qualities)\n",
    "\n",
    "        return new_individual\n",
    "    \n",
    "    def modify_instance_properties(self, instance, **kwargs):\n",
    "        \"\"\"at this stage, the user of this metod must know if\n",
    "            the prop is functionnal which means one value, or non\n",
    "            functional and the value or values must be introduced in a list \"\"\"\n",
    "        # add maybe types checking methods / and method to verify if is_functional_for\n",
    "        for key, value in kwargs.items():\n",
    "            prop_name = str(key)\n",
    "            setattr(instance, prop_name, value)\n",
    "        return instance\n",
    "\n",
    "    \n",
    "    def copy_individual(self, original_instance, name_of_copied_individual=None, \n",
    "                        visited=None, dict_of_copied_individuals = None):\n",
    "        if dict_of_copied_individuals is None:\n",
    "            dict_of_copied_individuals = {}\n",
    "\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "\n",
    "        if original_instance in visited:\n",
    "            if original_instance.name not in dict_of_copied_individuals:\n",
    "                raise MalissiaBaseError('the instance {} is visited but not copied !'.format(original_instance.name))\n",
    "            return original_instance, dict_of_copied_individuals[original_instance.name]\n",
    "\n",
    "        if original_instance not in visited:\n",
    "            visited.add(original_instance)\n",
    "            sign = True if original_instance.name in dict_of_copied_individuals else False\n",
    "            associa_prop = original_instance.get_properties()\n",
    "            associa_prop\n",
    "            dict_of_prop = {}\n",
    "            for prop in associa_prop:\n",
    "                dict_of_prop[prop.name] = getattr(original_instance, prop.name)\n",
    "            copied_individual, dict_of_copied_individuals = self.copy_an_instance_with_direct_qualities(instance_to_copy = original_instance,\n",
    "                                                            already_copied_individual_but_not_related_individuals = sign,\n",
    "                                                            dict_of_copied_individuals = dict_of_copied_individuals,\n",
    "                                                            name_of_copied_individual = None,\n",
    "                                                            **dict_of_prop ) \n",
    "            for key, value in dict_of_copied_individuals.items():\n",
    "                original_instance, copy = self.copy_individual(\n",
    "                   'value' , name_of_copied_individual=None,  # if the value is used it enters in an endles loop\n",
    "                        visited=visited, dict_of_copied_individuals = dict_of_copied_individuals)\n",
    "                if original_instance.name not in  dict_of_copied_individuals:\n",
    "                    dict_of_copied_individuals[original_instance.name ] = copy\n",
    "                if value not in visited:\n",
    "                    visited.add(value)\n",
    "                \n",
    "            return   original_instance, dict_of_copied_individuals[original_instance.name]\n",
    "                        \n",
    "    def copy_an_instance_with_direct_qualities(self, instance_to_copy, \n",
    "                                               already_copied_individual_but_not_related_individuals = False,\n",
    "                                               dict_of_copied_individuals = None,\n",
    "                                               name_of_copied_individual = None,\n",
    "                                                **kwargs ):\n",
    "        \n",
    "        if dict_of_copied_individuals is None:\n",
    "            dict_of_copied_individuals = {}\n",
    "        if instance_to_copy.name not in dict_of_copied_individuals :\n",
    "            name_of_copied_individual = str(name_of_copied_individual) if \\\n",
    "                                                name_of_copied_individual is not None \\\n",
    "                                                            else f'copy_of_{instance_to_copy.name}'\n",
    "\n",
    "            # Copy the individual\n",
    "            if len(instance_to_copy.is_instance_of) >= 1:\n",
    "                class_of_the_instance = instance_to_copy.is_instance_of[0] if type(instance_to_copy.is_instance_of[0]) \\\n",
    "                    != owl.Thing else original_instance.is_instance_of[1]\n",
    "                copied_individual = class_of_the_instance(name=name_of_copied_individual)\n",
    "                for cls in instance_to_copy.is_instance_of:\n",
    "                    if cls == class_of_the_instance:\n",
    "                        pass\n",
    "                    else:\n",
    "                        copied_individual.is_instance_of.append(cls)\n",
    "                dict_of_copied_individuals[instance_to_copy.name] = copied_individual\n",
    "\n",
    "                for key,  value in kwargs.items():\n",
    "                    if type(getattr(self(), key)) == owl.ObjectPropertyClass and \\\n",
    "                                                            len(getattr(instance_to_copy, key)) >0 :\n",
    "                        related_individuals = getattr(instance_to_copy, key)\n",
    "                        related_copied_individuals = set()\n",
    "                        for related in related_individuals:\n",
    "                            class_of_the_related = related.is_instance_of[0] if type(related.is_instance_of[0]) != owl.Thing else related.is_instance_of[1]\n",
    "                            name_of_copied_related = str('copy_of_{}'.format(related.name))\n",
    "                            copied_related = class_of_the_related(name=name_of_copied_related)\n",
    "                            for cls in related.is_instance_of:\n",
    "                                if cls == class_of_the_related:\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    copied_related.is_instance_of.append(cls)\n",
    "                            if copied_related not in  related_copied_individuals:\n",
    "                                related_copied_individuals.add(copied_related)\n",
    "                            if related.name not in dict_of_copied_individuals:\n",
    "                                dict_of_copied_individuals[related.name] = copied_related\n",
    "                        setattr(copied_individual, key, list(related_copied_individuals)) \n",
    "\n",
    "                for key,  value in kwargs.items():\n",
    "                    if type(getattr(self(), key)) == owl.DataPropertyClass and \\\n",
    "                                    len(getattr(instance_to_copy, key)) >0 :\n",
    "                        setattr(copied_individual, key, value )\n",
    "            return copied_individual, dict_of_copied_individuals\n",
    "        \n",
    "        elif instance_to_copy.name  in dict_of_copied_individuals and \\\n",
    "            already_copied_individual_but_not_related_individuals == False:\n",
    "\n",
    "            copied_individual = dict_of_copied_individuals[instance_to_copy.name]\n",
    "            for key,  value in kwargs.items():\n",
    "                if type(getattr(self(), key)) == owl.ObjectPropertyClass and \\\n",
    "                                                        len(getattr(instance_to_copy, key)) >0 :\n",
    "                    related_individuals = getattr(instance_to_copy, key)\n",
    "                    related_copied_individuals = set()\n",
    "                    for related in related_individuals:\n",
    "                        class_of_the_related = related.is_instance_of[0] if type(related.is_instance_of[0]) != owl.Thing else related.is_instance_of[1]\n",
    "                        name_of_copied_related = str('copy_of_{}'.format(related.name))\n",
    "                        copied_related = class_of_the_related(name=name_of_copied_related)\n",
    "                        for cls in related.is_instance_of:\n",
    "                            if cls == class_of_the_related:\n",
    "                                pass\n",
    "                            else:\n",
    "                                copied_related.is_instance_of.append(cls)\n",
    "                        if copied_related not in  related_copied_individuals:\n",
    "                            related_copied_individuals.add(copied_related)\n",
    "                        if related.name not in dict_of_copied_individuals:\n",
    "                            dict_of_copied_individuals[related.name] = copied_related\n",
    "                    setattr(copied_individual, key, list(related_copied_individuals)) \n",
    "\n",
    "            for key,  value in kwargs.items():\n",
    "                if type(getattr(self(), key)) == owl.DataPropertyClass and \\\n",
    "                                len(getattr(instance_to_copy, key)) >0 :\n",
    "                    setattr(copied_individual, key, value )\n",
    "            return copied_individual, dict_of_copied_individuals\n",
    "        elif instance_to_copy.name  in dict_of_copied_individuals and \\\n",
    "            already_copied_individual_but_not_related_individuals == True:\n",
    "            return dict_of_copied_individuals[instance_to_copy.name], dict_of_copied_individuals\n",
    "\n",
    "    \n",
    "    def remove_instance(self, instance):\n",
    "        self._ontology_backend.destroy_entity(instance)\n",
    "        \n",
    "    def remove_all_instances(self, instances = None):\n",
    "        if instances is None:\n",
    "            instances = self._onto.individuals()\n",
    "        for instance in instances:\n",
    "            self.remove_instance(instance)\n",
    "        \n",
    "    def get_all_classes_of_instance(self,instance):\n",
    "        \"\"\"Returns all the classes of this instance and all its ancestors\n",
    "        \n",
    "        Parameters:\n",
    "        - instance: an instance to be investigated\n",
    "        Returns:\n",
    "        - a set containing all the classes this instance belongs to including the mother classes\n",
    "        Note: be carefull because this is returnin owl.Thing too and may cause issues if not handled\n",
    "        \"\"\"\n",
    "        instance_classes = set(instance.is_instance_of)\n",
    "        instance_classes.discard(self._ontology_backend.Thing)\n",
    "        return set.union(*[instance_class.ancestors() for instance_class in instance_classes])\n",
    "        \n",
    "    def show_instance_qualities(self,instance):\n",
    "        if instance is None: \n",
    "            print(\"The instance is None.\")\n",
    "            return\n",
    "        print(\"instance:\",instance)\n",
    "        print(\"types:\", \",\".join([str(i) for i in instance.is_a]))\n",
    "        print(\"properties:\")\n",
    "        for prop in instance.get_properties():\n",
    "            print(\"|- {}:{}\".format(prop.name, prop[instance]))\n",
    "            \n",
    "    def get_all_instances(self):\n",
    "        return list(self._onto.individuals())\n",
    "    \n",
    "    def show_all_instance_qualities(self, instances= None):\n",
    "        instances = instances if instances is not None else self.get_all_instances()\n",
    "        print(\"Number of instances:\",len(instances))\n",
    "        for i in instances:\n",
    "            self.show_instance_qualities(i)\n",
    "            \n",
    "    def has_quality(self, object, quality):\n",
    "        return getattr(self._onto, quality) in object.get_properties()\n",
    "    \n",
    "    def has_qualities(self, object, qualities):\n",
    "        qualities = np.array(qualities).ravel()\n",
    "        return np.all([self.has_quality(object, quality_i) for quality_i in qualities])\n",
    "            \n",
    "    def get_possible_interpretations_of(self, object):\n",
    "        \"\"\"Returns a list of classes potentially explaining this object\"\"\"\n",
    "        object_classes = object.is_instance_of\n",
    "        if len(object_classes) == 0:\n",
    "            raise MalissiaBaseError(\"Trying to interprete an object without class.\")\n",
    "        possible_interpretations = [interpretation for class_i in object_classes \n",
    "                                    for interpretation in class_i.has_Possible_Explanation ] \n",
    "        return possible_interpretations\n",
    "        \n",
    "    def get_objects_potentially_explained_by(self, object):\n",
    "        \"\"\"Returns the classes this object is potentially explaining.\"\"\"\n",
    "        return object.is_Possible_Explanation_Of\n",
    "    \n",
    "    def isinstance(self, candidate_object, candidate_class):\n",
    "        \"\"\"Checks whether an object is an instance of a given class\"\"\"\n",
    "        return isinstance(candidate_object, candidate_class)\n",
    "    \n",
    "    def issubclass(self, candidate_subclass, candidate_class):\n",
    "        \"\"\"Checks whether an object is an instance of a given class\"\"\"\n",
    "        return issubclass(candidate_subclass, candidate_class)\n",
    "    \n",
    "    def has_representation_of_type(self, candidate_object, representation_type, return_representations= False):\n",
    "        \"\"\"Checks if an instance has a representation of a given type\"\"\"\n",
    "        if (candidate_object.has_Representation is None) or (len(candidate_object.has_Representation) < 1):\n",
    "            return False\n",
    "        reps = np.array(candidate_object.has_Representation)\n",
    "        selected_representations = [self.isinstance(rep_i, representation_type) for rep_i in reps]\n",
    "        if return_representations:\n",
    "            return reps[selected_representations]\n",
    "        else:\n",
    "            return np.any(selected_representations)\n",
    "        \n",
    "    def has_point_representation(self, candidate_object, return_representations= False):\n",
    "        return self.has_representation_of_type(candidate_object, self._onto.Point, return_representations)\n",
    "    \n",
    "    @classmethod\n",
    "    def register_constructor(cls, object_class_name, constructor, condition = None):\n",
    "        \"\"\"Registers a constructor for the given ontology class\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class_name: the type of object to be created (typically, the ontology class name)\n",
    "        - constructor: a function that can be called to generate the given object class.\n",
    "          It will typically be method of the GeologicalKnowledgeFramework and called by\n",
    "          constructor( knowledge_framework= None, interpretation_situation = None, interpretation_status = None, **kargs)\n",
    "        \"\"\"\n",
    "        if (object_class_name is None) or (not isinstance(object_class_name, str)):\n",
    "            raise MalissiaBaseError(\"Registering constructor for an undefined object class.\")\n",
    "        if constructor is None: raise MalissiaBaseError(\"Registering  an undefined constructor for an object class.\")\n",
    "        \n",
    "        if object_class_name in cls.registered_constructors:\n",
    "            cls.registered_constructors[object_class_name] += [constructor]\n",
    "        else:\n",
    "            cls.registered_constructors[object_class_name] = [constructor]\n",
    "        cls.constructor_conditions[constructor] = condition\n",
    "            \n",
    "    def filter_explainable_features(self, object_class, features):\n",
    "        \"\"\"Selects the features that can be explained by a given ontology class.\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class [object]: a type of object used as an explanation\n",
    "        - features: the list of features to be filtered.\n",
    "        Returns:\n",
    "        - a list of features that can be explained by the proposed class\"\"\"\n",
    "        explainable_class_set = set(self.get_objects_potentially_explained_by(object_class))\n",
    "        return [feature_i for feature_i in features \n",
    "                if not self.get_all_classes_of_instance(feature_i).isdisjoint(explainable_class_set)]\n",
    "                # test wether the is an intersection between the classes of the instance and the explainable classes\n",
    "    \n",
    "    def filter_explainable_features_by_type(self, features, type_of_feature = 'any'):\n",
    "        \"\"\"filter list of features according to a given type. Returns all if noe type is specified.\n",
    "        \n",
    "        Parameters:\n",
    "        - features: the list of features to be filtered.\n",
    "        - type_of_feature = an ontology object designating which type of features to retain\n",
    "        \"\"\"\n",
    "        if type_of_feature == 'any':\n",
    "            return features\n",
    "        features =  [feature_i for feature_i in features if isinstance(feature_i, type_of_feature)]\n",
    "        return features\n",
    "\n",
    "    def filter_explainable_features_by_number(self, features, nb_feature = 0):\n",
    "        \"\"\"Select a given number of features.\n",
    "        \n",
    "        Parameters:\n",
    "        - features: the list of features to be filtered.\n",
    "        Returns:\n",
    "        - a list of nb features \"\"\"\n",
    "        if len(features) == 0 :\n",
    "            print(\"Warning: Not enough explainable features in {}. To be implemented.\".format(features))\n",
    "            return None\n",
    "        if nb_feature == 0:\n",
    "            return features\n",
    "        else:\n",
    "            if len(features) < 2 :\n",
    "                print(\"Warning: Not enough explainable features in {}. To be implemented.\".format(features))\n",
    "                return None\n",
    "            else: \n",
    "                return random.sample(features, 2)\n",
    "        \n",
    "    def select_object_constructor(self, object_class_name, \n",
    "                                    interpretation_situation = None,\n",
    "                                    interpretation_status = None,\n",
    "                                    random_choice = False,\n",
    "                                    debug= False,\n",
    "                                    **kargs):\n",
    "        \"\"\"Factory function that generates a constructor for the given ontology class\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class_name: the type of object to be created (typically, the ontology class name)\n",
    "        - interpretation_situation: a `InterpretationSituation` providing a context for the creation\n",
    "        - interpretation_status: the status of the current interpretation process,\n",
    "        which might provide contextual information to decide which constructor to generate\n",
    "        - random_choice: if False (default), the first available constructor is selected, else it is picked randomly\n",
    "        - debug: if True, some debug information is sent, default False. \n",
    "        - kargs: keyword arguments passing available qualities and properties.\n",
    "        This will be used to check whether required information is available.\n",
    "        \n",
    "        Return:\n",
    "        - a function that can be called for creating a new instance of the given object.\n",
    "        \"\"\"\n",
    "        object_class_name = object_class_name if isinstance(object_class_name,str) else object_class_name.name\n",
    "        if debug:\n",
    "            print(\"Looking for a constructor for:\", object_class_name)\n",
    "            print(\" - registered constructors:\\n\", \"\\n\".join([str(constructor) for constructor in self.registered_constructors[object_class_name]]))\n",
    "        if object_class_name not in self.registered_constructors:\n",
    "            raise MalissiaBaseError(\"No available constructor for {}.\".format(object_class_name))\n",
    "        \n",
    "        candidate_constructors = [constructor for constructor in self.registered_constructors[object_class_name]\n",
    "                                  if self.constructor_conditions[constructor](\n",
    "                                        knowledge_framework = self,\n",
    "                                        interpretation_situation = interpretation_situation,\n",
    "                                        interpretation_status = interpretation_status,\n",
    "                                        **kargs)\n",
    "                                  ]\n",
    "        if debug:\n",
    "            print(\" - candidate constructors:\\n\", \"\\n\".join([str(constructor) for constructor in candidate_constructors]))\n",
    "        if len(candidate_constructors) == 0:\n",
    "            raise MalissiaBaseError(\"No available constructor for {} in these conditions.\".format(object_class_name))\n",
    "        \n",
    "        selected_constructor = random.choice(candidate_constructors) if random_choice else candidate_constructors[0]\n",
    "        if debug: print(\"Selected constructor:\",str(selected_constructor))\n",
    "        return selected_constructor\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def register_internal_consistency_evaluator(cls, object_class_name, method):\n",
    "        \"\"\"Registers an evaluation method for internal consistency for the given ontology class\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class_name: the type of object to be created (typically, the ontology class name)\n",
    "        - method: a function that can be called to evaluate internal consistency of the given object class.\n",
    "        \"\"\"\n",
    "        if (object_class_name is None) or (not isinstance(object_class_name, str)):\n",
    "            raise MalissiaBaseError(\"Registering evaluator for an undefined object class.\")\n",
    "        if method is None: raise MalissiaBaseError(\"Registering  an undefined evaluator for an object class.\")\n",
    "        \n",
    "        cls.registered_internal_consistency_evaluation[object_class_name] = method\n",
    "    \n",
    "    def get_internal_consistency_evaluation_method(self, object_class_name):\n",
    "        if not(isinstance(object_class_name, str)):\n",
    "            object_class_name = type(object_class_name).name\n",
    "        return self.registered_internal_consistency_evaluation[object_class_name]\n",
    "    \n",
    "    def evaluate_internal_consistency(self, object, **kargs):\n",
    "        method = self.get_internal_consistency_evaluation_method(object)\n",
    "        return method(object, self, **kargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def register_explanation_consistency_evaluator(cls, object_class_name, method):\n",
    "        \"\"\"Registers an evaluation method for explanation consistency for the given ontology class\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class_name: the type of object to be created (typically, the ontology class name)\n",
    "        - method: a function that can be called to evaluate explanation consistency of the given object class.\n",
    "        \"\"\"\n",
    "        if (object_class_name is None) or (not isinstance(object_class_name, str)):\n",
    "            raise MalissiaBaseError(\"Registering evaluator for an undefined object class.\")\n",
    "        if method is None: raise MalissiaBaseError(\"Registering  an undefined evaluator for an object class.\")\n",
    "        \n",
    "        cls.registered_explanation_consistency_evaluation[object_class_name] = method\n",
    "    \n",
    "    def get_explanation_consistency_evaluation_method(self, object_class_name):\n",
    "        if not(isinstance(object_class_name, str)):\n",
    "            object_class_name = type(object_class_name).name\n",
    "        return self.registered_explanation_consistency_evaluation[object_class_name]\n",
    "    \n",
    "    def evaluate_explanation_consistency(self, object, **kargs):\n",
    "        method = self.get_explanation_consistency_evaluation_method(object)\n",
    "        return method(object, self, **kargs)\n",
    "    \n",
    "    def sync_reasoner(self, **kargs):\n",
    "        \"\"\"Synchronise the reasoner.\n",
    "        \n",
    "        Parameters:\n",
    "        - **kargs:\n",
    "        |- infer_property_values\"\"\"\n",
    "        self._ontology_backend.sync_reasoner(**kargs)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"Describe the knowledge framework\"\"\"\n",
    "        desc = [\"Geological Knowledge Framework:\"]\n",
    "        desc += [\" |- Name: {}\".format(self.name)]\n",
    "        desc += [\" |- Backend: {}\".format(self._ontology_backend)]\n",
    "        desc += [\" |- Source: {}\".format(self._source)]\n",
    "        desc += [\" |- Ontology: {}\".format(self._onto)]\n",
    "        return \"\\n\".join(desc)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def register_updater(cls, object_class_name, updater, condition = None):\n",
    "        \"\"\"Registers an updater for the given ontology class\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class_name: the type of object to be updated (typically, the ontology class name)\n",
    "        - constructor: a function that can be called to updated the given object class.\n",
    "        \"\"\"\n",
    "\n",
    "        if (object_class_name is None) or (not isinstance(object_class_name, str)):\n",
    "            raise MalissiaBaseError(\"Registering constructor for an undefined object class.\")\n",
    "        if updater is None: raise MalissiaBaseError(\"Registering  an undefined updater for an object class.\")\n",
    "        \n",
    "        if object_class_name in cls.registered_updaters:\n",
    "            cls.registered_updaters[object_class_name] += [updater]\n",
    "        else:\n",
    "            cls.registered_updaters[object_class_name] = [updater]\n",
    "        cls.updater_conditions[updater] = condition\n",
    "            \n",
    "    \n",
    "    def select_object_updater(self, object_class_name, \n",
    "                                    interpretation_situation = None,\n",
    "                                    interpretation_status = None,\n",
    "                                    random_choice = False,\n",
    "                                    debug= False,\n",
    "                                    **kargs):\n",
    "        \"\"\"Factory function that generates an updater for the given ontology class\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class_name: the type of object to be created (typically, the ontology class name)\n",
    "        - interpretation_situation: a `InterpretationSituation` providing a context for the update\n",
    "        - interpretation_status: the status of the current interpretation process,  \n",
    "\n",
    "        # must provide anomaly\n",
    "\n",
    "        which might provide contextual information to decide which updater to generate\n",
    "        - random_choice: if False (default), the first available updater is selected, else it is picked randomly\n",
    "        - debug: if True, some debug information is sent, default False. \n",
    "        - kargs: keyword arguments passing available qualities and properties.\n",
    "        This will be used to check whether required information is available.\n",
    "        \n",
    "        Return:\n",
    "        - a function that can be called for creating a new instance of the given object.\n",
    "        \"\"\"\n",
    "        object_class_name = object_class_name if isinstance(object_class_name,str) else object_class_name.name\n",
    "        if debug:\n",
    "            print(\"Looking for a constructor for:\", object_class_name)\n",
    "            print(\" - registered constructors:\\n\", \"\\n\".join([str(updater) for updater in self.registered_updaters[object_class_name]]))\n",
    "        if object_class_name not in self.registered_updaters:\n",
    "            raise MalissiaBaseError(\"No available updater for {}.\".format(object_class_name))\n",
    "        candidate_updaters = [updater for updater in self.registered_updaters[object_class_name]\n",
    "                                  if self.updater_conditions[updater](\n",
    "                                        knowledge_framework = self,\n",
    "                                        interpretation_situation = interpretation_situation,\n",
    "                                        interpretation_status = interpretation_status,\n",
    "                                        **kargs)\n",
    "                                  ]\n",
    "        if debug:\n",
    "            print(\" - candidate constructors:\\n\", \"\\n\".join([str(updater) for updater in candidate_updaters]))\n",
    "        if len(candidate_updaters) == 0:\n",
    "            raise MalissiaBaseError(\"No available updater for {} in these conditions.\".format(object_class_name))\n",
    "        \n",
    "        selected_updater = random.choice(candidate_updaters) if random_choice else candidate_updaters[0]\n",
    "        if debug: print(\"Selected updater:\",str(selected_updater))\n",
    "        return selected_updater\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_coord_dicts(coord_labels, coords):\n",
    "    label_keys = [\"coord{}_label\".format(i+1) for i in range(len(coord_labels))]\n",
    "    coord_keys = [\"coord{}\".format(i+1) for i in range(len(coords))]\n",
    "    coord_label_dict = {label_key:[coord_label_i] for label_key,coord_label_i in zip(label_keys, coord_labels)}\n",
    "    coord_dict = {coord_key:[float(coord_i)] for coord_key,coord_i in zip(coord_keys, coords)}\n",
    "    return coord_label_dict, coord_dict\n",
    "\n",
    "def constructor_point(knowledge_framework, coord_labels, coords, name= None):\n",
    "    coord_label_dict, coord_dict = format_coord_dicts(coord_labels=coord_labels, coords= coords)\n",
    "    point =knowledge_framework().Point(name= name, **coord_label_dict, **coord_dict)\n",
    "    \n",
    "    # it is represented by himself\n",
    "    point.has_Representation = [point]\n",
    "    point.has_Center = [point]\n",
    "    return point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructor_vector(knowledge_framework, coord_labels, coords, name= None):\n",
    "    coord_label_dict, coord_dict = format_coord_dicts(coord_labels= coord_labels, coords= coords)\n",
    "    vector =  knowledge_framework().Vector(name= name, **coord_label_dict, **coord_dict)\n",
    "    \n",
    "    # it is represented by itmself\n",
    "    vector.has_Representation = [vector]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Planar_Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coord_labels(plan):\n",
    "    node = plan.has_Node0[0]\n",
    "    return [node.coord1_label[0],node.coord2_label[0],node.coord3_label[0]]\n",
    "\n",
    "def get_nodes(plan):\n",
    "    return [plan.has_Node0[0], plan.has_Node1[0], plan.has_Node2[0], plan.has_Node3[0]]\n",
    "\n",
    "def get_coord(nodes):\n",
    "    \"\"\"returns the coordinates of a series of nodes as an array (node_index, coord_index)\"\"\"\n",
    "    return [[node_i.coord1[0],node_i.coord2[0],node_i.coord3[0]] for node_i in nodes]\n",
    "    \n",
    "def compute_center(nodes):\n",
    "    coords = get_coord(nodes= nodes)\n",
    "    center_coord = np.mean(coords,axis=0).tolist() \n",
    "    return center_coord\n",
    "\n",
    "def set_center(knowledge_framework, plan):\n",
    "    nodes = get_nodes(plan= plan)\n",
    "    center_coord = compute_center(nodes= nodes) \n",
    "    coord_labels = get_coord_labels(plan= plan)\n",
    "    \n",
    "    name = plan.name + \"_center\"\n",
    "    center = constructor_point(knowledge_framework= knowledge_framework, coord_labels= coord_labels, coords= center_coord, name= name)\n",
    "    plan.has_Center = [center]\n",
    "    return center\n",
    "\n",
    "def compute_principal_vectors_from_nodes(nodes):\n",
    "    coords = np.array(get_coord(nodes= nodes))\n",
    "    u = coords[1] - coords[0]\n",
    "    v = coords[3] - coords[0]\n",
    "    normal = np.cross(u,v)\n",
    "    normal = normal/np.linalg.norm(normal)\n",
    "    return u.tolist(), v.tolist(), normal.tolist()\n",
    "\n",
    "def compute_size_from_nodes(nodes):\n",
    "    coords = np.array(get_coord(nodes= nodes))\n",
    "    u = coords[1] - coords[0]\n",
    "    size = np.linalg.norm(u)\n",
    "    return float(size)\n",
    "\n",
    "def compute_dip_dir_from_normal(normal):\n",
    "    x,y,z = np.sign(normal[2]) * np.array(normal) / np.linalg.norm(normal)\n",
    "    if z == 1:\n",
    "        return 0.,0.\n",
    "    dip = np.rad2deg(np.arccos(z))\n",
    "    dip_dir = np.rad2deg(np.arctan2(x,y)) % 360\n",
    "    return float(np.round(dip, 3)), float(np.round(dip_dir, 3))\n",
    "\n",
    "def set_attitude(knowledge_framework, plan):\n",
    "    nodes = get_nodes(plan= plan)\n",
    "    coord_labels = get_coord_labels(plan= plan)\n",
    "    u, v, normal = compute_principal_vectors_from_nodes(nodes= nodes)\n",
    "    size = compute_size_from_nodes(nodes= nodes)\n",
    "    plan.size = [size]\n",
    "    \n",
    "    normal_name = plan.name + \"_normal\"\n",
    "    normal_entity = constructor_vector(knowledge_framework= knowledge_framework, coord_labels= coord_labels, coords= normal, name= normal_name)\n",
    "    plan.has_Normal = [normal_entity]\n",
    "    \n",
    "    dip, dip_dir = compute_dip_dir_from_normal(normal= normal)\n",
    "    plan.dip = [dip]\n",
    "    plan.dip_dir = [dip_dir]\n",
    "    return dip, dip_dir, normal, size\n",
    "\n",
    "def set_nodes(plan, nodes):\n",
    "    if(len(nodes) != 4): raise MalissiaBaseError(\"Please give 4 nodes in, {} were given.\".format(len(nodes)))\n",
    "    plan.has_Node0 = [nodes[0]]\n",
    "    plan.has_Node1 = [nodes[1]]\n",
    "    plan.has_Node2 = [nodes[2]]\n",
    "    plan.has_Node3 = [nodes[3]]\n",
    "    plan.has_Representation = nodes\n",
    "\n",
    "def set_nodes_succession(node0, node1, node2, node3):\n",
    "\n",
    "    node0.has_Next_Nodes.append([node1, node2, node3])\n",
    "    node0.has_Previous_Nodes.append([node3, node2, node1])\n",
    "\n",
    "    node1.has_Next_Nodes.append([node2, node3, node0])\n",
    "    node1.has_Previous_Nodes.append([node0, node3, node2])\n",
    "\n",
    "    node2.has_Next_Nodes.append([node3, node0, node1])\n",
    "    node2.has_Previous_Nodes.append([node1, node0, node3])\n",
    "\n",
    "    node3.has_Next_Nodes.append([node0, node1, node2])\n",
    "    node3.has_Previous_Nodes.append([node2, node1, node0])\n",
    "    \n",
    "def create_planar_surface(knowledge_framework, nodes, name= None):\n",
    "    plan = knowledge_framework().Planar_Surface(name= name)\n",
    "    set_nodes(plan,nodes)\n",
    "    return plan\n",
    "\n",
    "def create_nodes_from_coords(knowledge_framework, coords, coord_labels, name= None):\n",
    "    names = [None if name is None else name + \"_N\" + str(i) for i in range(len(coords))]\n",
    "    nodes = [\n",
    "        constructor_point(knowledge_framework= knowledge_framework, coord_labels= coord_labels, coords= coord_i, name= name_i)\n",
    "        for coord_i, name_i in zip(coords,names)\n",
    "    ]\n",
    "    return nodes\n",
    "\n",
    "def compute_normal_from_dip_dir(dip, dip_dir, polarity= 1):\n",
    "    dip_rad = np.deg2rad(dip)\n",
    "    dip_dir_rad = np.deg2rad(dip_dir)\n",
    "    z = np.cos(dip_rad)\n",
    "    h = np.sin(dip_rad)\n",
    "    y = h * np.cos(dip_dir_rad)\n",
    "    x = h * np.sin(dip_dir_rad)\n",
    "    normal = polarity * np.array([x,y,z])\n",
    "    return normal.tolist()\n",
    "\n",
    "def compute_principal_vectors_from_dip_dir(dip, dip_dir, polarity= 1, as_array= False):\n",
    "    \"\"\"returns normal, dip_vector, azimuth_vector\"\"\"\n",
    "    dip_rad = np.deg2rad(dip)\n",
    "    dip_dir_rad = np.deg2rad(dip_dir)\n",
    "    nz =  np.cos(dip_rad)\n",
    "    h = np.sin(dip_rad)\n",
    "    hy = np.cos(dip_dir_rad)\n",
    "    hx = np.sin(dip_dir_rad)\n",
    "    ny = h * hy\n",
    "    nx = h * hx\n",
    "    normal = polarity * np.array([nx,ny,nz])\n",
    "    \n",
    "    uz = -h\n",
    "    ux = nz * hx\n",
    "    uy = nz * hy\n",
    "    dip_vector = np.array([ux,uy,uz])\n",
    "    \n",
    "    az_vector = np.cross(normal, dip_vector)\n",
    "    \n",
    "    if as_array:\n",
    "        return normal, dip_vector, az_vector\n",
    "    else:\n",
    "        return normal.tolist(), dip_vector.tolist(), az_vector.tolist()\n",
    "\n",
    "def compute_coords(center, dip_vector, az_vector, size):\n",
    "    center     = np.array(center)\n",
    "    dip_vector = np.array(dip_vector)\n",
    "    az_vector  = np.array(az_vector)\n",
    "    coords = np.array([\n",
    "        center - size/2 * dip_vector - size/2 * az_vector,\n",
    "        center + size/2 * dip_vector - size/2 * az_vector,\n",
    "        center + size/2 * dip_vector + size/2 * az_vector,\n",
    "        center - size/2 * dip_vector + size/2 * az_vector\n",
    "    ])\n",
    "    return coords.tolist()\n",
    "\n",
    "def polarity_to_bool(polarity):\n",
    "    return bool(polarity > 0)\n",
    "\n",
    "def polarity_to_float(polarity:bool):\n",
    "    return 1 if polarity else -1\n",
    "\n",
    "def constructor_planar_surface_from_nodes(knowledge_framework, nodes, name= None):\n",
    "    plan = create_planar_surface(knowledge_framework= knowledge_framework, nodes= nodes, name= name)\n",
    "    center = set_center(knowledge_framework= knowledge_framework, plan= plan)\n",
    "    dip, dip_dir, normal, size = set_attitude(knowledge_framework= knowledge_framework, plan= plan)\n",
    "    return plan\n",
    "\n",
    "def constructor_planar_surface_from_coords(knowledge_framework, coords, coord_labels, name= None):\n",
    "    nodes = create_nodes_from_coords(knowledge_framework= knowledge_framework, coords= coords, coord_labels= coord_labels, name= name)\n",
    "    return constructor_planar_surface_from_nodes(knowledge_framework= knowledge_framework, nodes= nodes, name= name)\n",
    "\n",
    "def constructor_planar_surface_from_center_attitude(knowledge_framework, coord_labels, \n",
    "                                                    center, dip, dip_dir, size= None,\n",
    "                                                     polarity= True, name= None):\n",
    "    normal, dip_vector, az_vector = compute_principal_vectors_from_dip_dir(dip= dip, dip_dir= dip_dir, polarity= polarity)\n",
    "    coords = compute_coords(center= center, dip_vector= dip_vector, az_vector= az_vector, size= size)\n",
    "    nodes = create_nodes_from_coords(knowledge_framework= knowledge_framework, coords= coords, coord_labels= coord_labels)\n",
    "    plan = create_planar_surface(knowledge_framework= knowledge_framework, nodes= nodes, name= name)\n",
    "    \n",
    "    plan.dip = [float(dip)]\n",
    "    plan.dip_dir = [float(dip_dir)]\n",
    "    plan.polarity = [polarity_to_bool(polarity)]\n",
    "    plan.size = [float(size)]\n",
    "    \n",
    "    center_name = plan.name + \"_center\"\n",
    "    center_entity = constructor_point(knowledge_framework= knowledge_framework, coord_labels= coord_labels, coords= center, name= center_name)\n",
    "    plan.has_Center= [center_entity]\n",
    "    \n",
    "    normal_name = plan.name + \"_normal\"\n",
    "    normal_entity = constructor_vector(knowledge_framework= knowledge_framework, coord_labels= coord_labels, coords= normal, name= normal_name)\n",
    "    plan.has_Normal = [normal_entity]\n",
    "    return plan\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Planar Surface anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consistency between nodes order and normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conditions_for_constructor_observation(knowledge_framework:GeologicalKnowledgeFramework, **kargs):\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for observation constructor: knowledge framework\")\n",
    "        return False\n",
    "    if (\"dataset\" not in kargs) or (kargs[\"dataset\"] is None):\n",
    "        print(\"Wrong condition for observation constructor: dataset\")\n",
    "        return False\n",
    "    dataset = kargs[\"dataset\"]\n",
    "    if (dataset.physical_space is None):\n",
    "        print(\"Wrong condition for observation constructor: physical space\")\n",
    "        return False\n",
    "    if not np.all([coord in kargs for coord in dataset.physical_space.coordinate_labels]):\n",
    "        print(\"Wrong condition for observation constructor: coord\")\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "def constructor_observation(knowledge_framework= None, name: str= None,\n",
    "                            dataset= None, physical_space= None, coord_labels= None,\n",
    "                            **kargs):\n",
    "    \"\"\"Constructor of observations\n",
    "    \n",
    "    Parameters:\n",
    "    - knowledge_framework: defining the existing objects, if None it is inferred from the dataset if given\n",
    "    - name: the name of the observation (similar to an observation id). If None a default one is given.\n",
    "    - dataset: (optional) the dataset to which this observation belongs.\n",
    "    If given, the physical space is taken from there unless it is specified\n",
    "    - physical_space: (optional) the physical space in which this observation is defined.\n",
    "    If not given, it is taken from the dataset, or if not dataset is provided, the coordinate_labels must be given\n",
    "    - coord_labels: (optional) a list of coordinate names, required only if no physical space is provided\n",
    "    - kargs: keyword arguments, containing:\n",
    "        - spatial coordinates with names matching the dataset.physical_space coordinate labels\n",
    "        - any other quality that should be associated with the observation\n",
    "         * size: should be set, this is the size of the area represented by this observation\n",
    "         * dip and dip_dir: optional, make it an orientation observation, should both be set together\n",
    "         * polarity: optional, only used if dip an ddip_dir are set, boolean indicating the polarity of the observed feature.\n",
    "         True means up (or towards the dip_dir if vertical), False is the opposite\n",
    "         * occurrence: optional make it an occurrence observation, True (thing has been observed)\n",
    "         or False (it has been observed that the thing is not here). Note: if the observation is not made on the\n",
    "         occurrence of the observed object, don't set this property, do not set it to False.\n",
    "         * observed_object: the name of the observed object\n",
    "        - qualities whose values is None are not set\n",
    "        - Note that the dataset.physical_space will be asked to filter the provided information\n",
    "        \n",
    "    Return:\n",
    "    - the created observation\n",
    "    \"\"\"\n",
    "    \n",
    "    if knowledge_framework is None:\n",
    "        if dataset is not None:\n",
    "            knowledge_framework = dataset.knowledge_framework\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"provide at least a knowledge framework or a dataset\")\n",
    "    \n",
    "    # create Observation\n",
    "    obs = knowledge_framework().PointBased_Observation(name= name)\n",
    "\n",
    "    # create a point as geometrical support\n",
    "    physical_space = physical_space if physical_space is not None else \\\n",
    "        (dataset.physical_space if dataset is not None else None)\n",
    "    coord_labels = physical_space.coordinate_labels if physical_space is not None else coord_labels\n",
    "    if coord_labels is None:\n",
    "        raise MalissiaBaseError(\"coordinate labels must be specified somehow.\")\n",
    "    coords = [kargs[i] for i in coord_labels]\n",
    "    point = constructor_point(knowledge_framework= knowledge_framework,\n",
    "                              coord_labels= coord_labels, coords= coords,\n",
    "                              name= obs.name + \"_point\")\n",
    "    obs.has_Center = [point]\n",
    "    obs.has_Representation = [point]\n",
    "    \n",
    "    # add size\n",
    "    if \"size\" in kargs:\n",
    "        obs.size = [kargs[\"size\"]]\n",
    "    elif physical_space is not None:\n",
    "        domain_size = float(max(physical_space.get_size()))\n",
    "        obs.size = [domain_size / 10]\n",
    "    else:\n",
    "        obs.size = [1]\n",
    "    \n",
    "    # add observed qualities\n",
    "    if \"occurrence\" in kargs:\n",
    "        obs.occurrence = [kargs[\"occurrence\"]]\n",
    "    if (\"dip\" in kargs) and (\"dip_dir\" in kargs):\n",
    "        obs.dip     = [kargs[\"dip\"]]\n",
    "        obs.dip_dir = [kargs[\"dip_dir\"]]\n",
    "        if \"polarity\" in kargs:\n",
    "            obs.polarity = [polarity_to_bool(kargs[\"polarity\"])]\n",
    "        else:\n",
    "            obs.polarity = [True]\n",
    "            \n",
    "    # find or create observed object\n",
    "    if \"observed_object\" in kargs and kargs[\"observed_object\"] is not None:\n",
    "        observed = knowledge_framework.search(type=knowledge_framework().Geological_Concept, name= kargs[\"observed_object\"])\n",
    "        if len(observed) == 0:\n",
    "            new_observed = knowledge_framework().Stratigraphic_Surface(name=kargs[\"observed_object\"])\n",
    "            obs.is_Observation_Of = [new_observed]\n",
    "        elif len(observed) == 1:\n",
    "            obs.is_Observation_Of = observed\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"Warning: observed object ({}) could correspond to more than one among: {}\".format(\n",
    "                kargs[\"observed_object\"],\n",
    "                \", \".join([str(observed_i) for observed_i in observed])\n",
    "                ))\n",
    "    \n",
    "    obs.is_Explained_By = []\n",
    "    \n",
    "    return obs\n",
    "\n",
    "if \"Observation\" in GeologicalKnowledgeFramework.registered_constructors:\n",
    "    del GeologicalKnowledgeFramework.registered_constructors[\"Observation\"]\n",
    "GeologicalKnowledgeFramework.register_constructor(\"Observation\", \n",
    "                                                  constructor= constructor_observation,\n",
    "                                                  condition= conditions_for_constructor_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal consistency anomaly\n",
    "# eg. dip, dip_dir out of range\n",
    "# must have at least (dip and dip_dir) or occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InterpretationSituation(object):\n",
    "    \"\"\"Defines a situation of interpretation.\n",
    "    \n",
    "    A situation is gathering:\n",
    "    - self.features: features to be explained\n",
    "    - self.context: an interpretation context, ie. elements to be considered during the interpretation\n",
    "    - self.process: the interpretation process in which this situation arises\n",
    "    - self.candidate_explanation_class: the class selected as possible explanation\n",
    "    - self.explaining_object: an instance of the self.candidate_explanation_class, either newly created or reused\n",
    "    - self.existing_explaining_object: a boolean to tell if the self.explaining_object is reused (True) or new (False)\n",
    "    - self.anomalies: the detected anomalies regarding this situation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features, process= None, context = None, knowledge_framework= None) -> None:\n",
    "        \"\"\"Initialises the situation\n",
    "        \n",
    "        Parameters:\n",
    "         - self.features: features to be explained\n",
    "         - self.context: an interpretation context, ie. elements to be considered during the interpretation\n",
    "         - self.process: the interpretation process in which this situation arises\n",
    "         - self.candidate_explanation_class: the class selected as possible explanation\n",
    "         - self.explaining_object: an instance of the self.candidate_explanation_class, either newly created or reused\n",
    "         - self.existing_explaining_object: a boolean to tell if the self.explaining_object is reused (True) or new (False)\n",
    "         - self.anomalies: the detected anomalies regarding this situation\n",
    "         \"\"\"\n",
    "        self.features = np.array(features).ravel() if features is not None else np.array([])\n",
    "        self.context = np.array(context).ravel() if context is not None else np.array([])\n",
    "        self.candidate_explanation_class = None\n",
    "        self.explaining_object = None\n",
    "        self.existing_explaining_object = None\n",
    "        self.anomalies = []\n",
    "        self.process = process\n",
    "        self.knowledge_framework = knowledge_framework if knowledge_framework is not None else \\\n",
    "            (process.knowledge_framework if process is not None else None)\n",
    "    \n",
    "    def __str__(self):\n",
    "        out = []\n",
    "        #out += [\"Process: {}\".format(self.process)]\n",
    "        #out += [\"Knowledge Framework: {}\".format(self.knowledge_framework)]\n",
    "        out += [\"Features: {}\".format(\"None\" if (self.features is None) or (len(self.features) == 0)else\", \".join([str(i) for i in self.features]))]\n",
    "        out += [\"Context: {}\".format(\"None\" if (self.context is None) or (len(self.context) == 0) else \", \".join([str(i) for i in self.context]))]\n",
    "        out += [\"Candidate explaining class: \" + str(self.candidate_explanation_class)]\n",
    "        out += [\"Explaining Object: {}\".format(None if self.explaining_object is None else str(self.explaining_object))\\\n",
    "            +(\"(reused)\" if self.existing_explaining_object else \"\")\n",
    "            ]\n",
    "        out += [\"Anomalies: {}\".format(\"None\" if (self.anomalies is None) or (len(self.anomalies) == 0)else\", \".join([str(i) for i in self.anomalies]))]\n",
    "        return \"\\n\".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratigraphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conditions_for_constructor_surface_part_from_interpretation(\n",
    "                                            knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            interpretation_situation:InterpretationSituation, **kargs):\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for Surface Part constructor: knowledge framework\")\n",
    "        return False\n",
    "    if (interpretation_situation is None):\n",
    "        print(\"Wrong condition for Surface Part constructor: interpretation situation\")\n",
    "        return False\n",
    "    if  (interpretation_situation.process is None) or (interpretation_situation.process.representation_space is None):\n",
    "        print(\"Wrong condition for Surface Part constructor: physical space\")\n",
    "        return False\n",
    "    if  (interpretation_situation.features is None) or (len(interpretation_situation.features) == 0):\n",
    "        print(\"Wrong condition for Surface Part constructor: features\")\n",
    "        return False\n",
    "    \n",
    "    constructed_class = knowledge_framework().Stratigraphic_Part\n",
    "    explainable_features = knowledge_framework.filter_explainable_features(constructed_class, interpretation_situation.features)\n",
    "    if len(explainable_features) == 0:\n",
    "        print(\"Wrong condition for Surface Part constructor: No explainable feature in the situation with this interpretation. To be implemented.\")\n",
    "        return False\n",
    "    # in any other cases\n",
    "    return True\n",
    "\n",
    "def conditions_for_constructor_surface_part_from_interpretation_of_parts(\n",
    "                                            knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            interpretation_situation:InterpretationSituation, **kargs):\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for Surface Part constructor: knowledge framework\")\n",
    "        return False\n",
    "    if (interpretation_situation is None):\n",
    "        print(\"Wrong condition for Surface Part constructor: interpretation situation\")\n",
    "        return False\n",
    "    if  (interpretation_situation.process is None) or (interpretation_situation.process.representation_space is None):\n",
    "        print(\"Wrong condition for Surface Part constructor: physical space\")\n",
    "        return False\n",
    "    if  (interpretation_situation.features is None) or (len(interpretation_situation.features) == 0):\n",
    "        print(\"Wrong condition for Surface Part constructor: features\")\n",
    "        return False\n",
    "   \n",
    "    explained_class = knowledge_framework().Stratigraphic_Part\n",
    "    explainable_features_strati = knowledge_framework.filter_explainable_features_by_type( interpretation_situation.features, \n",
    "                                                                                   type_of_feature = explained_class)\n",
    "    \n",
    "    if len(explainable_features_strati) < 2:\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "def get_physical_space_in_constructor(interpretation_situation, physical_space= None, **kargs):\n",
    "    \"\"\"Gets a physical representation space from an interpreted situation of context\"\"\"\n",
    "    if physical_space is not None:\n",
    "        return physical_space\n",
    "    if interpretation_situation is not None \\\n",
    "    and interpretation_situation.process is not None \\\n",
    "    and interpretation_situation.process.representation_space is not None:\n",
    "        return interpretation_situation.process.representation_space\n",
    "    else:\n",
    "        raise MalissiaBaseError(\"Wrong conditions for physical space definition.\")\n",
    "\n",
    "def get_feature_locations(features, physical_space):\n",
    "    \"\"\"Returns the coordinates of the feature representations\n",
    "    \n",
    "    Parameters:\n",
    "    - features: a list of features, must have a representation\n",
    "    - physical_space: either a physical space that will be used to recover there geometry or\n",
    "    \"original\" keyword, meaning the coordinates must be recovered from the original coordinates directly.\n",
    "    NB: this will assume all the features have the same coordinates and labels.\"\"\"\n",
    "    if physical_space == \"original\":\n",
    "        reps = [feature_i.has_Representation for feature_i in features]\n",
    "        filtered_reps = [rep_i[0] for rep_i in reps if (rep_i is not None) and (len(rep_i)>0)]\n",
    "        coords = [[rep_i.coord1[0], rep_i.coord2[0], rep_i.coord3[0]] for rep_i in filtered_reps]\n",
    "        return np.array(coords)\n",
    "    else:\n",
    "        return np.array([physical_space.get_object_coordinates(feature_i) for feature_i in features])\n",
    "\n",
    "def location_constructor(features, physical_space, method= \"average\", return_as_dict= False, **kargs):\n",
    "    \"\"\"Estimated a location from a given situation and or space\n",
    "    \n",
    "    Parameters:\n",
    "    - features: a list of features that can inform the location, if empty the location is set from the physical_space\n",
    "    - physical_space: the space in whichthe new location is to be defined\n",
    "    - method: determines how th e new location should be estimated (pick, random, average)\n",
    "    Returns:\n",
    "    - a dict matching each coordinate label with its coordinate value\n",
    "    \"\"\" \n",
    "    if physical_space is None: raise MalissiaBaseError(\"a physical space is required, use get_physical_space_in_constructor\")\n",
    "    if len(features) == 0:\n",
    "        # set from the physical space\n",
    "        if method == \"average\":\n",
    "            location = physical_space.get_center()\n",
    "        else:\n",
    "            location = physical_space.generate_random_location()\n",
    "    else:\n",
    "        coords = get_feature_locations(features, physical_space)\n",
    "        if method == \"average\":\n",
    "            # take the average position as new position\n",
    "            location = np.mean(coords, axis= 0)\n",
    "        elif method == \"pick\":\n",
    "            # pick a random location from the dataset\n",
    "            location = np.random.default_rng().choice(coords)\n",
    "        elif method == \"random\":\n",
    "            # generate a random location in the space covered by the data points\n",
    "            mean = np.mean(coords, axis= 0)\n",
    "            cov = np.cov(coords, rowvar=False)\n",
    "            location = np.random.default_rng().multivariate_normal(mean, cov)\n",
    "        return physical_space.coordinates_to_dict(location) if return_as_dict else location.tolist()\n",
    "        \n",
    "\n",
    "def format_attitude_qualities(dip, dip_dir, polarity= True, size= 1):\n",
    "    return {\"dip\":float(dip),\"dip_dir\":float(dip_dir), \"polarity\": polarity_to_bool(polarity), \"size\": float(size)}\n",
    "\n",
    "def attitude_constructor_from_space(physical_space):\n",
    "    \"\"\"Estimated attitude from a list of points\"\"\"\n",
    "    dip = physical_space.generate_random_dip()\n",
    "    dip_dir = physical_space.generate_random_dip_dir()\n",
    "    polarity = physical_space.generate_random_polarity()\n",
    "    size = physical_space.generate_random_size()\n",
    "    return format_attitude_qualities(dip= dip, dip_dir= dip_dir, polarity= polarity, size= size)\n",
    "    \n",
    "def compute_extremities(features, physical_space):\n",
    "    principal_vectors = [compute_principal_vectors_from_dip_dir(\n",
    "        dip= feature_i.dip[0],\n",
    "        dip_dir= feature_i.dip_dir[0],\n",
    "        polarity= polarity_to_float(feature_i.polarity[0]),\n",
    "        as_array=True\n",
    "    )\n",
    "     for feature_i in features]\n",
    "    centers = get_feature_locations(features, physical_space)\n",
    "    sizes = [feature_i.size[0] for feature_i in features]\n",
    "    coords = [[c_i - 0.5*s_i * vec_i[1] - 0.5*s_i * vec_i[2],\n",
    "              c_i + 0.5*s_i * vec_i[1] - 0.5*s_i * vec_i[2],\n",
    "              c_i + 0.5*s_i * vec_i[1] + 0.5*s_i * vec_i[2],\n",
    "              c_i - 0.5*s_i * vec_i[1] + 0.5*s_i * vec_i[2],\n",
    "              ]\n",
    "             for vec_i, c_i, s_i in zip(principal_vectors,centers,sizes)]\n",
    "    return np.array(coords).reshape((-1,3))\n",
    "\n",
    "def average_attitude_feature_from_extremities(features, physical_space):\n",
    "    \"\"\"Estimates attitude by associating extremities to each feature and computing a medium plan\"\"\"\n",
    "    coords = compute_extremities(features, physical_space).reshape((-1,3))\n",
    "    attitude = attitude_constructor_from_points(coords, physical_space, method= \"average\")\n",
    "    attitude[\"polarity\"] = True # todo: take account of polarity\n",
    "    return attitude\n",
    "    \n",
    "def average_attitude_feature_from_vectors(features, physical_space):\n",
    "    \"\"\"Estimate the attitude by taking the average normal vector\"\"\"\n",
    "    vectors = [compute_normal_from_dip_dir(\n",
    "                dip= feature_i.dip[0],\n",
    "                dip_dir= feature_i.dip_dir[0],\n",
    "                polarity= polarity_to_float(feature_i.polarity[0])\n",
    "            ) for feature_i in features]\n",
    "    normal = physical_space.compute_average_vector(vectors)\n",
    "    dip, dip_dir, polarity = physical_space.compute_dip_dir_from_normal(normal)\n",
    "    size = np.mean([feature_i.size[0] for feature_i in features])\n",
    "    polarity = physical_space.generate_random_polarity()\n",
    "    \n",
    "    return format_attitude_qualities(\n",
    "        dip= dip, dip_dir= dip_dir,\n",
    "        polarity= polarity, size= size\n",
    "    )\n",
    "        \n",
    "def attitude_constructor_from_attitude_features(features, physical_space, method= \"average\"):\n",
    "    \"\"\"Estimated attitude from a list of features having attitudes\"\"\"\n",
    "    if method == \"pick\":\n",
    "        picked_feature = np.random.default_rng().choice(features)\n",
    "        return format_attitude_qualities(\n",
    "            dip= picked_feature.dip[0], dip_dir= picked_feature.dip_dir[0],\n",
    "            polarity= picked_feature.polarity[0], size= picked_feature.size[0]\n",
    "        )\n",
    "    elif method == \"average\":\n",
    "        return average_attitude_feature_from_extremities(features, physical_space)\n",
    "    elif method == \"average_vector\":\n",
    "        return average_attitude_feature_from_vectors(features, physical_space)\n",
    "    else:\n",
    "        raise MalissiaBaseError(\"unimplemented method: \"+method)\n",
    "    \n",
    "\n",
    "def attitude_constructor_from_points(points, physical_space, method= \"average\"):\n",
    "    \"\"\"Estimated attitude from a list of points\"\"\"\n",
    "    if len(points) < 2:\n",
    "        raise MalissiaBaseError(\"not enough points to estimate attitude\")\n",
    "    if len(points) == 2:\n",
    "        line = physical_space.compute_line_attitude_from_two_points(*points)\n",
    "        if method == \"average\":\n",
    "            plane = line\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"method not implemented yet: \" + method)\n",
    "    else:\n",
    "        if method == \"average\":\n",
    "            plane = physical_space.compute_attitude_from_points(points)\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"method not implemented yet: \" + method)\n",
    "        \n",
    "    polarity = physical_space.generate_random_polarity()\n",
    "    return format_attitude_qualities(dip= plane[\"dip\"], dip_dir= plane[\"dip_dir\"], size= plane[\"size\"], polarity= polarity)\n",
    "\n",
    "def attitude_constructor_from_single_feature(feature, physical_space, knowledge_framework= None):\n",
    "    # if it has dip and dip_dir -> use it\n",
    "    dip = feature.dip[0] if knowledge_framework.has_quality(feature, \"dip\") else physical_space.generate_random_dip()\n",
    "    dip_dir = feature.dip_dir[0] if knowledge_framework.has_quality(feature, \"dip_dir\") else physical_space.generate_random_dip_dir()\n",
    "    size = feature.size[0] if knowledge_framework.has_quality(feature, \"size\") else physical_space.generate_random_size(min_perc= 0.05, max_perc= 0.25)\n",
    "    return format_attitude_qualities(dip= dip, dip_dir= dip_dir, size=size)\n",
    "\n",
    "def get_coords_from_planar_surfaces(features, physical_space, knowledge_framework):\n",
    "    features = [feature_i for feature_i in features\n",
    "                if (knowledge_framework.isinstance(feature_i, knowledge_framework().Planar_Surface))\n",
    "                or (knowledge_framework.isinstance(feature_i, knowledge_framework().Stratigraphic_Part))]\n",
    "    coords = [physical_space.get_object_coordinates(feature_i, kind=\"nodes\") for feature_i in features]\n",
    "    return coords\n",
    "    \n",
    "def get_coords_from_occurrence_observations(features, physical_space, knowledge_framework):\n",
    "    features = [feature_i for feature_i in features\n",
    "                if (knowledge_framework.isinstance(feature_i, knowledge_framework().Point))\n",
    "                or ((knowledge_framework.isinstance(feature_i, knowledge_framework().PointBased_Observation))\n",
    "                    and ((feature_i.dip is None) or (len(feature_i.dip) == 0)))]\n",
    "    coords = [physical_space.get_object_coordinates(feature_i, kind=\"center\") for feature_i in features]\n",
    "    return coords\n",
    "    \n",
    "def get_coords_from_orientation_observations(features, physical_space, knowledge_framework):\n",
    "    features = [feature_i for feature_i in features\n",
    "                if (knowledge_framework.isinstance(feature_i, knowledge_framework().PointBased_Observation))\n",
    "                and ((feature_i.dip is not None) and (len(feature_i.dip) > 0))]\n",
    "    coords = compute_extremities(features, physical_space)\n",
    "    return coords.tolist()\n",
    "    \n",
    "def attitude_constructor(features, physical_space, knowledge_framework= None, method= \"average\",\n",
    "                         regression=\"orthogonal\", extension_factor= 0.05, **kargs):\n",
    "    \"\"\"Estimated the attitude of a feature from a given situation and or space\n",
    "    \n",
    "    Parameters:\n",
    "    - features: a list of features that can inform the location\n",
    "    - physical_space: the space in which the new location is to be defined\n",
    "    - method: determines how the new attitude should be estimated (pick, random, average)\n",
    "    - extension_factor: float, default 0.05, amount of extra size on both sides \n",
    "    for the inferred length to go slightly beyond the data points\n",
    "    \"\"\" \n",
    "    if physical_space is None: raise MalissiaBaseError(\"a physical space is required, use get_physical_space_in_constructor\")\n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework() if knowledge_framework is None else knowledge_framework\n",
    "   \n",
    "    if len(features) == 0:\n",
    "        return attitude_constructor_from_space(physical_space)\n",
    "    elif len(features) == 1:\n",
    "        return attitude_constructor_from_single_feature(features[0], \n",
    "                                                        physical_space= physical_space, \n",
    "                                                        knowledge_framework= knowledge_framework)\n",
    "    else:\n",
    "        # filtering features        \n",
    "        # planar surfaces have nodes -> use the nodes\n",
    "        coords = get_coords_from_planar_surfaces(features, physical_space, knowledge_framework)\n",
    "        if isinstance(coords, list):\n",
    "            coords = [item for sublist in coords for item in sublist]\n",
    "        if isinstance(coords, np.ndarray):\n",
    "            coords = coords.ravel()\n",
    "        # Observation of orientation have dip_dir dip size -> compute and use extremities\n",
    "        coords +=  get_coords_from_orientation_observations(features, physical_space, knowledge_framework)\n",
    "        \n",
    "        # Observations with orientation -> use their center 4 times (to equilibrate with the others)\n",
    "        coords += 4 * get_coords_from_occurrence_observations(features, physical_space, knowledge_framework)\n",
    "\n",
    "        coords = [np.array(co) for co in coords]\n",
    "        coords = np.array(coords)\n",
    "        \n",
    "        plane = {}\n",
    "        plane[\"center\"] = np.mean(coords, axis=0)\n",
    "        dx = coords[:,0] - plane[\"center\"][0]\n",
    "        dz = coords[:,2] - plane[\"center\"][2]\n",
    "        if np.all(dx == 0):\n",
    "            plane[\"dip\"] = 90.\n",
    "            plane[\"dip_dir\"] = 90.\n",
    "            plane[\"polarity\"] = 1\n",
    "            plane[\"size\"] = 2*np.max(np.abs(dz))\n",
    "        else:\n",
    "            cov =  np.dot(dx,dz)\n",
    "            vx = np.dot(dx,dx)\n",
    "            vz = np.dot(dz,dz)\n",
    "            if regression == \"orthogonal\":# orthogonal regression\n",
    "                if cov == 0.:\n",
    "                    a = 0\n",
    "                else:\n",
    "                    C = 0.5 * (vx - vz) / cov\n",
    "                    a = -C + np.sign(cov) * np.sqrt(C*C + 1)\n",
    "            elif regression == \"z\":\n",
    "                a = cov / vz\n",
    "            else:\n",
    "                a = cov / vx\n",
    "            \n",
    "            plane[\"dip\"] = np.rad2deg(np.arctan(np.abs(a)))\n",
    "            plane[\"dip_dir\"] = 270. if np.sign(a) >= 0 else 90.\n",
    "            plane[\"polarity\"] = 1\n",
    "            \n",
    "            dip_vector = np.array([np.cos(a), np.sin(a)])\n",
    "            p_dxdz = np.array([dx,dz]).T\n",
    "            plane[\"size\"] = 2 * (1 + extension_factor) * np.max(np.abs(np.dot(p_dxdz, dip_vector)))\n",
    "        return format_attitude_qualities(dip= plane[\"dip\"], dip_dir= plane[\"dip_dir\"], \n",
    "                                         size= plane[\"size\"], polarity= plane[\"polarity\"])\n",
    "    \n",
    "        # is_instance_of_point = [knowledge_framework.has_point_representation(feature_i) for feature_i in features]\n",
    "        # point_features = features[is_instance_of_point]\n",
    "        # points = [physical_space.get_object_coordinates(point_feature_i) for point_feature_i in point_features]\n",
    "        \n",
    "        # is_attitude_instance = [knowledge_framework.has_qualities(feature_i,[\"dip\",\"dip_dir\"]) for feature_i in features]\n",
    "        # attitude_features = features[is_attitude_instance]\n",
    "        \n",
    "        # # if only points then create from them\n",
    "        # if np.all(is_attitude_instance):\n",
    "        #     return attitude_constructor_from_attitude_features(attitude_features, physical_space= physical_space, method= method)\n",
    "        # elif np.all(is_instance_of_point):\n",
    "        #     return attitude_constructor_from_points(points, physical_space= physical_space, method= method)\n",
    "        # else:\n",
    "        #     return attitude_constructor_from_points_and_attitude_features(points, attitude_features, physical_space= physical_space, method= method)    \n",
    "    \n",
    "def create_surface_part(knowledge_framework, surface_representation, explained_features = None,  name=None, **kargs):\n",
    "\n",
    "    # Instanciate a Stratigraphic_Part pointing to the planarsurface representation\n",
    "    class_type = knowledge_framework().Stratigraphic_Part\n",
    "    surface_part = knowledge_framework.create_instance(class_type, \n",
    "                                                       name, has_Representation= surface_representation, **kargs)\n",
    "    \n",
    "    # explanation relationship\n",
    "    if explained_features is not None : \n",
    "        surface_part.explain = explained_features\n",
    "    surface_part.is_Explained_By = []\n",
    "    return surface_part\n",
    "     \n",
    "def constructor_surface_part_from_interpretation( interpretation_situation, knowledge_framework= None,\n",
    "                                                 physical_space= None, name= None, **kargs):\n",
    "    \"\"\"Constructor of observations\n",
    "    \n",
    "    Parameters:\n",
    "    - knowledge_framework: the knowledge framework in which this object must be created\n",
    "    - interpretation_situation: the interpretation situation selected in the given state of interpretation process\n",
    "    - physical_space: the physical representation space where the surface is to be defined, if None the one from the interpretation_situation.process is used\n",
    "        \n",
    "    Return:\n",
    "    - the created Surface Part\n",
    "    \"\"\"\n",
    "    knowledge_framework = knowledge_framework if knowledge_framework is not None else interpretation_situation.knowledge_framework\n",
    "    physical_space = get_physical_space_in_constructor(interpretation_situation, physical_space, **kargs)\n",
    "    \n",
    "    constructed_class = knowledge_framework().Stratigraphic_Part\n",
    "    explainable_features = knowledge_framework.filter_explainable_features(constructed_class, interpretation_situation.features)\n",
    "    if len(explainable_features) == 0:\n",
    "        print(\"Warning: No explainable feature in the situation with this interpretation. Surface part can not be built. To be implemented.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    #estimate location\n",
    "    location_qualities = location_constructor(features= explainable_features, physical_space= physical_space, method= \"average\", **kargs)\n",
    "    \n",
    "    # estimate attitude\n",
    "    attitude_qualities = attitude_constructor(features= explainable_features, physical_space= physical_space,\n",
    "                                            knowledge_framework= knowledge_framework, method= \"average\", **kargs)\n",
    "    # Instanciate a PlanarSurface\n",
    "    name_rep = None if name is None else name+\"_rep\"\n",
    "    surface_representation = constructor_planar_surface_from_center_attitude(\n",
    "                                    knowledge_framework= knowledge_framework,\n",
    "                                    coord_labels= physical_space.coordinate_labels,\n",
    "                                    center= location_qualities,\n",
    "                                    dip= attitude_qualities[\"dip\"],\n",
    "                                    dip_dir= attitude_qualities[\"dip_dir\"],\n",
    "                                    polarity= attitude_qualities[\"polarity\"],\n",
    "                                    size= attitude_qualities[\"size\"], \n",
    "                                    name= name_rep\n",
    "                                    )\n",
    "    return create_surface_part(knowledge_framework, surface_representation, explainable_features,  name, **kargs)\n",
    "\n",
    "\n",
    "def constructor_surface_part_from_interpretation_of_parts( interpretation_situation, knowledge_framework= None,\n",
    "                                                 physical_space= None, name= None, **kargs):\n",
    "    \"\"\"Constructor of observations\n",
    "    \n",
    "    Parameters:\n",
    "    - knowledge_framework: the knowledge framework in which this object must be created\n",
    "    - interpretation_situation: the interpretation situation selected in the given state of interpretation process\n",
    "    - physical_space: the physical representation space where the surface is to be defined, if None the one from the interpretation_situation.process is used\n",
    "        \n",
    "    Return:\n",
    "    - the created Surface Part\n",
    "    \"\"\"\n",
    "    knowledge_framework = knowledge_framework if knowledge_framework is not None else \\\n",
    "                                                         interpretation_situation.knowledge_framework\n",
    "    physical_space = get_physical_space_in_constructor(interpretation_situation, physical_space, **kargs)\n",
    "    \n",
    "    used_feature_class = knowledge_framework().Stratigraphic_Part\n",
    "    explainable_features_strati = knowledge_framework.filter_explainable_features_by_type( interpretation_situation.features, \n",
    "                                                                                   type_of_feature = used_feature_class)\n",
    "    if len(explainable_features_strati) < 2:\n",
    "        print(\"Warning: No explainable feature in the situation with this interpretation. Thus Surface can not be constructed. To be implemented. \")\n",
    "        return None\n",
    "    selected_pair_of_features = random.sample(explainable_features_strati, 2)\n",
    "    name_rep = None if name is None else name+\"_rep\"\n",
    "    return construct_complex_stratigraphic_part(knowledge_framework, selected_pair_of_features, physical_space,\n",
    "                                         name_object = name, name_rep = name_rep)\n",
    "    \n",
    "def constructor_surface_part_from_nodes(knowledge_framework, nodes, name= None, **kargs):\n",
    "    name_rep = None if name is None else name+\"_rep\"\n",
    "    plan = constructor_planar_surface_from_nodes(\n",
    "        knowledge_framework= knowledge_framework, nodes= nodes, name= name_rep\n",
    "    )\n",
    "    return create_surface_part(\n",
    "        knowledge_framework= knowledge_framework, surface_representation= plan, explained_features=[],  name= name, **kargs\n",
    "    )\n",
    "    \n",
    "def constructor_surface_part_from_coords(knowledge_framework, coords, coord_labels, name= None, **kargs):\n",
    "    name_rep = None if name is None else name+\"_rep\"\n",
    "    plan = constructor_planar_surface_from_coords(\n",
    "        knowledge_framework= knowledge_framework, coords= coords, coord_labels= coord_labels, name= name_rep\n",
    "    )\n",
    "    return create_surface_part(\n",
    "        knowledge_framework=knowledge_framework, surface_representation= plan, explained_features=[],  name= name, **kargs\n",
    "    )\n",
    "    \n",
    "def constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework, coord_labels, center, dip, dip_dir, size= None, polarity= True, name= None, **kargs\n",
    "):\n",
    "    name_rep = None if name is None else name+\"_rep\"\n",
    "    plan = constructor_planar_surface_from_center_attitude(\n",
    "        knowledge_framework, coord_labels, center, dip, dip_dir, size= size, polarity= polarity, name= name_rep\n",
    "    )\n",
    "    return create_surface_part(\n",
    "        knowledge_framework=knowledge_framework, surface_representation= plan, explained_features=[],  name= name, **kargs\n",
    "    )\n",
    "\n",
    "\n",
    "if \"Stratigraphic_Part\" in GeologicalKnowledgeFramework.registered_constructors:\n",
    "    del GeologicalKnowledgeFramework.registered_constructors[\"Stratigraphic_Part\"]\n",
    "\n",
    "GeologicalKnowledgeFramework.register_constructor(\"Stratigraphic_Part\", \n",
    "                                                  constructor= constructor_surface_part_from_interpretation_of_parts,\n",
    "                                                  condition= conditions_for_constructor_surface_part_from_interpretation_of_parts)\n",
    "\n",
    "\n",
    "GeologicalKnowledgeFramework.register_constructor(\"Stratigraphic_Part\", \n",
    "                                                  constructor= constructor_surface_part_from_interpretation,\n",
    "                                                  condition= conditions_for_constructor_surface_part_from_interpretation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratigraphic part anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal consistency anomaly\n",
    "\n",
    "def missing_representation_anomaly_constructor(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                       object,\n",
    "                                    ):\n",
    "    \"\"\"Constructor method for creating an instance of RepresentationAnomaly\n",
    "    \n",
    "    This is an anomaly for any object with no representation.\n",
    "    \n",
    "    Parameters:\n",
    "    - object: the object with no representation\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for RepresentationAnomaly : knowledge framework\")\n",
    "    if (object is None) :\n",
    "        raise MalissiaBaseError(\"Wrong condition for RepresentationAnomaly : object is None\")\n",
    "    if (isinstance(object, list)) :\n",
    "        raise MalissiaBaseError(\"Wrong condition for RepresentationAnomaly : object is given as list\")\n",
    "    \n",
    "    anomaly = knowledge_framework().MissingRepresentationAnomaly(is_Related_To = [object])\n",
    "    return anomaly\n",
    "\n",
    "\n",
    "def representation_anomaly_constructor(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                       object,\n",
    "                                       representation\n",
    "                                    ):\n",
    "    \"\"\"Constructor method for creating an instance of RepresentationAnomaly\n",
    "    \n",
    "    This is an anomaly for any object with bad representation, e.g., parameter not defined of with wrong type.\n",
    "    \n",
    "    Parameters:\n",
    "    - object: the object with a bad representation\n",
    "    - representation: the problematic representation (objects may have several representations)\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for RepresentationAnomaly : knowledge framework\")\n",
    "    if (object is None) or (representation is None):\n",
    "        raise MalissiaBaseError(\"Wrong condition for RepresentationAnomaly : parts is None\")\n",
    "    if (isinstance(object, list)) or (isinstance(representation, list)):\n",
    "        raise MalissiaBaseError(\"Wrong condition for RepresentationAnomaly : parts are given as lists\")\n",
    "    \n",
    "    anomaly = knowledge_framework().RepresentationAnomaly(\n",
    "            is_Related_To = [object],\n",
    "            is_Related_To_Representation = [representation]\n",
    "        )\n",
    "    return anomaly\n",
    "    \n",
    "def dipping_stratigraphy_anomaly_constructor(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            dipping_object):\n",
    "    \"\"\" Constructor method for creating an instance of DippingStratigraphyAnomaly\n",
    "    \n",
    "    This is an anomaly for a StratigraphicPart with a planar geometry having a dip above a threshold,\n",
    "    which represents the maximum dip a sedimentary surface could have without being non_planar.\n",
    "    \n",
    "    Parameters:\n",
    "    - dipping_object: the object being anomalously dipping\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for DippingStratigraphyAnomaly_constructor : knowledge framework\")\n",
    "    if dipping_object is None:\n",
    "        raise MalissiaBaseError(\"Wrong condition for DippingStratigraphyAnomaly_constructor : dipping_object is None\")\n",
    "    if isinstance(dipping_object, list):\n",
    "        if len(dipping_object) == 1:\n",
    "            dipping_object = dipping_object[0]\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"Wrong condition for DippingStratigraphyAnomaly_constructor : multiple objects given, only one should be given\")\n",
    "            \n",
    "    anomaly = knowledge_framework().DippingStratigraphyAnomaly( is_Related_To = [dipping_object])\n",
    "    return anomaly\n",
    "\n",
    "# relational anomalies\n",
    "def dip_variation_anomaly_constructor(\n",
    "            knowledge_framework:GeologicalKnowledgeFramework, \n",
    "            parts:list\n",
    "        ):\n",
    "    \"\"\"Constructor method for creating an instance of DipVariationAnomaly\n",
    "    \n",
    "    This is an anomaly for a StratigraphicPart with a ComplexSurface geometry \n",
    "    when two successive PlanarSurfaces have a variation of orientation.\n",
    "    \n",
    "    Parameters:\n",
    "    - parts: a list containing exactly two surface parts identified as discontinuous\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for DipVariationAnomaly : knowledge framework\")\n",
    "    if parts is None:\n",
    "        raise MalissiaBaseError(\"Wrong condition for DipVariationAnomaly : parts is None\")\n",
    "    if (not isinstance(parts, list)) or (len(parts) != 2):\n",
    "        raise MalissiaBaseError(\"Wrong condition for DipVariationAnomaly : parts is not a list of length 2\")\n",
    "    \n",
    "    anomaly = knowledge_framework().DipVariationAnomaly(is_Related_To = parts)\n",
    "    return anomaly\n",
    "\n",
    "def discontinuous_stratigraphy_anomaly_constructor(\n",
    "            knowledge_framework:GeologicalKnowledgeFramework,\n",
    "            parts\n",
    "        ):\n",
    "    \"\"\"Constructor method for creating an instance of DiscontinuousStratigraphyAnomaly\n",
    "    \n",
    "    This anomaly is for StratigraphicParts with a ComplexSurface representation\n",
    "    when two successive PlanarSurfaces do not intersect properly suggesting a discontinuity\n",
    "    \n",
    "    Parameters:\n",
    "    - parts: a list containing exactly two surface parts identified as discontinuous\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for DiscontinuousStratigraphyAnomaly : knowledge framework\")\n",
    "    if parts is None:\n",
    "        raise MalissiaBaseError(\"Wrong condition for DiscontinuousStratigraphyAnomaly : parts is None\")\n",
    "    if (not isinstance(parts, list)) or (len(parts) != 2):\n",
    "        raise MalissiaBaseError(\"Wrong condition for DiscontinuousStratigraphyAnomaly : parts is not a list of length 2\")\n",
    "    \n",
    "    anomaly = knowledge_framework().DiscontinuousStratigraphyAnomaly(is_Related_To = parts)\n",
    "    return anomaly\n",
    "\n",
    "def polarity_anomaly_constructor(\n",
    "        knowledge_framework:GeologicalKnowledgeFramework,\n",
    "        parts:list\n",
    "    ):\n",
    "    \"\"\"Constructor method for creating an instance of PolarityAnomaly\n",
    "    \n",
    "    This should occur when two neighbour or supposedly connected surfaces have opposite polarities\n",
    "    or when a fold has two limbs with incompatible polarities.\n",
    "    \n",
    "    Parameters:\n",
    "    - parts: a list containing the two surface parts or limbs related to this anomaly\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for PolarityAnomaly : knowledge framework\")\n",
    "    if parts is None:\n",
    "        raise MalissiaBaseError(\"Wrong condition for PolarityAnomaly : parts is None\")\n",
    "    if (not isinstance(parts, list)) or (len(parts) != 2):\n",
    "        raise MalissiaBaseError(\"Wrong condition for PolarityAnomaly : parts is not a list of length 2\")\n",
    "    \n",
    "    anomaly = knowledge_framework().PolarityAnomaly(is_Related_To = parts)\n",
    "    return anomaly\n",
    "\n",
    "#explanation anomalies\n",
    "def explanation_anomaly_constructor(\n",
    "                                    knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                    explaining_object, \n",
    "                                    explained_object\n",
    "                                ):\n",
    "    \"\"\"Constructor method for creating an instance of ExplanationAnomaly\n",
    "    \n",
    "    For when an explained object geometry doesn't match the explaining object geometry\n",
    "    (distance between surface too big).\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for ExplanationAnomaly : knowledge framework\")\n",
    "        return False\n",
    "    if (explaining_object is None) or (explained_object is None):\n",
    "        raise MalissiaBaseError(\"Wrong condition for StratigraphyBorderAnomaly : at least one of the objects is None\")\n",
    "    if (isinstance(explaining_object, list)) or (isinstance(explained_object, list)):\n",
    "        raise MalissiaBaseError(\"Wrong condition for StratigraphyBorderAnomaly : at least one of the objects is a list\")\n",
    "    \n",
    "    anomaly = knowledge_framework().ExplanationAnomaly( \n",
    "                                            is_Related_To_Explaining_Object = [explaining_object],\n",
    "                                            is_Related_To_Explained_Object = [explained_object]\n",
    "                                        )\n",
    "    return anomaly\n",
    "\n",
    "\n",
    "\n",
    "# complex surfaces anomalies\n",
    "def complex_surface_planar_anomaly_constructor(\n",
    "            knowledge_framework:GeologicalKnowledgeFramework, \n",
    "            parts:list\n",
    "        ):\n",
    "    \"\"\"Constructor method for creating an instance of ComplexSurfacePlanarAnomaly\n",
    "    \n",
    "    This is an anomaly for a StratigraphicPart 'representation' with a planar ComplexSurface geometry \n",
    "    and the two parts of it are planar, coplanar, and parallel. It could be upgraded to include more then 2 parts\n",
    "\n",
    "    Parameters:\n",
    "    - parts: a list containing exactly two surface parts identified\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for ComplexSurfacePlanarAnomaly : knowledge framework\")\n",
    "    if parts is None:\n",
    "        raise MalissiaBaseError(\"Wrong condition for ComplexSurfacePlanarAnomaly : parts is None\")\n",
    "    if (not isinstance(parts, list)) or (len(parts) != 2):\n",
    "        raise MalissiaBaseError(\"Wrong condition for ComplexSurfacePlanarAnomaly : parts is not a list of length 2\")\n",
    "    \n",
    "    anomaly = knowledge_framework().ComplexSurfacePlanarAnomaly(is_Related_To = parts)\n",
    "    return anomaly\n",
    "\n",
    "def complex_surface_shared_sides_size_anomaly_constructor(\n",
    "            knowledge_framework:GeologicalKnowledgeFramework, \n",
    "            complex_surface\n",
    "        ):\n",
    "    \"\"\"Constructor method for creating an instance of ComplexSurfaceSizeSharedSidesAnomaly\n",
    "    \n",
    "    This is an anomaly for a StratigraphicPart 'representation' with a non planar ComplexSurface geometry \n",
    "    where the shared sides between their parts are not equal\n",
    "\n",
    "    Parameters:\n",
    "    - complex_surface: the anomalic complex_surface\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for ComplexSurfaceSizeSharedSidesAnomaly : knowledge framework\")\n",
    "\n",
    "    if (not isinstance(complex_surface, knowledge_framework().Complex_Surface)):\n",
    "        raise MalissiaBaseError(\"Wrong condition for ComplexSurfaceSizeSharedSidesAnomaly : complex_surface is not an instance of Complex_Surface\")\n",
    "    \n",
    "    if len(complex_surface.has_Part) < 2 :\n",
    "        raise MalissiaBaseError(\"Wrong condition for ComplexSurfaceSizeSharedSidesAnomaly : complex_surface has less then two parts\")\n",
    "    \n",
    "    anomaly = knowledge_framework().ComplexSurfaceSizeSharedSidesAnomaly(is_Related_To = [complex_surface])\n",
    "\n",
    "    return anomaly\n",
    "\n",
    "\n",
    "\n",
    "def complex_surface_neighbor_number_anomaly(\n",
    "            knowledge_framework:GeologicalKnowledgeFramework, \n",
    "            complex_surface\n",
    "        ):\n",
    "    \"\"\"Constructor method for creating an instance of ComplexSurfaceNeighborNumberAnomaly\n",
    "    \n",
    "    This is an anomaly for a StratigraphicPart 'representation' with a non planar ComplexSurface geometry \n",
    "    and the actual count of neighors part is not equal to the expected number (nb_parts - 1)*2. \n",
    "    \n",
    "    Parameters:\n",
    "    - complex_surface: the anomalic complexs_surface\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for ComplexSurfacePlanarAnomaly : knowledge framework\")\n",
    "    if complex_surface is None:\n",
    "        raise MalissiaBaseError(\"Wrong condition for ComplexSurfacePlanarAnomaly : complex_surface is None\")\n",
    "\n",
    "    \n",
    "    anomaly = knowledge_framework().ComplexSurfaceNeighborNumberAnomaly(is_Related_To = [complex_surface])\n",
    "    return anomaly\n",
    "\n",
    "def shared_side_not_parallel_anomaly(\n",
    "            knowledge_framework:GeologicalKnowledgeFramework, \n",
    "            complex_surface\n",
    "        ):\n",
    "    \"\"\"Constructor method for creating an instance of SharedSideNotParallelAnomaly\n",
    "    \n",
    "    This is an anomaly for a StratigraphicPart 'representation' with a planar ComplexSurface geometry \n",
    "    and the shared sides between the different parts are not parallel.\n",
    "\n",
    "    Parameters:\n",
    "    - parts: a list containing exactly two surface parts identified\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for ComplexSurfacePlanarAnomaly : knowledge framework\")\n",
    "    if complex_surface is None:\n",
    "        raise MalissiaBaseError(\"Wrong condition for ComplexSurfacePlanarAnomaly : complex_surface is None\")\n",
    "    \n",
    "    anomaly = knowledge_framework().SharedSideNotParallelAnomaly(is_Related_To = [complex_surface])\n",
    "    return anomaly\n",
    "\n",
    "# deformation related anomaly\n",
    "\n",
    "def internal_dip_variation_anomaly_two_parts_constructor(knowledge_framework, stratigraphic_part):\n",
    "    \n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for InternalDipVariationAnomalyTwoParts : knowledge framework\")\n",
    "    if stratigraphic_part is None:\n",
    "        raise MalissiaBaseError(\"Wrong condition for InternalDipVariationAnomalyTwoParts : stratigraphic_part is None\")\n",
    "    \n",
    "    anomaly = knowledge_framework().InternalDipVariationAnomalyTwoParts(is_Related_To = [stratigraphic_part])\n",
    "    return anomaly\n",
    "\n",
    "def internal_dip_variation_anomaly_multi_parts_constructor(knowledge_framework, stratigraphic_part):\n",
    "    \n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for InternalDipVariationAnomalyMultiParts : knowledge framework\")\n",
    "    if stratigraphic_part is None:\n",
    "        raise MalissiaBaseError(\"Wrong condition for InternalDipVariationAnomalyMultiParts : stratigraphic_part is None\")\n",
    "    \n",
    "    anomaly = knowledge_framework().InternalDipVariationAnomalyMultiParts(is_Related_To = [stratigraphic_part])\n",
    "    return anomaly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_to_surface_part(part, node, physical_space):\n",
    "    # todo : compute distance for different nodes and aggregate results depeding on the type of the object \n",
    "    node_coord = np.array(physical_space.get_object_coordinates(node, kind=\"node\")[0])\n",
    "    center_coord = np.array(physical_space.get_object_coordinates(part, kind=\"center\"))\n",
    "    normal = np.array(physical_space.get_object_coordinates(part, kind=\"normal\"))\n",
    "    normal /= np.linalg.norm(normal)\n",
    "    vec = node_coord - center_coord\n",
    "    return float(np.dot(vec, normal))\n",
    "\n",
    "def compute_surface_part_confidence_interval(part, physical_space, std_perc_size= 0.025):\n",
    "    size = physical_space.get_object_coordinates(part, kind=\"size\")\n",
    "    return 2 * std_perc_size * size\n",
    "\n",
    "def evaluate_stratigraphic_part_explanation_consistency(part, knowledge_framework, physical_space, std_perc_size= 0.025, **kargs):\n",
    "    # todo :  generate coverage anomalies (for points projecting outside of the explaining object)\n",
    "\n",
    "    # get representation\n",
    "    rep = part.has_Representation\n",
    "    if (rep is None) or (len(rep) == 0):\n",
    "        return [ missing_representation_anomaly_constructor(\n",
    "            knowledge_framework= knowledge_framework,\n",
    "            object= part)]\n",
    "    \n",
    "    # case of a Planar Stratigraphic Part\n",
    "    if (len(rep) == 1) and (isinstance(rep[0], knowledge_framework().Planar_Surface)):\n",
    "\n",
    "        explained_objects = part.explain\n",
    "        # Todo:  filter explained object and keep only those with a physical representation\n",
    "        distances = [compute_distance_to_surface_part(part, obj_i, physical_space= physical_space) for obj_i in explained_objects]\n",
    "        confidence_interval = compute_surface_part_confidence_interval(part, physical_space, std_perc_size= std_perc_size)\n",
    "        \n",
    "        # generate anomalies\n",
    "        anomalies = [explanation_anomaly_constructor(knowledge_framework, part, obj_i) \n",
    "                    for (obj_i, dist_i) in zip(explained_objects,distances)\n",
    "                    if np.abs(dist_i) > confidence_interval ]\n",
    "        return anomalies\n",
    "    \n",
    "    # small fix when part is a complex_part\n",
    "    elif (len(rep) == 1) and (isinstance(rep[0], knowledge_framework().Complex_Surface)):\n",
    "        return []\n",
    "    \n",
    "    else:\n",
    "        raise MalissiaNotImplementedYet(\"Multiple representations while evaluating consistency\")\n",
    "    \n",
    "\n",
    "def evaluate_statigraphic_part_internal_consistency(part, knowledge_framework, physical_space, max_dip= 4, **kargs):\n",
    "    \n",
    "    # get representation\n",
    "    rep = part.has_Representation\n",
    "    if (rep is None) or (len(rep) == 0):\n",
    "        return [missing_representation_anomaly_constructor(\n",
    "            knowledge_framework= knowledge_framework,\n",
    "            object= part)]\n",
    "    \n",
    "    # case of a Planar Stratigraphic Part\n",
    "    if (len(rep) == 1) and (isinstance(rep[0], knowledge_framework().Planar_Surface)):\n",
    "        return evaluate_planar_statigraphic_part_internal_consistency(\n",
    "            part= part,\n",
    "            representation= rep[0],\n",
    "            knowledge_framework= knowledge_framework, physical_space = physical_space,\n",
    "            max_dip= max_dip,\n",
    "            **kargs\n",
    "        )\n",
    "    # case of a Complex Stratigraphic Part\n",
    "    elif (len(rep) == 1) and (isinstance(rep[0], knowledge_framework().Complex_Surface)):\n",
    "        return evaluate_complex_statigraphic_part_internal_consistency(\n",
    "            representation= rep[0],\n",
    "            knowledge_framework= knowledge_framework, physical_space = physical_space,\n",
    "            **kargs\n",
    "        )\n",
    "    else:\n",
    "        raise MalissiaNotImplementedYet(\"Multiple representations while evaluating consistency\")\n",
    "\n",
    "def evaluate_planar_statigraphic_part_internal_consistency(part, representation, \n",
    "                                                           knowledge_framework, physical_space, max_dip= 4, **kargs):\n",
    "    \n",
    "    # evaluation of dip consistency\n",
    "    dip = representation.dip\n",
    "    if (dip is None) or (len(dip) == 0):\n",
    "        return [representation_anomaly_constructor(\n",
    "            knowledge_framework= knowledge_framework,\n",
    "            object= part,\n",
    "            representation= representation\n",
    "        )]\n",
    "        \n",
    "    # case of out of range dip\n",
    "    elif (dip[0] < 0) or (dip[0] > 90):\n",
    "        return [representation_anomaly_constructor(\n",
    "            knowledge_framework= knowledge_framework,\n",
    "            object= part,\n",
    "            representation= representation\n",
    "        )]\n",
    "        \n",
    "    # case of stratigraphy dipping too much\n",
    "    elif dip[0] > max_dip:\n",
    "        return [dipping_stratigraphy_anomaly_constructor(\n",
    "                    knowledge_framework= knowledge_framework,\n",
    "                    dipping_object= part\n",
    "            )]\n",
    "    else:\n",
    "        # no anomaly\n",
    "        return []\n",
    "    \n",
    "def evaluate_complex_statigraphic_part_internal_consistency(representation, knowledge_framework, physical_space,\n",
    "                                                             **kargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Method to check the geometry of the complex surface. It returns True if all conditions are met, \n",
    "        or raise an anomaly depedning on the type of the situation detected. \n",
    "\n",
    "    Parameter:\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    \"\"\" \n",
    "    complex_surface = representation\n",
    "\n",
    "    class_stratigraphic_part = knowledge_framework().Stratigraphic_Part\n",
    "    stratigraphic_parts = [obj for obj in complex_surface.is_Representation_Of if class_stratigraphic_part in obj.is_a]\n",
    "    if len(stratigraphic_parts) != 1 :\n",
    "        raise MalissiaBaseError('the complex_surface is representation of {} stratigraphic_part'.format(len(stratigraphic_parts)))\n",
    "\n",
    "    stratigraphic_part = stratigraphic_parts[0]\n",
    "\n",
    "    # if complex surface has two parts and parallel ==> this surface must be transformed to a simple planar surface\n",
    "    if complex_surface.non_planar != False and complex_surface.non_planar != True:\n",
    "        raise MalissiaBaseError('the complex_surface non_planar_property must be given \"True\" or \"False\"')\n",
    "    if len(complex_surface.has_Part) <2 :\n",
    "        raise MalissiaBaseError('the complex_surface must have at least two parts')\n",
    "    if len(complex_surface.has_Part) >2 and complex_surface.non_planar == False:\n",
    "        raise MalissiaNotImplementedYet('the actual algorithm handle only a non deformed complex_surface of exactly two parts') \n",
    "    \n",
    "    if len(complex_surface.has_Part) ==2 and complex_surface.non_planar == False:\n",
    "        # raise anomaly that indicates that this complex_surface will be transformed to a planar\n",
    "        return [complex_surface_planar_anomaly_constructor(knowledge_framework,complex_surface.has_Part)] \n",
    "    \n",
    "    if len(complex_surface.has_Part) >2 and complex_surface.non_planar == True:\n",
    "    # calcualate the common nodes and shared sides\n",
    "        # check if all shared sides are parallel\n",
    "        if not shared_sides_are_parallel(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            #he geometry of the complex surface is not conforme because shared sides are not parallel ==> undo last interpetation\n",
    "            return [shared_side_not_parallel_anomaly(knowledge_framework, complex_surface)] \n",
    "        \n",
    "        # check if all shared sides in the complex_surface has the same length in the correct allignement\n",
    "        if not shared_sides_are_equal(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            # raise anomaly that indicates that this complex_surface must be extended ==> look at unmatched neighbor\n",
    "            return [complex_surface_shared_sides_size_anomaly_constructor(knowledge_framework, complex_surface)]\n",
    "        \n",
    "        if not shared_sides_have_uniform_allignement(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            # raise anomaly that indicates that this complex_surface must be extended ==> look at unmatched neighbor\n",
    "            return [complex_surface_shared_sides_size_anomaly_constructor(knowledge_framework, complex_surface)]\n",
    "        \n",
    "        #check neighbors number and arrangement in the complex_surface\n",
    "        if not check_complex_surface_neighbor_number_and_arrangement(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            # the geometry of the complex surface is not conforme because the number of neighbors is not adequate with\n",
    "                                                            ## the number of parts (n-1)*2 ==> undo last interpetation\n",
    "            return [complex_surface_neighbor_number_anomaly(knowledge_framework, complex_surface)]\n",
    "         \n",
    "        # check chirality between all neighbors\n",
    "        for pair in extract_neighbors_in_complex_surface(complex_surface) :\n",
    "               if not check_chirality_projection(*pair, knowledge_framework, physical_space ):\n",
    "                   return [polarity_anomaly_constructor(knowledge_framework, parts = pair)]\n",
    "        class_folding_train = knowledge_framework().Fold_Train\n",
    "        if not any([class_folding_train in rep.is_a for rep in complex_surface.is_Representation_Of ]) :\n",
    "            return [internal_dip_variation_anomaly_multi_parts_constructor(knowledge_framework, stratigraphic_part)]\n",
    "       \n",
    "    class_fold = knowledge_framework().Fold\n",
    "    if len(complex_surface.has_Part) == 2 and complex_surface.non_planar == True and \\\n",
    "        not any([class_fold in rep.is_a for rep in complex_surface.is_Representation_Of ]) :\n",
    "        return [internal_dip_variation_anomaly_two_parts_constructor(knowledge_framework, stratigraphic_part)]\n",
    "\n",
    "    # if all checks come verified the function will return a an empty list \n",
    "    return []\n",
    "\n",
    "\n",
    "GeologicalKnowledgeFramework.register_internal_consistency_evaluator(\"Stratigraphic_Part\", \n",
    "                                                  method= evaluate_statigraphic_part_internal_consistency)\n",
    "GeologicalKnowledgeFramework.register_explanation_consistency_evaluator(\"Stratigraphic_Part\", \n",
    "                                                  method= evaluate_stratigraphic_part_explanation_consistency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratigraphic Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditions_for_constructor_stratigraphic_surface(\n",
    "                                            knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            interpretation_situation:InterpretationSituation, **kargs):\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for Surface Part constructor: knowledge framework\")\n",
    "        return False\n",
    "    if (interpretation_situation is None):\n",
    "        print(\"Wrong condition for Surface Part constructor: interpretation situation\")\n",
    "        return False\n",
    "    if  (interpretation_situation.process is None) or (interpretation_situation.process.representation_space is None):\n",
    "        print(\"Wrong condition for Surface Part constructor: physical space\")\n",
    "        return False\n",
    "    if  (interpretation_situation.features is None) or (len(interpretation_situation.features) == 0):\n",
    "        print(\"Wrong condition for Surface Part constructor: features\")\n",
    "        return False\n",
    "    \n",
    "    constructed_class = knowledge_framework().Stratigraphic_Surface\n",
    "    explainable_features = knowledge_framework.filter_explainable_features(constructed_class, interpretation_situation.features)\n",
    "    if len(explainable_features) == 0:\n",
    "        print(\"Wrong condition for Surface Part constructor: No explainable feature in the situation with this interpretation. To be implemented.\")\n",
    "        return False\n",
    "    # in any other cases\n",
    "    return True\n",
    "\n",
    "def constructor_stratigraphic_surface_from_interpretation( interpretation_situation, \n",
    "                                            knowledge_framework= None,\n",
    "                                            physical_space= None, name= None, **kargs):\n",
    "    \"\"\"Constructor of observations\n",
    "    \n",
    "    Parameters:\n",
    "    - knowledge_framework: the knowledge framework in which this object must be created\n",
    "    - interpretation_situation: the interpretation situation selected in the given state of interpretation process\n",
    "    - physical_space: the physical representation space where the surface is to be defined, if None the one from the interpretation_situation.process is used\n",
    "        \n",
    "    Return:\n",
    "    - the created Stratigraphic Surface \n",
    "    \"\"\"\n",
    "    knowledge_framework = knowledge_framework if knowledge_framework is not None else interpretation_situation.knowledge_framework\n",
    "    physical_space = get_physical_space_in_constructor(interpretation_situation, physical_space, **kargs)\n",
    "    \n",
    "    constructed_class = knowledge_framework().Stratigraphic_Surface\n",
    "    explainable_features = knowledge_framework.filter_explainable_features(constructed_class,\n",
    "                                                             interpretation_situation.features)\n",
    "    if len(explainable_features) == 0:\n",
    "        print(\"Warning: No explainable feature in the situation with this interpretation. Thus, stratigraphic surface could not be constructed. To be implemented.\".format(explainable_features))\n",
    "        return None\n",
    "    # possible indications of which surface from interpretation situation \n",
    "    ## for ex : stratigraphic order, location of surfaces ...etc to be implemented\n",
    "    strati_surf = stratigraphic_surface_constructor(stratigraphic_surface_name = None, \n",
    "                                                    knowledge_framework = knowledge_framework)\n",
    "    strati_surf.explain.extend(explainable_features)\n",
    "\n",
    "    return strati_surf\n",
    "\n",
    "\n",
    "def stratigraphic_surface_constructor(stratigraphic_surface_name, knowledge_framework, indications = None, age = None, \n",
    "                                      force_creation_of_new_stratigraphic_surface = False):\n",
    "\n",
    "    stratigraphic_surface_name = 'Permian' if stratigraphic_surface_name is None and indications is None else stratigraphic_surface_name\n",
    "\n",
    "    possible_existing_instances = list(set(knowledge_framework.search(\n",
    "        name= stratigraphic_surface_name)).intersection(set(knowledge_framework().Stratigraphic_Surface.instances())))\n",
    "    if len(possible_existing_instances) == 0 :\n",
    "        return knowledge_framework().Stratigraphic_Surface(name = stratigraphic_surface_name)\n",
    "    elif len(possible_existing_instances) == 1 :\n",
    "        if force_creation_of_new_stratigraphic_surface != True :\n",
    "            print('Object {} already, exists. It will be returned'.format(possible_existing_instances[0]))\n",
    "            return possible_existing_instances[0]\n",
    "        if force_creation_of_new_stratigraphic_surface == True :\n",
    "            raise MalissiaNotImplementedYet('method to be implemented to create multiple stratigraphic surfaces with the same age.')\n",
    "            #knowledge_framework().Stratigraphic_Surface.instances(name = stratigraphic_surface_name)\n",
    "            pass\n",
    "    else:\n",
    "        print('Warning: multiple instnaces of {} with the name {} exist [{}], returning [0]'.format(knowledge_framework().Stratigraphic_Surface, \n",
    "                                                                                     stratigraphic_surface_name,  possible_existing_instances))\n",
    "        return possible_existing_instances[0]\n",
    "\n",
    "if \"Stratigraphic_Surface\" in GeologicalKnowledgeFramework.registered_constructors:\n",
    "    del GeologicalKnowledgeFramework.registered_constructors[\"Stratigraphic_Surface\"]\n",
    "\n",
    "GeologicalKnowledgeFramework.register_constructor(\"Stratigraphic_Surface\", \n",
    "                    constructor= constructor_stratigraphic_surface_from_interpretation, \n",
    "                    condition= conditions_for_constructor_stratigraphic_surface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal anomalies\n",
    "def stratigraphy_border_anomaly_constructor(\n",
    "                                    knowledge_framework:GeologicalKnowledgeFramework, \n",
    "                                    stratigraphic_surface,\n",
    "                                    part,\n",
    "                                    border\n",
    "                                    ):\n",
    "    \"\"\"Constructor method for creating an instance of StratigraphyBorderAnomaly\n",
    "    \n",
    "    This anomaly is for a stratigraphic surface when its geometry doesn't reach the border of the domain\n",
    "    \n",
    "    Parameters:\n",
    "    - stratigraphic_surface: the stratigraphic_surface bearing the anomaly\n",
    "    - part: the surface part being affected by the anomaly (the one that should connect to the border)\n",
    "    - border: the border that the part should be connected to\n",
    "    \"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for StratigraphyBorderAnomaly : knowledge framework\")\n",
    "    if (stratigraphic_surface is None) or (part is None) or (border is None):\n",
    "        raise MalissiaBaseError(\"Wrong condition for StratigraphyBorderAnomaly : parts is None\")\n",
    "    if (isinstance(stratigraphic_surface, list)) or (isinstance(part, list)) or (isinstance(border, list)):\n",
    "        raise MalissiaBaseError(\"Wrong condition for StratigraphyBorderAnomaly : parts are given as lists\")\n",
    "    \n",
    "    anomaly = knowledge_framework().StratigraphyBorderAnomaly(is_Related_To = [stratigraphic_surface],\n",
    "                                                              is_Related_To_Part = [part],\n",
    "                                                              is_Related_To_Border = [border])\n",
    "    return anomaly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "def compute_direction_toward_hinge_along_limb_1(limb_normal, fold_axis):\n",
    "    \"\"\"\n",
    "    Method to compute the direction toward the hinge along limb1. It returns an np.array([x,y,z]) vector.\n",
    "    Parameter:\n",
    "    - limb_normal:  limb1_normal\n",
    "    - fold_axis: fold_axis\n",
    "    \"\"\" \n",
    "    \n",
    "    along_limb_vec = list(map(float,-1 * np.cross(limb_normal,fold_axis)))\n",
    "    return along_limb_vec / np.linalg.norm(along_limb_vec)\n",
    "\n",
    "def compute_direction_toward_hinge_along_limb_2(limb_normal, fold_axis):\n",
    "    \"\"\"\n",
    "    Method to compute the direction toward the hinge along limb2. It returns an np.array([x,y,z]) vector.\n",
    "    Parameter:\n",
    "    - limb_normal:  limb2_normal\n",
    "    - fold_axis: fold_axis\n",
    "    \"\"\" \n",
    "    along_limb_vec = list(map(float, np.cross(limb_normal,fold_axis)))\n",
    "\n",
    "    return along_limb_vec / np.linalg.norm(along_limb_vec)\n",
    "\n",
    "def compute_s_vector(n1, c1 , n2 , c2):\n",
    "    \"\"\"\n",
    "    Method to compute the s vector that points from the first limb toward the second limb. It returns an np.array([x,y,z]) vector.\n",
    "    Parameter:\n",
    "    - n1 : first limb normal np.array([x,y,z])\n",
    "    - c1 : first limb center np.array([x,y,z])\n",
    "    - n2 : second limb normal np.array([x,y,z])\n",
    "    - c2 : second limb center np.array([x,y,z])  \n",
    "    \"\"\" \n",
    "\n",
    "    pp = np.cross(n1, n2)\n",
    "    pp_norm = np.linalg.norm(pp)\n",
    "    if pp_norm == 0:\n",
    "        raise MalissiaBaseError(\"Limbs are parallel.\")\n",
    "    pp = pp/pp_norm\n",
    "    t = n1 + n2\n",
    "    t /= np.linalg.norm(t)\n",
    "    c = c2 - c1\n",
    "    s = c - np.dot(c,t)*t - np.dot(c,pp)*pp\n",
    "    s /= np.linalg.norm(s)\n",
    "    return s\n",
    "\n",
    "def compute_fold_axis(n1, c1, n2,  c2):\n",
    "    \"\"\"\n",
    "    Method to compute the fold axis based on s vector and t 'n1+n2'. It returns an np.array([x,y,z]) vector.\n",
    "    Parameter:\n",
    "    - n1 : first limb normal np.array([x,y,z])\n",
    "    - c1 : first limb center np.array([x,y,z])\n",
    "    - n2 : second limb normal np.array([x,y,z])\n",
    "    - c2 : second limb center np.array([x,y,z])  \n",
    "    \"\"\" \n",
    "    s_vector = compute_s_vector(n1, c1, n2,  c2)\n",
    "    t = n1+n2\n",
    "    t /= np.linalg.norm(t)\n",
    "    fold_axis = np.cross(t, s_vector)\n",
    "    fold_axis /= np.linalg.norm(fold_axis)\n",
    "    return fold_axis\n",
    "\n",
    "def compute_intersection_point(plane1_center, plane1_normal, plane2_center, plane2_normal, fold_axis):\n",
    "    \"\"\"\n",
    "    Method to compute the intersection point of the projection of the normal vectors. It returns an np.array([x,y,z]) point.\n",
    "    Parameter:\n",
    "    - plane1_normal : first limb normal np.array([x,y,z])\n",
    "    - plane1_center : first limb center np.array([x,y,z])\n",
    "    - plane2_normal : second limb normal np.array([x,y,z])\n",
    "    - plane2_center : second limb center np.array([x,y,z])\n",
    "    - fold_axis = the fold_axis  \n",
    "    \"\"\" \n",
    "    # Convert input to NumPy arrays for easier vector operations\n",
    "    plane1_center = np.array(plane1_center)\n",
    "    plane1_normal = np.array(plane1_normal)\n",
    "    plane2_center = np.array(plane2_center)\n",
    "    plane2_normal = np.array(plane2_normal)\n",
    "\n",
    "    # Find the direction vector of the line of intersection\n",
    "    v2_to_hinge = compute_direction_toward_hinge_along_limb_2(plane2_normal, fold_axis)\n",
    "    c1_c2 = plane2_center - plane1_center\n",
    "    k = -np.dot(c1_c2, plane1_normal)/np.dot(plane1_normal, v2_to_hinge)\n",
    "    intersection_point = plane2_center  + k * np.array(v2_to_hinge)\n",
    "    return intersection_point\n",
    "\n",
    "def compute_point_projection_onto_line(point_to_project, line_origin, line_vector):\n",
    "    \"\"\"\n",
    "    Method to compute the projection of a point onto a line defined by a vector and an origin. It returns an np.array([x,y,z]) point.\n",
    "    Parameter:\n",
    "    - point_to_project : point_to_project np.array([x,y,z])\n",
    "    - line_origin : line_origin np.array([x,y,z])\n",
    "    - line_vector : line_vector np.array([x,y,z])  \n",
    "    \"\"\" \n",
    "\n",
    "    # Normalize the line vector\n",
    "    line_normalized = np.array(line_vector / np.linalg.norm(line_vector))\n",
    "    # Calculate the vector from the origin to the point\n",
    "    w = np.array(np.array(point_to_project) - line_origin)\n",
    "    # Project w onto the line\n",
    "    projection_length = np.dot(w, line_normalized)\n",
    "    # Find the projected point on the line\n",
    "    projected_point = line_origin + projection_length * line_normalized\n",
    "    return np.array(projected_point)\n",
    "\n",
    "def compute_distance_to_vector(point, vector_origin, vector_orientation):\n",
    "    \"\"\"\n",
    "    Method to compute the distnace of a point from a vector. It returns a float.\n",
    "    Parameter:\n",
    "    - point : point np.array([x,y,z])\n",
    "    - vector_origin : line_origin np.array([x,y,z])\n",
    "    - vector_orientation : line_vector np.array([x,y,z])  \n",
    "    \"\"\" \n",
    "    # Convert input to NumPy arrays for easier vector operations\n",
    "    point = np.array(point)\n",
    "    vector_origin = np.array(vector_origin)\n",
    "    vector_orientation = np.array(vector_orientation)\n",
    "\n",
    "    # Calculate the vector from the center to the point\n",
    "    vector_to_point = point - vector_origin\n",
    "\n",
    "    # Calculate the projection of vector_to_point onto vector_orientation\n",
    "    projection = np.dot(vector_to_point, vector_orientation) / np.dot(vector_orientation, vector_orientation) * vector_orientation\n",
    "\n",
    "    # Calculate the distance between the point and the projection\n",
    "    distance = np.linalg.norm(vector_to_point - projection)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_projection_info (projected_points,  fold_axis, tolerance = 0.2):\n",
    "    \"\"\"\n",
    "    Method to compute retrieverelated information of a projection of a point onto a vector . It returns a dict:\n",
    "    {'start_point': np.array([x,y,z]), 'end_point': np.array([x,y,z]), 'side_size': np.array([x,y,z])}.\n",
    "    It raises an error if the extreme points from list coincide or when the projected points are not collinear.\n",
    "    Parameter:\n",
    "    - projected_points : list of projected_points in the form [np.array([x,y,z]), np.array([x,y,z])....]\n",
    "    - fold_axis : vector np.array([x,y,z])  \n",
    "    \"\"\" \n",
    "    # Initialize variables to store extreme points and distances\n",
    "    max_distance = -1\n",
    "    extreme_points_max = None\n",
    "    # Calculate pairwise distances and the extreme points\n",
    "    for pair in itertools.combinations(projected_points, 2):\n",
    "        point1, point2 = pair\n",
    "        distance = np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "        if distance > max_distance:\n",
    "            max_distance = distance\n",
    "            extreme_points_max = [point1, point2]\n",
    "\n",
    "    point0, point1 = np.array(extreme_points_max[0]), np.array(extreme_points_max[1])\n",
    "    if all(point0 == point1):\n",
    "        raise MalissiaBaseError ('the projected points coincide')\n",
    "    side_size = max_distance\n",
    "    vec = np.array([point1[0] - point0[0], \n",
    "           point1[1] - point0[1],\n",
    "             point1[2] - point0[2]])\n",
    "    \n",
    "    cross_product_magnitude = np.linalg.norm(np.cross(vec / np.linalg.norm(vec), fold_axis / np.linalg.norm(fold_axis)))\n",
    "    \n",
    "    if  not cross_product_magnitude <= tolerance:\n",
    "        raise MalissiaBaseError('projected points on the fold axis are not collinear')\n",
    "    if np.dot(vec, fold_axis) > 0:\n",
    "        start_point = point0\n",
    "        end_point = point1\n",
    "    else: \n",
    "        start_point = point1\n",
    "        end_point = point0\n",
    "    return {'start_point': start_point, 'end_point': end_point,\n",
    "             'side_size': side_size}\n",
    "\n",
    "def compute_side_size_and_start_point(limb_part1,  limb_part2, physical_space,  fold_axis, intersection_point, all_surfaces_nodes = None):\n",
    "    \"\"\"\n",
    "    Method to compute side_size and start_point for constructing fold limbs. It returns : float(side_size) and np.array(start_point)\n",
    "\n",
    "    Parameter:\n",
    "    - limb_part1 : limb_part1\n",
    "    - limb_part2 : limb_part2\n",
    "    - physical_space : the physical_space\n",
    "    - fold_axis : vector np.array([x,y,z])  \n",
    "    - intersection_point : intersection_point in the form of np.array([x,y,z]) \n",
    "    \"\"\" \n",
    "    if all_surfaces_nodes == None:\n",
    "        all_surfaces_nodes = [v for v in np.array(physical_space.get_object_coordinates( object= limb_part1, kind= \"nodes\")) ]\n",
    "        for v in np.array(physical_space.get_object_coordinates( object= limb_part2, kind= \"nodes\")):\n",
    "            all_surfaces_nodes.append(v) \n",
    "    else:\n",
    "        all_surfaces_nodes = all_surfaces_nodes\n",
    "    projected_nodes = []\n",
    "    for node_i in all_surfaces_nodes:\n",
    "        projected_nodes.append(compute_point_projection_onto_line(node_i, \n",
    "                intersection_point, fold_axis)) \n",
    "        \n",
    "    information = compute_projection_info(projected_nodes,  fold_axis)\n",
    "    all_distances = [compute_distance_to_vector(n_i, intersection_point, fold_axis) \n",
    "                    for n_i in all_surfaces_nodes]\n",
    "    max_distance = np.max(all_distances)\n",
    "    side_size =  information['side_size'] if information['side_size']>= max_distance else  max_distance\n",
    "    return side_size, information['start_point']\n",
    "\n",
    "def compute_shape_limb1(start_point, fold_axis, side_size,  limb_normal):\n",
    "    \"\"\"\n",
    "    Method to compute the shape of the first limb of a fold based on a start_point and a side_size and the limb_normal \n",
    "        It returns : a list of vertices [vertex0, vertex1, vertex2, vertex3]\n",
    "\n",
    "    Parameter:\n",
    "    - start_point : point np.array([x,y,z])\n",
    "    - fold_axis : vector np.array([x,y,z])\n",
    "    - side_size : float \n",
    "    - limb_normal : vector np.array([x,y,z])  \n",
    "    \"\"\" \n",
    "    vertex0 = start_point\n",
    "    vertex1 = compute_endpoint(vertex0, fold_axis, side_size)\n",
    "    vertex2 = compute_endpoint(vertex1, \n",
    "                               (-compute_direction_toward_hinge_along_limb_1(limb_normal,\n",
    "                                fold_axis)), side_size)\n",
    "    vertex3 = compute_endpoint(vertex2, -fold_axis, side_size)\n",
    "    return [vertex0, vertex1, vertex2, vertex3]   \n",
    "\n",
    "def compute_shape_limb2( vertex0, vertex1 , fold_axis, side_size,  limb_normal):\n",
    "    \"\"\"\n",
    "    Method to compute the shape of the second limb of a fold based on a vertex0, vertex1; a side_size and the limb_normal \n",
    "        It returns : a list of vertices [vertex0, vertex1, vertex2, vertex3]\n",
    "\n",
    "    Parameter:\n",
    "    - vertex0 : point np.array([x,y,z])\n",
    "    - vertex1 : point np.array([x,y,z])\n",
    "    - fold_axis : vector np.array([x,y,z])\n",
    "    - side_size : float \n",
    "    - limb_normal : vector np.array([x,y,z])  \n",
    "    \"\"\" \n",
    "    vertex0 = vertex0\n",
    "    vertex1 = vertex1\n",
    "    vertex2 = compute_endpoint(vertex1, \n",
    "                               (-compute_direction_toward_hinge_along_limb_2(limb_normal,\n",
    "                                fold_axis)), side_size)\n",
    "    vertex3 = compute_endpoint(vertex2,fold_axis, side_size)\n",
    "    return [vertex0, vertex1, vertex2, vertex3]   \n",
    "\n",
    "def compute_endpoint(start_point, direction_vector, distance):\n",
    "    \"\"\"\n",
    "    Method to compute a projection of a point toward a direction defined by a vector and at a distance.\n",
    "        It returns :point np.array([x,y,z])\n",
    "\n",
    "    Parameter:\n",
    "    - start_point : point np.array([x,y,z])\n",
    "    - direction_vector : vector np.array([x,y,z])\n",
    "    - distance : float \n",
    "    \"\"\" \n",
    "    direction_vector = np.array(direction_vector)\n",
    "    direction_vector = direction_vector/np.linalg.norm(direction_vector)\n",
    "    endpoint = [float(start_point[i] + distance * direction_vector[i]) for i in range(len(start_point))]\n",
    "    return endpoint\n",
    "\n",
    "def compute_axial_surface_normal(limb1_normal, limb2_normal, fold_axis):\n",
    "    \"\"\"\n",
    "    Method to compute the axial surface normal vector based on the first limb ormal, second limb normal and the fold axis.\n",
    "        It returns :a vector np.array([x,y,z])\n",
    "\n",
    "    Parameter:\n",
    "    - limb1_normal : vector np.array([x,y,z])\n",
    "    - limb2_normal : vector np.array([x,y,z])\n",
    "    - fold_axis : vector np.array([x,y,z])\n",
    "    \"\"\" \n",
    "    limb1_normal = np.array(limb1_normal).astype(float)\n",
    "    limb2_normal = np.array(limb2_normal).astype(float)\n",
    "    axial_surface_normal = np.cross(fold_axis, limb1_normal+limb2_normal)\n",
    "    return axial_surface_normal / np.linalg.norm(axial_surface_normal)\n",
    "\n",
    "def limb1_constuctor(fold, knowledge_framework, physical_space,\n",
    "                     fold_axis = None, side_size = None, limb1_normal = None,\n",
    "                     start_point = None, \n",
    "                     limb1_nodes_tempo = None, use_original_part_as_limb1 = False , original_part = None):\n",
    "    \"\"\"\n",
    "    Method to construct the first limb of a fold. If limb1_nodes_tempo are not given, \n",
    "    the method will compute the nodes from the start_point,fold_axis, side_size and the limb1_normal. In this case these parameters must be given.\n",
    "        It returns : an object of type limb and stratigraphic_part.\n",
    "\n",
    "    Parameter:\n",
    "    - fold : object of type fold\n",
    "    - knowledge_framework : the knowledge_framework\n",
    "    - fold_axis = vector np.array([x,y,z])\n",
    "    - side_size = float\n",
    "    - limb1_normal = vector np.array([x,y,z]\n",
    "    - start_point = point np.array([x,y,z]\n",
    "    - limb1_nodes_tempo : [vertex0, vertex1, vertex2, vertex3]\n",
    "    \"\"\" \n",
    "    if use_original_part_as_limb1 == False:\n",
    "\n",
    "        # must add checking if not all none\n",
    "        limb1_nodes_tempo = limb1_nodes_tempo if limb1_nodes_tempo is not None else compute_shape_limb1(start_point,fold_axis, side_size,limb1_normal) \n",
    "    \n",
    "        ## format them for the knowledgeframework\n",
    "        \n",
    "        coor_lab = physical_space.coordinate_labels\n",
    "        limb1_nodes = [\n",
    "            constructor_point(knowledge_framework, coor_lab ,[float(x) for x in limb1_nodes_tempo[0]]),\n",
    "            constructor_point(knowledge_framework, coor_lab,[float(x) for x in limb1_nodes_tempo[1]]),\n",
    "            constructor_point(knowledge_framework, coor_lab ,[float(x) for x in limb1_nodes_tempo[2]]),\n",
    "            constructor_point(knowledge_framework, coor_lab,[float(x) for x in limb1_nodes_tempo[3]])\n",
    "            ]\n",
    "        ## create the stratigraphic_part1 that will be the first fold limb\n",
    "        strat_part_limb1 = constructor_surface_part_from_nodes(knowledge_framework = knowledge_framework,\n",
    "                                                    nodes= limb1_nodes, \n",
    "                                                    )\n",
    "    else:\n",
    "        if original_part is None:\n",
    "            raise MalissiaBaseError('original part must be given if use_original_part_as_limb1 is True')\n",
    "        else: \n",
    "            strat_part_limb1 = original_part\n",
    "\n",
    "    ## define this stratigraphic part as the first fold limb\n",
    "    if knowledge_framework().Fold_Limb not in strat_part_limb1.is_a:\n",
    "        strat_part_limb1.is_a.append(knowledge_framework().Fold_Limb)\n",
    "\n",
    "    return strat_part_limb1\n",
    "\n",
    "def limb2_constuctor( fold, knowledge_framework,  physical_space,\n",
    "                     vertex0 = None , vertex1 = None , fold_axis = None , side_size = None ,\n",
    "                       limb2_normal = None ,limb2_nodes_tempo = None, use_original_part_as_limb2 = False , original_part = None):\n",
    "    \"\"\"\n",
    "    Method to construct the first limb of a fold. If limb2_nodes_tempo are not given, \n",
    "    the method will compute the nodes from the start_point,fold_axis, side_size and the limb2_normal. In this case these parameters must be given.\n",
    "        It returns : an object of type limb and stratigraphic_part.\n",
    "\n",
    "    Parameter:\n",
    "    - fold : object of type fold\n",
    "    - knowledge_framework : the knowledge_framework\n",
    "    - vertex0 = point np.array([x,y,z]\n",
    "    - vertex1 = point np.array([x,y,z]\n",
    "    - fold_axis = vector np.array([x,y,z])\n",
    "    - side_size = float\n",
    "    - limb2_normal = vector np.array([x,y,z]\n",
    "    - limb2_nodes_tempo : [vertex0, vertex1, vertex2, vertex3]\n",
    "    \"\"\" \n",
    "    if use_original_part_as_limb2 == False:\n",
    "\n",
    "        limb2_nodes_tempo = limb2_nodes_tempo if limb2_nodes_tempo is not None else compute_shape_limb2(vertex0= vertex0,vertex1= vertex1,\n",
    "                                        fold_axis= fold_axis, \n",
    "                                        side_size = side_size,\n",
    "                                        limb_normal= limb2_normal) \n",
    "        \n",
    "        ## format them for the knowledgeframework\n",
    "        coor_lab = physical_space.coordinate_labels\n",
    "        limb2_nodes = [\n",
    "            constructor_point(knowledge_framework, coor_lab,[float(x) for x in limb2_nodes_tempo[0]],),\n",
    "            constructor_point(knowledge_framework, coor_lab,[float(x) for x in limb2_nodes_tempo[1]]),\n",
    "            constructor_point(knowledge_framework, coor_lab,[float(x) for x in limb2_nodes_tempo[2]]),\n",
    "            constructor_point(knowledge_framework, coor_lab,[float(x) for x in limb2_nodes_tempo[3]])\n",
    "            ]\n",
    "        \n",
    "        ## create the stratigraphic_part1 that will be the first fold limb\n",
    "        strat_part_limb2 = constructor_surface_part_from_nodes(knowledge_framework = knowledge_framework,\n",
    "                                                    nodes= limb2_nodes, )\n",
    "    else:\n",
    "        if original_part is None:\n",
    "            raise MalissiaBaseError('original part must be given if use_original_part_as_limb1 is True')\n",
    "        else: \n",
    "            strat_part_limb2 = original_part\n",
    "\n",
    "    ## define this stratigraphic part as the first fold limb\n",
    "    if knowledge_framework().Fold_Limb not in strat_part_limb2.is_a:\n",
    "        strat_part_limb2.is_a.append(knowledge_framework().Fold_Limb)\n",
    "    \n",
    "    return strat_part_limb2\n",
    "\n",
    "def axial_surface_constructor(knowledge_framework, physical_space, fold, limb1_normal, limb2_normal,\n",
    "                               fold_axis, midpoint, side_size):\n",
    "    \"\"\"\n",
    "    Method to construct the fold axial surface. It returns : an object of type planar surface.\n",
    "\n",
    "    Parameter:\n",
    "    - fold : object of type fold\n",
    "    - knowledge_framework : the knowledge_framework\n",
    "    - fold_axis = vector np.array([x,y,z])\n",
    "    - limb1_normal = vector np.array([x,y,z]\n",
    "    - physical_space: the physical_space\n",
    "    - limb2_normal : vector np.array([x,y,z]\n",
    "    - midpoint : midpoint between the two shared vertices of the two limbs\n",
    "    - side_size : float \n",
    "    \"\"\" \n",
    "    axial_surf_normal = compute_axial_surface_normal(limb1_normal ,limb2_normal, fold_axis)\n",
    "    dip, dip_dir, polarity = physical_space.compute_dip_dir_from_normal(axial_surf_normal)  \n",
    "\n",
    "    \n",
    "    axial_surface_representtation = constructor_planar_surface_from_center_attitude(knowledge_framework= knowledge_framework, \n",
    "                                                    coord_labels= physical_space.coordinate_labels,\n",
    "                                                            center= midpoint,\n",
    "                                                       size= side_size*3, dip= dip, dip_dir= dip_dir, polarity= polarity,\n",
    "                                                       name= \"{}_axial_surface_rep\".format(fold.name)) \n",
    "                                                  \n",
    "    axial_surface = knowledge_framework().Axial_Surface(name = \"{}_axial_surface\".format(fold.name), \n",
    "                                                        has_Representation = [axial_surface_representtation])\n",
    "    return axial_surface\n",
    "\n",
    "def fold_constructor_from_two_stratigraphic_parts(candidate_stratigraphic_part1 = None, \n",
    "                     candidate_stratigraphic_part2 = None,  \n",
    "                    candidate_limb_part1 = None, candidate_limb_part2 = None,\n",
    "                     physical_space= None, name= None, \n",
    "                     knowledge_framework= None, difference_in_dipping_anomaly = None, **kargs, \n",
    "                      ):\n",
    "    \"\"\"\n",
    "    Method to construct a fold from two stratigraphic parts. If the stratigraphic parts are not designate as candidate_limb_part1 and candidate_limb_part2, \n",
    "    the method takes candidate_stratigraphic_part1 and candidate_stratigraphic_part2 and makes a fold based on a random assignements of limbs order.\n",
    "    It returns : an object of type fold with its properties and relations \n",
    "        (has_Axial_Surface, has_Fold_Axis, has_Limb1, has_Limb2)\n",
    "\n",
    "    Parameter:\n",
    "    - candidate_stratigraphic_part1 = the candidate_stratigraphic_part1\n",
    "    - candidate_stratigraphic_part2 = the candidate_stratigraphic_part2,  \n",
    "    - candidate_limb_part1 = the candidate_limb_part1\n",
    "    - candidate_limb_part2 = the candidate_limb_part2\n",
    "    - physical_space= the physical_space \n",
    "    - name= str possible name of the new fold, \n",
    "    - knowledge_framework : the knowledge_framework\n",
    "     \n",
    "    **kargs :\n",
    "\n",
    "    \"\"\" \n",
    "    # check for errors in the inputs\n",
    "    if candidate_limb_part1 != None and  candidate_limb_part2 != None:\n",
    "        limb_part1 = candidate_limb_part1\n",
    "        limb_part2 = candidate_limb_part2\n",
    "    elif candidate_stratigraphic_part1 is not None and candidate_stratigraphic_part2 is not None:\n",
    "        choice =  random.randint(1, 2)\n",
    "        if choice == 1 :\n",
    "            limb_part1 = candidate_stratigraphic_part1\n",
    "            limb_part2 = candidate_stratigraphic_part2\n",
    "        else : \n",
    "            limb_part2 = candidate_stratigraphic_part1\n",
    "            limb_part1 = candidate_stratigraphic_part2\n",
    "    else :\n",
    "        raise MalissiaBaseError('two cnadidate stratigraphic parts must be given')\n",
    "    \n",
    "    knowledge_framework = knowledge_framework #if knowledge_framework is not None else .knowledge_framework\n",
    "    #constructed_class = knowledge_framework().Chevron_Fold  ==> to be used with interpretation_situation but need to specficy which str_part as limb 1 or 2\n",
    "    #explainable_features = knowledge_framework.filter_explainable_features(constructed_class, interpretation_situation.features)\n",
    "    # creating an instance of a Chevron_Fold\n",
    "    physical_space = physical_space # add checking part\n",
    "\n",
    "    # check if the stratigraphic parts representations are already parts of an existing complex_surface\n",
    "    intersection = list(set(limb_part1.has_Representation[0].is_Part_Of).intersection(set(limb_part2.has_Representation[0].is_Part_Of)).intersection(set(knowledge_framework().Complex_Surface.instances())))\n",
    "    if len(intersection) > 0:\n",
    "        for inter_i in intersection:\n",
    "            possible_folds =  multiple_fold_detector_in_deformed_stratigraphic_part(deformed_stratigraphic_part = None, physical_space = physical_space,\n",
    "                                                                                     knowledge_framework = knowledge_framework ,force_complex_surface = inter_i )\n",
    "            if len(possible_folds) > 0 :\n",
    "                if len(possible_folds)==1:\n",
    "                    print('Warning: {} and {} already limbs of an existing fold {}. This latter will be returned'.format(limb_part1.name, limb_part2.name, possible_folds[0].name))\n",
    "                    return possible_folds[0]\n",
    "                if len(possible_folds) > 1:\n",
    "                    print('Warning: {} and {} are limbs of of multiple folds {} return the first of the list {}.'.format(limb_part1.name, limb_part2.name, possible_folds,  possible_folds[0].name))\n",
    "                    return possible_folds[0]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # get candidate limbs paramters\n",
    "    n1 = np.array(physical_space.get_object_coordinates( object= limb_part1, kind= \"normal\")) \n",
    "    n2 = np.array(physical_space.get_object_coordinates( object= limb_part2, kind= \"normal\")) \n",
    "    c1 = np.array(physical_space.get_object_coordinates( object= limb_part1, kind= \"center\")) \n",
    "    c2 = np.array(physical_space.get_object_coordinates( object= limb_part2, kind= \"center\"))\n",
    "    # create the fold\n",
    "    fold = knowledge_framework().Chevron_Fold()\n",
    "    # get initial fold parameters\n",
    "    fold_axis = compute_fold_axis(n1, c1, n2, c2)\n",
    "    intersection_point = compute_intersection_point(plane1_center=c1, \n",
    "                                                    plane1_normal=n1,plane2_center=c2, \n",
    "                                                    plane2_normal= n2, fold_axis= fold_axis)\n",
    "    \n",
    "    midpoint = None\n",
    "    nodes_limb1 = np.array(physical_space.get_object_coordinates( object= limb_part1, kind= \"nodes\"))\n",
    "    nodes_limb2 = np.array(physical_space.get_object_coordinates( object= limb_part2, kind= \"nodes\"))\n",
    "    common_nodes = common_two_nodes_two_planars(nodes_limb1, nodes_limb2, return_error = False)\n",
    "    if common_nodes is not None:\n",
    "        print(' will use two')\n",
    "        midpoint = (np.array(common_nodes[0]) + np.array(common_nodes[1])) /2 \n",
    "        side_size = float(np.linalg.norm((np.array(common_nodes[0]) - np.array(common_nodes[1]))))\n",
    "        strat_part_limb1 = limb1_constuctor(fold = fold, knowledge_framework = knowledge_framework, physical_space= physical_space, use_original_part_as_limb1= True, original_part= limb_part1)\n",
    "        strat_part_limb2 = limb1_constuctor(fold = fold, knowledge_framework = knowledge_framework, physical_space= physical_space, use_original_part_as_limb1= True, original_part= limb_part2)\n",
    "    else:\n",
    "\n",
    "        ## get side_size and start point\n",
    "        side_size, start_point = compute_side_size_and_start_point(limb_part1 = limb_part1,  limb_part2 = limb_part2,\n",
    "                                    physical_space = physical_space,  fold_axis = fold_axis, \n",
    "                                    intersection_point = intersection_point)\n",
    "        \n",
    "        # now create limbs \n",
    "        strat_part_limb1 = limb1_constuctor(start_point = start_point, \n",
    "                                            fold_axis = fold_axis, side_size = side_size, \n",
    "                                            limb1_normal = n1, fold = fold, \n",
    "                                            knowledge_framework = knowledge_framework, physical_space= physical_space)\n",
    "        ## get surface1 nodes to pass them for the second limb\n",
    "        limb1_nodes_tempo = compute_shape_limb1(start_point,fold_axis, side_size,n1) # repeated but a must\n",
    "        vertex0_for_limb2, vertex1_for_limb2 = limb1_nodes_tempo[1],limb1_nodes_tempo[0]\n",
    "        ## create second limb \n",
    "        strat_part_limb2 = limb2_constuctor(vertex0 = vertex0_for_limb2, \n",
    "                                            vertex1 = vertex1_for_limb2, \n",
    "                                            fold_axis = fold_axis, side_size = side_size, limb2_normal = n2,\n",
    "                                            fold = fold, knowledge_framework = knowledge_framework, physical_space= physical_space)\n",
    "\n",
    "\n",
    "\n",
    "    # create axial_surface and its representation \n",
    "    coord_labels = physical_space.coordinate_labels\n",
    "    midpoint = [float((i1 + i2) / 2) for i1, i2 in zip(limb1_nodes_tempo[1],limb1_nodes_tempo[0])] if midpoint is None else midpoint\n",
    "\n",
    "    axial_surface = axial_surface_constructor(knowledge_framework = knowledge_framework, physical_space = physical_space,\n",
    "                                               fold = fold, limb1_normal = n1, limb2_normal = n2, fold_axis = fold_axis,\n",
    "                                                 midpoint = midpoint, side_size = side_size)\n",
    "    # foramt the fold axis\n",
    "    fold_axis_vector_rep = constructor_vector(knowledge_framework, coord_labels, \n",
    "                                              [float(x) for x in fold_axis], name= \"{}_fold_axis_rep\".format(fold.name))\n",
    "    fold_axis_object = knowledge_framework().Fold_Axis(name = \"{}_fold_axis\".format(fold.name), \n",
    "                                                       has_Representation = [fold_axis_vector_rep])\n",
    "    # attach paramters and objects to the fold \n",
    "    strati_graphic_part = construct_complex_stratigraphic_part(knowledge_framework,[strat_part_limb1, strat_part_limb2], physical_space )\n",
    "    strati_graphic_part_rep = strati_graphic_part.has_Representation\n",
    "    fold.has_Representation = strati_graphic_part_rep\n",
    "    if difference_in_dipping_anomaly is not None:\n",
    "        fold.explain.append(difference_in_dipping_anomaly)\n",
    "    \n",
    "    fold.has_Axial_Surface= [axial_surface] \n",
    "    fold.has_Fold_Axis =  [fold_axis_object]\n",
    "    fold.has_Limb1 = [strat_part_limb1]\n",
    "    fold.has_Limb2 = [strat_part_limb2]\n",
    "    return  fold\n",
    "\n",
    "\n",
    "\n",
    "def choose_square_side(limb_nodes):\n",
    "    \"\"\"\n",
    "    Method to choose a square side index based on four nodes.It returns a square side index\n",
    "\n",
    "    Parameter:\n",
    "    - limb_nodes : [vertex0, vertex1, vertex2, vertex3]\n",
    "    \"\"\" \n",
    "    z = limb_nodes[:,2]\n",
    "    dz1 = np.abs(z[1] - z[0])\n",
    "    dz2 = np.abs(z[2] - z[1])\n",
    "    if dz1 < dz2:\n",
    "        selected_indices = [0,2]\n",
    "    elif dz1 > dz2:\n",
    "        selected_indices = [1,3]\n",
    "    else:\n",
    "        selected_indices = np.arange(4)\n",
    "    return random.choice(selected_indices)\n",
    "\n",
    "def get_square_side(limb_nodes, i):\n",
    "    \"\"\"\n",
    "    Method to get a square side based on its index .It returns a square side start, end points and the vector between them [nodes[0], nodes[1], vec].\n",
    "\n",
    "    Parameter:\n",
    "    - limb_nodes : [vertex0, vertex1, vertex2, vertex3]\n",
    "    - i : side_index int 0-3\n",
    "    \"\"\" \n",
    "    edges = np.array([[0,1],[1,2],[2,3],[3,0]])\n",
    "    nodes = limb_nodes[edges[i]]\n",
    "    vec = nodes[1] - nodes[0]\n",
    "    return [nodes[0], nodes[1], vec]\n",
    "\n",
    "def compute_complement_angle(angle_radians):\n",
    "    \"\"\"\n",
    "    Method to compute the complement angle to 180. It returns a float in radians.\n",
    "    Parameter:\n",
    "    - angle_radians : initial angle type float in radians \n",
    "    \"\"\"     \n",
    "    # Calculate the complement in radians\n",
    "    return  math.pi -angle_radians\n",
    "\n",
    "def rotate_vector_around_axis(n1, p, theta_rad ):\n",
    "    \"\"\"\n",
    "    Method to compute a vector rotation around another vector by a given angle. It returns an np.array([x,y,z]) vector.\n",
    "    Parameter:\n",
    "    - n1 : the vector to be rotated, in the form of np.array([x,y,z]) vector\n",
    "    - p :  the vector on which n1 will be rotated, in the form of np.array([x,y,z]) vector\n",
    "    - theta_rad : a rotation angle in radians as type float\n",
    "    \"\"\" \n",
    "    n1 = n1/ np.linalg.norm(n1)\n",
    "    # Calculate the rotation matrix\n",
    "    # Normalize the direction vector p over which the rotation will be performed\n",
    "    p = p / np.linalg.norm(p)\n",
    "    \n",
    "    # Calculate the cross product matrix of p\n",
    "    P = np.array([\n",
    "        [0, -p[2], p[1]],\n",
    "        [p[2], 0, -p[0]],\n",
    "        [-p[1], p[0], 0]\n",
    "    ])\n",
    "    # Calculate the rotation matrix using the Rodriguez formula\n",
    "    R = np.eye(3) + np.sin(theta_rad) * P + (1 - np.cos(theta_rad)) * np.dot(P, P)\n",
    "    # Step 2: Apply the rotation to n1\n",
    "    rotated_n1 = np.round(np.array(np.dot(R, n1)),8) \n",
    "    return rotated_n1 / np.linalg.norm(np.dot(R, n1))\n",
    "\n",
    "def fold_constructor_one_stratigraphic_part(candidate_stratigraphic_part,\n",
    "                                            knowledge_framework, physical_space, \n",
    "                                            name= None,  fold_opening_angle_degree  = 90., \n",
    "                                                 force_limb_assignment = None , \n",
    "                                                 force_fold_type = None , \n",
    "                                                 chosen_side_index = None, dipping_anomaly = None, \n",
    "                                                   **kargs, ):\n",
    "    \"\"\"\n",
    "    Method to compute a fold based on one candidate limb part. The candidate limb part could be assigned as the \n",
    "    first fold limb or the second. The fold type could be forced a type 'anticline' or 'syncline' must be given.\n",
    "    The construction could be established on a chosen side of the candidate limb part representation.\n",
    "        If a force_limb_assignment, force_fold_type and chosen_side_index are not given, the method will make a random choice.\n",
    "    The method returns : an object of type fold with its properties and relations \n",
    "    Parameter:\n",
    "    - candidate_stratigraphic_part : the candidate_stratigraphic_part as one fold limbs\n",
    "    - fold_opening_angle_degree  : the opening angle of the fold limbs, by default 90 degree,\n",
    "    - physical_space= thephysical_space \n",
    "    - name : possible name of the resulting fold, as type str \n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    - force_limb_assignment : assignement of the candidate limb part as limb1 or 2 a value from [1,2] must be given\n",
    "    - force_fold_type  : a fold type defined by the stratigraphic part polarty, if given a value from ['anticline','syncline'] mut be chosen\n",
    "    - chosen_side_index : an index of the chosen side on which the constructed limb will be attached to the candidate limb part an integer value between 0-3 must be chosen\n",
    "     \n",
    "    \"\"\" \n",
    "    # initial error checking:\n",
    "    #constructed_class = knowledge_framework().Chevron_Fold  ==> to be used with interpretation_situation but need to specficy which str_part as limb 1 or 2\n",
    "    #explainable_features = knowledge_framework.filter_explainable_features(constructed_class, interpretation_situation.features)\n",
    "    # creating an instance of a Chevron_Fold\n",
    "\n",
    "\n",
    "    ##  make a choice of the limbs assignement if it is not suggesed\n",
    "    initial_limb = force_limb_assignment if force_limb_assignment is not None else random.choice([1,2]) \n",
    "    if initial_limb not in (1,2):  \n",
    "        raise MalissiaBaseError('The choice of which limb could not be be made, try giving force_limb_assignment a value of 1 or 2')\n",
    "    ##  make a choice of fold type if it is not suggesed\n",
    "    fold_type = force_fold_type if force_fold_type is not None else random.choice(['anticline','syncline'])\n",
    "    if fold_type not in ['anticline','syncline']: \n",
    "        raise MalissiaBaseError('The choice of fold type could not be made, try giving force_limb_assignment a value of anticline syncline')\n",
    "    \n",
    "    # get candidate limb nodes coordiantes, center \n",
    "    candidate_limb_nodes = physical_space.get_object_coordinates( object= candidate_stratigraphic_part, kind= \"nodes\")\n",
    "    candidate_limb_center = physical_space.get_object_coordinates( object= candidate_stratigraphic_part, kind= \"center\")\n",
    "    \n",
    "    # choose a side to be used as fold axis\n",
    "    chosen_side_index = chosen_side_index if chosen_side_index is not None else choose_square_side(candidate_limb_nodes)\n",
    "    chosen_side = get_square_side(candidate_limb_nodes, chosen_side_index)\n",
    "    if len (chosen_side) == 0 :\n",
    "        raise MalissiaBaseError('the algorithm could not chose a square side')\n",
    "    # get a midpoint of the side\n",
    "    midpoint = np.mean((chosen_side[0], chosen_side[1]), axis=0)\n",
    "    # compute direction toward the chosen fold axis using the midpoint\n",
    "    vector_to_axis = midpoint -candidate_limb_center \n",
    "    # get an initial normal \n",
    "    initial_normal = physical_space.get_object_coordinates( object= candidate_stratigraphic_part, kind= \"normal\")\n",
    "    # propose a fold axis\n",
    "    proposed_fold_axis = np.cross(initial_normal, vector_to_axis) / np.linalg.norm(np.cross(initial_normal, vector_to_axis))\n",
    "    \n",
    "\n",
    "    # based on limb assignement : \n",
    "    ## keep the fold axis if limb 1 and choose the proper angle based on the fold type\n",
    "    if initial_limb == 1:\n",
    "        fold_opening_angle_degree = fold_opening_angle_degree if fold_type == 'anticline' else -fold_opening_angle_degree\n",
    "\n",
    "    ## invert  the fold axis if limb 2 and choose the proper angle based on the fold type\n",
    "    if initial_limb == 2:\n",
    "        proposed_fold_axis *= -1\n",
    "        fold_opening_angle_degree = fold_opening_angle_degree if fold_type == 'syncline' else -fold_opening_angle_degree\n",
    "\n",
    "    # get the angle of rotation in rad based on the openning angle\n",
    "    angle_of_normal_rotation_rad =  compute_complement_angle(np.deg2rad(fold_opening_angle_degree))\n",
    "\n",
    "    # rotate the initial normal over the fold axis \n",
    "    computed_rotated_normal = rotate_vector_around_axis(n1=initial_normal, \n",
    "                                                        p= proposed_fold_axis, \n",
    "                                                        theta_rad= angle_of_normal_rotation_rad)\n",
    "    # compute a side size based on the chosen side\n",
    "    side_size  = np.linalg.norm(chosen_side[1] - chosen_side[0])\n",
    "    # create the fold\n",
    "    fold = knowledge_framework().Chevron_Fold()\n",
    "\n",
    "    # construct now the stratigraphic part of the second limb based on the assignement (1 or 2)\n",
    "    \n",
    "    if initial_limb == 1:\n",
    "        limb1 = candidate_stratigraphic_part\n",
    "        limb2_v1 = chosen_side[1]\n",
    "        limb2_v2 = chosen_side[0]\n",
    "        direction_toward_hinge_2 = compute_direction_toward_hinge_along_limb_2(computed_rotated_normal, proposed_fold_axis)\n",
    "        limb2_v3 = compute_endpoint(limb2_v2, -direction_toward_hinge_2, side_size)\n",
    "        limb2_v4 = compute_endpoint(limb2_v3, proposed_fold_axis, side_size)\n",
    "        limb2_nodes = np.array([limb2_v1, limb2_v2, limb2_v3, limb2_v4])\n",
    "        limb2 = limb2_constuctor(fold = fold, knowledge_framework = knowledge_framework, physical_space= physical_space, limb2_nodes_tempo= limb2_nodes )\n",
    "\n",
    "    if initial_limb == 2:\n",
    "        limb2 = candidate_stratigraphic_part\n",
    "        limb1_v1 = chosen_side[1]\n",
    "        limb1_v2 =chosen_side[0]\n",
    "        direction_toward_hinge_1 = compute_direction_toward_hinge_along_limb_1(computed_rotated_normal, proposed_fold_axis)\n",
    "        limb1_v3 = compute_endpoint(limb1_v2, -direction_toward_hinge_1, side_size)\n",
    "        limb1_v4 = compute_endpoint(limb1_v3, -proposed_fold_axis, side_size)\n",
    "        limb1_nodes = np.array([limb1_v1, limb1_v2, limb1_v3, limb1_v4]) \n",
    "        limb1 = limb1_constuctor(fold = fold, knowledge_framework = knowledge_framework, physical_space= physical_space, limb1_nodes_tempo= limb1_nodes )\n",
    "        \n",
    "    # now generate fold parameters    \n",
    "    # create axial_surface and its representation \n",
    "    \n",
    "     # use a midpoint between the vertices of the chosen side as a center, this must be changed of the side is no longer chosen as the fold axis\n",
    "    n1 = np.array(physical_space.get_object_coordinates( object= limb1, kind= \"normal\")) \n",
    "    n2 = np.array(physical_space.get_object_coordinates( object= limb2, kind= \"normal\")) \n",
    "    axial_surface = axial_surface_constructor(knowledge_framework = knowledge_framework, physical_space = physical_space,\n",
    "                                               fold = fold, limb1_normal = n1, limb2_normal = n2, fold_axis = proposed_fold_axis,\n",
    "                                                 midpoint = midpoint.tolist(), side_size = float(side_size))\n",
    "    # format the fold axis\n",
    "    fold_axis_vector_rep = constructor_vector(knowledge_framework, physical_space.coordinate_labels, \n",
    "                                              [float(x) for x in proposed_fold_axis], name= \"{}_fold_axis_rep\".format(fold.name))\n",
    "    fold_axis_object = knowledge_framework().Fold_Axis(name = \"{}_fold_axis\".format(fold.name), \n",
    "                                                       has_Representation = [fold_axis_vector_rep])\n",
    "    \n",
    "    strati_graphic_part = construct_complex_stratigraphic_part(knowledge_framework,[limb1, limb2], physical_space )\n",
    "    strati_graphic_part_rep = strati_graphic_part.has_Representation\n",
    "    fold.has_Representation = strati_graphic_part_rep\n",
    "    \n",
    "    if dipping_anomaly is not None:\n",
    "        fold.explain.append(dipping_anomaly)\n",
    "\n",
    "    # attach parameters and objects to the fold \n",
    "    fold.has_Axial_Surface= [axial_surface] \n",
    "    fold.has_Fold_Axis =  [fold_axis_object]\n",
    "    fold.has_Limb1 = [limb1]\n",
    "    fold.has_Limb2 = [limb2]\n",
    "\n",
    "    return fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def are_parallel_and_same_direction(v, w):\n",
    "    \"\"\"\n",
    "    Method to check if two vectors are parallel and oriented in the same direction. \n",
    "        It returns True or False.\n",
    "    Parameter:\n",
    "    - v : first vector in the form of np.array([x,y,z])\n",
    "    - w : second vector in the form of np.array([x,y,z])\n",
    "    \"\"\" \n",
    "    # Check if vectors are parallel\n",
    "    if np.allclose(np.cross(v, w), np.zeros_like(v)):\n",
    "        # Check if vectors are in the same direction\n",
    "        dot_product = np.dot(v, w)\n",
    "        if dot_product > 0:\n",
    "            return True  # Vectors are parallel and in the same direction\n",
    "    return False\n",
    "\n",
    "\n",
    "def fold_detector_from_two_parts_of_deformed_stratigraphic_part(stratigraphic_part, part1, part2, physical_space, \n",
    "                                                                knowledge_framework,  part1_rep, part2_rep,\n",
    "                                                                fold = None,\n",
    "                                                                fold_name = None,\n",
    "                                                                force_part1_and_part2 = True, \n",
    "                                                                proposed_fold_axis = None, \n",
    "                                                                ):\n",
    "    \"\"\"\n",
    "    Method to detect folding geometry on two surface parts in the representation of a deformed stratigraphic part.\n",
    "    If the stratigraphic part is not deformed the method will return an error. part1 and part2 could be forced as the two representation \n",
    "    of limb1 and limb2 respectively. A fold axis could be proposed. Only one paramter of these two parameters proposed_fold_axis or force_part1_and_part2\n",
    "    could be given or not, but not both in the same time. if two vectors are parallel and oriented in the same direction. \n",
    "        if found, the method returns : an object of type fold with its properties and relations \n",
    "    Parameter:\n",
    "    - stratigraphic_part : the candidate deformed stratigraphic on which the folding could be detected\n",
    "    - part1 : part1 of the complex_rerpesentation of the stratigraphic_part\n",
    "    - part2 : part2 of the complex_rerpesentation of the stratigraphic_part\n",
    "    - physical_space : the physical space \n",
    "    - knowledge_framework : the knowledge_framework\n",
    "    - coord_labels : coordinates labels ex: [\"X\",\"Y\",\"Z\"] \n",
    "    - fold : the fold object if exist\n",
    "    - fold_name : str for fold name if exist \n",
    "    - force_part1_and_part2 : boolean to force the assignement of part1 and part2 as limb1 and limb2 respectively \n",
    "    - proposed_fold_axis : fold axis if exist in the form of np.array([x,y,z])\n",
    "    \"\"\" \n",
    "    intersection1 = list(set(part1.is_Limb1_Of).intersection(set(part2.is_Limb2_Of)).intersection(set(knowledge_framework().Chevron_Fold.instances())))\n",
    "    intersection2 = list(set(part2.is_Limb1_Of).intersection(set(part2.is_Limb1_Of)).intersection(set(knowledge_framework().Chevron_Fold.instances())))\n",
    "    if len(intersection1) > 0 : \n",
    "        print('Warning: {} and {} already limbs of an existing fold {}. This latter will be returned'.format(part1.name, part1.name,  intersection1[0].name))\n",
    "        return intersection1[0]\n",
    "    if len(intersection2) > 0 : \n",
    "        print('Warning: {} and {} already limbs of an existing fold {}. This latter will be returned'.format(part1.name, part2.name,  intersection2[0].name))\n",
    "        return intersection2[0]\n",
    "    \n",
    "    if proposed_fold_axis != None and force_part1_and_part2 == True:\n",
    "        raise MalissiaBaseError('fold axis could be forced or limbs assignement or both not forced\\\n",
    "                                 but not both forced in the same time')\n",
    "    \n",
    "    if stratigraphic_part.deformed != True:\n",
    "        raise MalissiaBaseError('the stratigraphic part that is possibly a fold must be deformed')\n",
    "    \n",
    "    fold_name = fold_name if fold_name is not None else str('fold_nb{}'.format(len(list(knowledge_framework().Chevron_Fold.instances()))+1))\n",
    "    fold = knowledge_framework().Chevron_Fold(name = fold_name) if fold is None else fold\n",
    "    if knowledge_framework().Fold_Limb not in part1.is_a:\n",
    "        part1.is_a.append(knowledge_framework().Fold_Limb)\n",
    "    if knowledge_framework().Fold_Limb not in part2.is_a:\n",
    "        part2.is_a.append(knowledge_framework().Fold_Limb)\n",
    "    coor_lab = physical_space.coordinate_labels\n",
    "    nodes1 = physical_space.get_object_coordinates( object= part1_rep, kind= \"nodes\")\n",
    "    nodes2 = physical_space.get_object_coordinates( object= part2_rep, kind= \"nodes\")\n",
    "    common_nodes = common_two_nodes_two_planars(nodes1, nodes2)\n",
    "    axis_from_nodes = np.array(common_nodes[0]) - np.array(common_nodes[1])\n",
    "    midpoint = (np.array(common_nodes[0]) + np.array(common_nodes[1])) / 2\n",
    "    side_size = np.linalg.norm(axis_from_nodes)\n",
    "\n",
    "\n",
    "    n1 = physical_space.get_object_coordinates( object= part1_rep, kind= \"normal\")\n",
    "    n2 = physical_space.get_object_coordinates( object= part2_rep, kind= \"normal\")\n",
    "    c1 = physical_space.get_object_coordinates( object= part1_rep, kind= \"center\")\n",
    "    c2 = physical_space.get_object_coordinates( object= part2_rep, kind= \"center\")\n",
    "\n",
    "    fold_axis = compute_fold_axis(n1, c1, n2,  c2)\n",
    "\n",
    "    fold_axis_vector_rep = constructor_vector(knowledge_framework, coor_lab, \n",
    "                                              [float(x) for x in fold_axis], \n",
    "                                              name= \"{}_fold_axis_rep\".format(fold.name))\n",
    "    fold_axis_object = knowledge_framework().Fold_Axis(name = \"{}_fold_axis\".format(fold.name), \n",
    "                                                       has_Representation = [fold_axis_vector_rep])\n",
    "    \n",
    "    if part1 in part2.has_Neighbor or part2  in part1.has_Neighbor or \\\n",
    "                part1_rep in part2_rep.has_Neighbor or part2_rep in part1_rep.has_Neighbor :\n",
    "        \n",
    "        if force_part1_and_part2 == True:\n",
    "            axial_surface = axial_surface_constructor(knowledge_framework = knowledge_framework, \n",
    "                                                      physical_space =physical_space, \n",
    "                                                      fold = fold, limb1_normal= n1, limb2_normal= n2,\n",
    "                                                      fold_axis = fold_axis, \n",
    "                                                      midpoint = midpoint, side_size = side_size, \n",
    "                                                      )\n",
    "            # attach paramters and objects to the fold \n",
    "            \n",
    "            fold.has_Representation = stratigraphic_part.has_Representation\n",
    "            fold.has_Axial_Surface= [axial_surface] \n",
    "            fold.has_Fold_Axis =  [fold_axis_object]\n",
    "            fold.has_Limb1 = [part1]\n",
    "            fold.has_Limb2 = [part2]\n",
    "            return fold\n",
    "        if proposed_fold_axis != None:\n",
    "            proposed_fold_axis = np.array(proposed_fold_axis)\n",
    "            if are_parallel_and_same_direction(fold_axis, proposed_fold_axis):\n",
    "               return fold_detector_from_two_parts_of_deformed_stratigraphic_part(stratigraphic_part = stratigraphic_part, \n",
    "                                                                                  part1 = part1, part2 = part2,\n",
    "                                                                                  part1_rep = part1_rep, part2_rep = part2_rep,\n",
    "                                                                                    physical_space = physical_space, \n",
    "                                                                knowledge_framework = knowledge_framework,                                                                 \n",
    "                                                                fold = fold,                                                \n",
    "                                                                force_part1_and_part2 = True, \n",
    "                                                                proposed_fold_axis = None) \n",
    "            else:\n",
    "                return fold_detector_from_two_parts_of_deformed_stratigraphic_part(stratigraphic_part = stratigraphic_part, \n",
    "                                                                                  part1 = part2, part2 = part1,\n",
    "                                                                                  part1_rep = part1_rep, part2_rep = part2_rep,\n",
    "                                                                                    physical_space = physical_space, \n",
    "                                                                knowledge_framework = knowledge_framework,                                                                  \n",
    "                                                                fold = fold ,                                               \n",
    "                                                                force_part1_and_part2 = True, \n",
    "                                                                proposed_fold_axis = None) \n",
    "            \n",
    "        if proposed_fold_axis == None and force_part1_and_part2 != True:\n",
    "            return fold_detector_from_two_parts_of_deformed_stratigraphic_part(stratigraphic_part = stratigraphic_part, \n",
    "                                                                                  part1 = part1, part2 = part2,\n",
    "                                                                                  part1_rep = part1_rep, part2_rep = part2_rep,\n",
    "                                                                                    physical_space = physical_space, \n",
    "                                                                knowledge_framework = knowledge_framework,\n",
    "                                                                fold = fold ,                                               \n",
    "                                                                force_part1_and_part2 = True, \n",
    "                                                                proposed_fold_axis = None) \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_neighbors_in_complex_surface(complex_surface):\n",
    "    \n",
    "    \"\"\"\n",
    "    Method to extract neighbors in a complex_surface. It returns a list of pair neighbors if found, else an empty list. \n",
    "    Parameter:\n",
    "    - complex_surface : the cadidate complex_surface\n",
    "    \"\"\" \n",
    "    list_of_neighbors = []\n",
    "    for part in complex_surface.has_Part:\n",
    "        for neighbor in part.has_Neighbor:\n",
    "             pair_of_neighbors = [part, neighbor]\n",
    "             inverse_pair = [neighbor, part]\n",
    "             if pair_of_neighbors not in list_of_neighbors and inverse_pair not in list_of_neighbors:\n",
    "                 list_of_neighbors.append(pair_of_neighbors)\n",
    "    return list_of_neighbors\n",
    "\n",
    "def folding_train_constructor(knowledge_framework, list_of_folds, stratigraphic_part):\n",
    "    return knowledge_framework().Fold_Train(has_Part = list_of_folds, \n",
    "                                               has_Representation = stratigraphic_part.has_Representation)             \n",
    "\n",
    "def folding_train_detector_in_deformed_stratigraphic_part(deformed_stratigraphic_part, physical_space,\n",
    "                                                           knowledge_framework,force_complex_surface = None, ):\n",
    "    \"\"\"\n",
    "    Method to detect mutliple  folding geometry in the representation of a deformed stratigraphic part.\n",
    "        It returns a list of detected fold objects, each fold is associated with its properties and relations. \n",
    "    Parameter:\n",
    "    - deformed_stratigraphic_part : the candidate deformed stratigraphic on which the folding could be detected\n",
    "    - physical_space : the physical space \n",
    "    - knowledge_framework : the knowledge_framework\n",
    "    \"\"\" \n",
    "\n",
    "    complex_representation = deformed_stratigraphic_part.has_Representation[0] if force_complex_surface\\\n",
    "                                         is None else force_complex_surface\n",
    "    if force_complex_surface is None:\n",
    "        if complex_representation.non_planar != True:\n",
    "            raise MalissiaBaseError('problem detected with the representation of the stratigraphic part')\n",
    "    \n",
    "\n",
    "    neighbor_constructor_from_multiple_planars(complex_representation, knowledge_framework, physical_space)\n",
    "    neighbors = extract_neighbors_in_complex_surface(complex_representation)\n",
    "    if len(neighbors) < 2 :\n",
    "        print('Warning no neighbours found in {}, thus no folding train could be detected'.format(complex_representation))\n",
    "        return None\n",
    "    list_of_folds = []\n",
    "    for neighb in neighbors:\n",
    "        neigh1 = neighb[0]\n",
    "        neigh2 = neighb[1]\n",
    "        poss_represented_objects1 = neigh1.is_Representation_Of\n",
    "        poss_represented_objects2 = neigh2.is_Representation_Of\n",
    "        part1 = poss_represented_objects1[0] if len(poss_represented_objects1) == 1 else create_surface_part(knowledge_framework,\n",
    "                                                                        neigh1)\n",
    "        part2 = poss_represented_objects2[0] if len(poss_represented_objects2) == 1 else create_surface_part(knowledge_framework,\n",
    "                                                                        neigh2)\n",
    "\n",
    "        detected_fold = fold_detector_from_two_parts_of_deformed_stratigraphic_part(stratigraphic_part = deformed_stratigraphic_part, \n",
    "                                                    part1 = part1, part2 = part2, part1_rep = neigh1, part2_rep = neigh2,\n",
    "                                                        physical_space= physical_space, knowledge_framework= knowledge_framework)\n",
    "        if detected_fold is not None :\n",
    "            list_of_folds.append(detected_fold)\n",
    "\n",
    "    if len(list_of_folds) < 2 :\n",
    "        return None\n",
    "    else:\n",
    "        return folding_train_constructor(knowledge_framework, list_of_folds, deformed_stratigraphic_part)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foldss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions\n",
    "def condition_for_fold_detector(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            interpretation_situation:InterpretationSituation, \n",
    "                                            **kargs):\n",
    "    \n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for condition_for_fold_detector: knowledge framework\")\n",
    "        return False\n",
    "    if (interpretation_situation is None):\n",
    "        print(\"Wrong condition for condition_for_fold_detector: interpretation situation\")\n",
    "        return False\n",
    "    if  (interpretation_situation.process is None) or (interpretation_situation.process.representation_space is None):\n",
    "        print(\"Wrong condition for condition_for_fold_detector: physical space\")\n",
    "        return False\n",
    "    if  (interpretation_situation.features is None) or (len(interpretation_situation.features) != 1 ):\n",
    "        print(\"Wrong condition for condition_for_fold_detector: features\")\n",
    "        return False\n",
    "    \n",
    "    feature = interpretation_situation.features[0]\n",
    "    rep = feature.has_Representation\n",
    "    if len(rep) != 1 :\n",
    "        print(\"Wrong condition for condition_for_fold_detector: feature representation\")\n",
    "        return False\n",
    "    if not isinstance(rep[0], knowledge_framework().Complex_Surface) or rep[0].non_planar != True:\n",
    "        print(\"Wrong condition for condition_for_fold_detector: feature representation\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def condition_fold_constructor_from_dipping_anomaly(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            interpretation_situation:InterpretationSituation, \n",
    "                                            **kargs):\n",
    "    \n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for condition_fold_constructor_dipping_anomaly: knowledge framework\")\n",
    "        return False\n",
    "    \n",
    "    if (interpretation_situation is None):\n",
    "        print(\"Wrong condition for condition_fold_constructor_dipping_anomaly: interpretation situation\")\n",
    "        return False\n",
    "    if  (interpretation_situation.process is None) or (interpretation_situation.process.representation_space is None):\n",
    "        print(\"Wrong condition for condition_fold_constructor_dipping_anomaly: physical space\")\n",
    "        return False\n",
    "    if  (interpretation_situation.features is None) or (len(interpretation_situation.features) == 0):\n",
    "        print(\"Wrong condition for condition_fold_constructor_dipping_anomaly: features\")\n",
    "        return False\n",
    "    \n",
    "    explained_class = knowledge_framework().DippingStratigraphyAnomaly\n",
    "    explainable_anomalies = knowledge_framework.filter_explainable_features_by_type( interpretation_situation.features, \n",
    "                                                                                   type_of_feature = explained_class)\n",
    "    \n",
    "    if len(explainable_anomalies) == 0 :\n",
    "        print(\"Wrong condition for condition_fold_constructor_dipping_anomaly: no DippingStratigraphyAnomaly\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def condition_fold_constructor_internal_dip_variation_anomaly_two_parts(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            interpretation_situation:InterpretationSituation, \n",
    "                                            **kargs):\n",
    "\n",
    "    # general check\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for fold  constructor InternalDipVariationAnomalyTwoParts: knowledge framework\")\n",
    "        return False\n",
    "    \n",
    "    if (interpretation_situation is None):\n",
    "        print(\"Wrong condition for fold constructor InternalDipVariationAnomalyTwoParts: interpretation situation\")\n",
    "        return False\n",
    "    if  (interpretation_situation.process is None) or (interpretation_situation.process.representation_space is None):\n",
    "        print(\"Wrong condition for fold constructor  InternalDipVariationAnomalyTwoParts: physical space\")\n",
    "        return False\n",
    "    if  (interpretation_situation.features is None) or (len(interpretation_situation.features) == 0):\n",
    "        print(\"Wrong condition for fold constructor InternalDipVariationAnomalyTwoParts: features\")\n",
    "        return False\n",
    "    \n",
    "    explained_class = knowledge_framework().InternalDipVariationAnomalyTwoParts\n",
    "    explainable_anomalies = knowledge_framework.filter_explainable_features_by_type( interpretation_situation.features, \n",
    "                                                                                   type_of_feature = explained_class)\n",
    "    \n",
    "    if len(explainable_anomalies) == 0 :\n",
    "        print(\"Wrong condition for condition_fold_constructor_internal_dip_variation_anomaly_two_parts: no InternalDipVariationAnomalyTwoParts\")\n",
    "        return False\n",
    "\n",
    "    # in any other cases\n",
    "    return True\n",
    "\n",
    "def condition_folding_train_detector_from_internal_dip_variation_anomaly_mutli_parts(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            interpretation_situation:InterpretationSituation, \n",
    "                                            **kargs):\n",
    "    # general check\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for fold  constructor InternalDipVariationAnomalyMultiParts: knowledge framework\")\n",
    "        return False\n",
    "    \n",
    "    if (interpretation_situation is None):\n",
    "        print(\"Wrong condition for fold constructor InternalDipVariationAnomalyMultiParts: interpretation situation\")\n",
    "        return False\n",
    "    if  (interpretation_situation.process is None) or (interpretation_situation.process.representation_space is None):\n",
    "        print(\"Wrong condition for fold constructor  InternalDipVariationAnomalyMultiParts: physical space\")\n",
    "        return False\n",
    "    if  (interpretation_situation.features is None) or (len(interpretation_situation.features) == 0):\n",
    "        print(\"Wrong condition for fold constructor InternalDipVariationAnomalyMultiParts: features\")\n",
    "        return False\n",
    "    \n",
    "    # anomaly check\n",
    "    explained_class = knowledge_framework().InternalDipVariationAnomalyMultiParts\n",
    "    explainable_anomalies = knowledge_framework.filter_explainable_features_by_type( interpretation_situation.features, \n",
    "                                                                                   type_of_feature = explained_class)\n",
    "    \n",
    "    if len(explainable_anomalies) == 0 :\n",
    "        print(\"Wrong condition for condition_folding_train_detector_from_internal_dip_variation_anomaly_mutli_parts: no InternalDipVariationAnomalyTwoParts\")\n",
    "        return False\n",
    "\n",
    "    # in any other cases\n",
    "    return True\n",
    "\n",
    "def condition_fold_constructor_dip_variation_anomaly(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            interpretation_situation:InterpretationSituation, \n",
    "                                            **kargs):\n",
    "    \n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for fold  constructor: knowledge framework\")\n",
    "        return False\n",
    "    \n",
    "    if (interpretation_situation is None):\n",
    "        print(\"Wrong condition for fold constructor: interpretation situation\")\n",
    "        return False\n",
    "    if  (interpretation_situation.process is None) or (interpretation_situation.process.representation_space is None):\n",
    "        print(\"Wrong condition for fold constructor: physical space\")\n",
    "        return False\n",
    "    if  (interpretation_situation.features is None) or (len(interpretation_situation.features) == 0):\n",
    "        print(\"Wrong condition for fold constructor: features\")\n",
    "        return False\n",
    "    \n",
    "    # anomaly check\n",
    "    explained_class = knowledge_framework().DipVariationAnomaly\n",
    "    explainable_anomalies = knowledge_framework.filter_explainable_features_by_type( interpretation_situation.features, \n",
    "                                                                                   type_of_feature = explained_class)\n",
    "    \n",
    "    if len(explainable_anomalies) == 0 :\n",
    "        print(\"Wrong condition for condition_fold_constructor_dip_variation_anomaly: no DipVariationAnomaly\")\n",
    "        return False\n",
    "\n",
    "    # in any other cases\n",
    "    return True\n",
    "\n",
    "\n",
    "# constructors\n",
    "\n",
    "def fold_constructor_from_dipping_anomaly(interpretation_situation:InterpretationSituation, \n",
    "                                            knowledge_framework = None,\n",
    "                                            **kargs\n",
    "                                        ):\n",
    "    knowledge_framework = knowledge_framework if knowledge_framework is not None else interpretation_situation.knowledge_framework\n",
    "    \n",
    "    physical_space = get_physical_space_in_constructor(interpretation_situation, physical_space= None)\n",
    "    \n",
    "    explained_class = knowledge_framework().DippingStratigraphyAnomaly\n",
    "    explainable_anomalies = knowledge_framework.filter_explainable_features_by_type( interpretation_situation.features, \n",
    "                                                                                   type_of_feature = explained_class)\n",
    "    anomaly = explainable_anomalies[0]\n",
    "    stratigraphic_part = anomaly.is_Related_To[0]\n",
    "    return fold_constructor_one_stratigraphic_part(candidate_stratigraphic_part=stratigraphic_part, \n",
    "                                                   knowledge_framework = knowledge_framework,\n",
    "                                                     physical_space = physical_space, \n",
    "                                                      dipping_anomaly= anomaly )\n",
    "\n",
    "\n",
    "def fold_constructor_from_dip_variation_anomaly(\n",
    "                                            interpretation_situation:InterpretationSituation, \n",
    "                                            knowledge_framework = None,\n",
    "                                            **kargs\n",
    "                                        ):\n",
    "    knowledge_framework = knowledge_framework if knowledge_framework is not None else interpretation_situation.knowledge_framework\n",
    "    physical_space = get_physical_space_in_constructor(interpretation_situation, physical_space= None)\n",
    "    \n",
    "    explained_class = knowledge_framework().DipVariationAnomaly\n",
    "    explainable_anomalies = knowledge_framework.filter_explainable_features_by_type( interpretation_situation.features, \n",
    "                                                                                   type_of_feature = explained_class)\n",
    "    anomaly = explainable_anomalies[0]\n",
    "    stratigraphic_part1 = anomaly.is_Related_To[0]\n",
    "    stratigraphic_part2 = anomaly.is_Related_To[1]\n",
    "    return fold_constructor_from_two_stratigraphic_parts(candidate_stratigraphic_part1 = stratigraphic_part1, \n",
    "                                                         candidate_stratigraphic_part2 = stratigraphic_part2,\n",
    "                                                   knowledge_framework = knowledge_framework,\n",
    "                                                     physical_space = physical_space, \n",
    "                                                      dipping_anomaly= anomaly ) \n",
    "\n",
    "def fold_constructor_from_stratigraphic_part_planar(interpretation_situation:InterpretationSituation, \n",
    "                                            knowledge_framework = None,\n",
    "                                            **kargs\n",
    "                                        ):\n",
    "    knowledge_framework = knowledge_framework if knowledge_framework is not None else interpretation_situation.knowledge_framework\n",
    "\n",
    "    physical_space = get_physical_space_in_constructor(interpretation_situation, physical_space= None)\n",
    "    features = list(interpretation_situation.features)\n",
    "    if len(features) != 1 :\n",
    "            raise MalissiaNotImplementedYet('warning: method for building fold for more than two features is not implemented yet')\n",
    "    \n",
    "    ## todo must add conditions for the stratigraphic_part type and representation\n",
    "\n",
    "    ## todo must add fold detector (returns foldS !)\n",
    "    return fold_constructor_one_stratigraphic_part(features[0], knowledge_framework, physical_space)\n",
    "        \n",
    "def fold_constructor_from_stratigraphic_parts(interpretation_situation:InterpretationSituation, \n",
    "                                            knowledge_framework = None,\n",
    "                                            **kargs\n",
    "                                        ):\n",
    "    knowledge_framework = knowledge_framework if knowledge_framework is not None else interpretation_situation.knowledge_framework\n",
    "    physical_space = get_physical_space_in_constructor(interpretation_situation)\n",
    "    features = list(interpretation_situation.features)\n",
    "    if len(features) != 2  :\n",
    "            raise MalissiaNotImplementedYet('warning: method for building fold for more than two features is not implemented yet')\n",
    "    \n",
    "    ## todo must add conditions for the stratigraphic_part type and representation\n",
    "\n",
    "    ## todo must add fold detector (returns foldS !)\n",
    "\n",
    "    return fold_constructor_from_two_stratigraphic_parts(candidate_stratigraphic_part1= features[0], \n",
    "                                                             candidate_stratigraphic_part2= features[1], \n",
    "                                                             physical_space= physical_space, \n",
    "                                                             knowledge_framework= knowledge_framework)\n",
    "        \n",
    "\n",
    "\n",
    "# detectors\n",
    "\n",
    "\n",
    "\n",
    "def fold_detector_from_internal_dip_variation_anomaly_two_parts(interpretation_situation:InterpretationSituation, \n",
    "                                            knowledge_framework = None,\n",
    "                                            **kargs\n",
    "                                        ):\n",
    "    knowledge_framework = knowledge_framework if knowledge_framework is not None else interpretation_situation.knowledge_framework\n",
    "    physical_space = get_physical_space_in_constructor(interpretation_situation, physical_space= None)\n",
    "    \n",
    "    explained_class = knowledge_framework().InternalDipVariationAnomalyTwoParts\n",
    "    explainable_anomalies = knowledge_framework.filter_explainable_features_by_type( interpretation_situation.features, \n",
    "                                                                                   type_of_feature = explained_class)\n",
    "    anomaly = explainable_anomalies[0]\n",
    "    \n",
    "    stratigraphic_part = anomaly.is_Related_To[0]\n",
    "    stratigraphic_part_rep = stratigraphic_part.has_Representation[0]\n",
    "    part1_rep = stratigraphic_part_rep.has_Part[0]\n",
    "    part2_rep = stratigraphic_part_rep.has_Part[1]\n",
    "    poss_represented_objects1 = part1_rep.is_Representation_Of\n",
    "    poss_represented_objects2 = part2_rep.is_Representation_Of\n",
    "\n",
    "    # creating stratigraphic parts from the complex_surface if they do not exist\n",
    "    part1 = poss_represented_objects1[0] if len(poss_represented_objects1) == 1 else create_surface_part(knowledge_framework,\n",
    "                                                                         part1_rep,)\n",
    "    part2 = poss_represented_objects2[0] if len(poss_represented_objects2) == 1 else create_surface_part(knowledge_framework,\n",
    "                                                                         part2_rep,)\n",
    "\n",
    "    my_fold =  fold_detector_from_two_parts_of_deformed_stratigraphic_part(stratigraphic_part, part1, part2, physical_space, \n",
    "                                                                knowledge_framework,  part1_rep, part2_rep,\n",
    "                                                                fold = None,\n",
    "                                                                fold_name = None,\n",
    "                                                                force_part1_and_part2 = True, \n",
    "                                                                proposed_fold_axis = None, \n",
    "                                                                )\n",
    "    if my_fold is None or not isinstance(my_fold, knowledge_framework().Chevron_Fold):\n",
    "        raise MalissiaBaseError('the created object {} is not a fold'.format(my_fold))\n",
    "    else:\n",
    "        my_fold.explain.append(anomaly)\n",
    "        return my_fold\n",
    "    \n",
    "    \n",
    "def folding_train_detector_from_internal_dip_variation_anomaly_mutli_parts(interpretation_situation:InterpretationSituation, \n",
    "                                            knowledge_framework = None,\n",
    "                                            **kargs\n",
    "                                        ):\n",
    "    knowledge_framework = knowledge_framework if knowledge_framework is not None else interpretation_situation.knowledge_framework\n",
    "    physical_space = get_physical_space_in_constructor(interpretation_situation, physical_space= None)\n",
    "    \n",
    "    explained_class = knowledge_framework().InternalDipVariationAnomalyMultiParts\n",
    "    explainable_anomalies = knowledge_framework.filter_explainable_features_by_type( interpretation_situation.features, \n",
    "                                                                                   type_of_feature = explained_class)\n",
    "    anomaly = explainable_anomalies[0]\n",
    "    stratigraphic_part = anomaly.is_Related_To[0]\n",
    "    my_folding_train = folding_train_detector_in_deformed_stratigraphic_part(deformed_stratigraphic_part= stratigraphic_part,\n",
    "                                                                 physical_space= physical_space,\n",
    "                                                                 knowledge_framework= knowledge_framework)\n",
    "    \n",
    "    if my_folding_train is None or not isinstance(my_folding_train, knowledge_framework().Fold_Train):\n",
    "        raise MalissiaBaseError('the created object {} is not a folding_train'.format(my_folding_train))\n",
    "    else:\n",
    "        my_folding_train.explain.append(anomaly)\n",
    "        return my_folding_train\n",
    "    \n",
    "# extra methods\n",
    "def fold_constructor_from_situation(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                    physical_space, # todo: add type hin :PhysicalRepresentationSpace,\n",
    "                                    interpretation_situation:InterpretationSituation, \n",
    "                                    **kargs\n",
    "                                ):\n",
    "    anomalies = interpretation_situation.anomalies\n",
    "    if len(anomalies) > 1:\n",
    "        raise MalissiaBaseError('warning: more than one anomaly in the situation for creating a fold, first one chosen.')\n",
    "    \n",
    "    if len(anomalies) == 1 :\n",
    "        return fold_from_anomalies(knowledge_framework, physical_space, interpretation_situation, anomalies[0])\n",
    "    \n",
    "    if len(anomalies) == 0 :\n",
    "        features = list(interpretation_situation.features)\n",
    "        if len(features) == 0:\n",
    "            raise MalissiaBaseError(\"The situation doesn't contain any selected feature so Fold cannot be created from it.\")\n",
    "        if len(features) > 2:\n",
    "            raise MalissiaNotImplementedYet('warning: method for building fold for more than two features is not implemented yet')\n",
    "        else: \n",
    "            return fold_constructor_from_stratigraphic_parts(knowledge_framework, interpretation_situation)\n",
    "\n",
    "def fold_from_anomalies(knowledge_framework,\n",
    "                        physical_space,\n",
    "                        interpretation_situation,  \n",
    "                        **kargs):\n",
    "    \n",
    "    anomaly = interpretation_situation.anomalies[0]\n",
    "\n",
    "    if not isinstance(anomaly,knowledge_framework().DippingStratigraphyAnomaly):\n",
    "        return fold_constructor_from_dipping_anomaly(knowledge_framework,\n",
    "                                    physical_space,\n",
    "                                    interpretation_situation, anomaly,)\n",
    "    if not isinstance(anomaly,knowledge_framework().DipVariationAnomaly):\n",
    "        return fold_constructor_from_dip_variation_anomaly(knowledge_framework,\n",
    "                                    physical_space,\n",
    "                                    interpretation_situation, anomaly)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "if \"Chevron_Fold\" in GeologicalKnowledgeFramework.registered_constructors:\n",
    "    del GeologicalKnowledgeFramework.registered_constructors[\"Chevron_Fold\"]\n",
    "\n",
    "\n",
    "GeologicalKnowledgeFramework.register_constructor(\"Chevron_Fold\", \n",
    "                                                  constructor= fold_detector_from_internal_dip_variation_anomaly_two_parts,\n",
    "                                                  condition= condition_fold_constructor_internal_dip_variation_anomaly_two_parts)\n",
    "\n",
    "\n",
    "GeologicalKnowledgeFramework.register_constructor(\"Chevron_Fold\", \n",
    "                                                  constructor= fold_constructor_from_dipping_anomaly,\n",
    "                                                  condition= condition_fold_constructor_from_dipping_anomaly)\n",
    "\n",
    "\n",
    "GeologicalKnowledgeFramework.register_constructor(\"Chevron_Fold\", \n",
    "                                                  constructor= fold_constructor_from_dip_variation_anomaly,\n",
    "                                                  condition= condition_fold_constructor_dip_variation_anomaly)\n",
    "\n",
    "\n",
    "if \"Fold_Train\" in GeologicalKnowledgeFramework.registered_constructors:\n",
    "    del GeologicalKnowledgeFramework.registered_constructors[\"Fold_Train\"]\n",
    "\n",
    "GeologicalKnowledgeFramework.register_constructor(\"Fold_Train\", \n",
    "                                                  constructor= folding_train_detector_from_internal_dip_variation_anomaly_mutli_parts,\n",
    "                                                  condition= condition_folding_train_detector_from_internal_dip_variation_anomaly_mutli_parts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fold anomalies and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_explained_objects_anomaly_constructor(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                       explaining_object):\n",
    "    \n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for missing_explained_objects : knowledge framework\")\n",
    "    if (explaining_object is None) :\n",
    "        raise MalissiaBaseError(\"Wrong condition for missing_explained_objects : explaining_object is None\")\n",
    "    \n",
    "    anomaly = knowledge_framework().MissingExplainedObjectsAnomaly(is_Related_To = explaining_object)\n",
    "    return anomaly\n",
    "\n",
    "def fold_anomaly_related_to_representation_constructor(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                       fold, complex_surface, complex_surface_specific_anomaly):\n",
    "    \n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for missing_explained_objects : knowledge framework\")\n",
    "    if (fold is None) or (complex_surface is None) or (complex_surface_specific_anomaly is None) :\n",
    "        raise MalissiaBaseError(\"Wrong condition for missing_explained_objects : one or multiple anomalic objects (fold, complex_surface, complex_surface_specific_anomaly) are None\")\n",
    "    \n",
    "    anomaly = knowledge_framework().FoldRepresentationAnomaly(is_Related_To_Fold = fold, is_Related_To_Complex_Surface = complex_surface, \n",
    "                                                                   is_Related_To_Anomaly = complex_surface_specific_anomaly)\n",
    "    return anomaly\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_fold_explanation_consistency(fold, knowledge_framework, **kargs):\n",
    "    # todo :  generate coverage anomalies (for points projecting outside of the explaining object)\n",
    "    print('will pass explanation check for folds')\n",
    "    # get representation\n",
    "    rep = fold.has_Representation\n",
    "    if (rep is None) or (len(rep) == 0):\n",
    "        return [ missing_representation_anomaly_constructor(\n",
    "            knowledge_framework= knowledge_framework,\n",
    "            object= fold)]\n",
    "    \n",
    "    if (len(rep) == 1) and (isinstance(rep[0], knowledge_framework().Complex_Surface)):\n",
    "        explained_objects = fold.explain\n",
    "        if len(explained_objects) < 1:\n",
    "            return [missing_explained_objects_anomaly_constructor(knowledge_framework, fold)]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    elif not isinstance(rep[0], knowledge_framework().Complex_Surface):\n",
    "        print('checking error : representations of the fold is not of the type complex_surf')\n",
    "        raise MalissiaNotImplementedYet(\"representations of the fold is not of the type complex_surface\")\n",
    "    else: \n",
    "        print('passing all explanation checks')\n",
    "        return []\n",
    "    \n",
    "\n",
    "def evaluate_fold_internal_consistency(fold, knowledge_framework, physical_space, max_dip= 4, **kargs):\n",
    "    \n",
    "    # get representation\n",
    "    rep = fold.has_Representation\n",
    "    if (rep is None) or (len(rep) == 0):\n",
    "        return [missing_representation_anomaly_constructor(\n",
    "            knowledge_framework= knowledge_framework,\n",
    "            object= fold)]\n",
    "    \n",
    "    complex_surface = rep[0]\n",
    "    # if complex surface has two parts and parallel ==> this surface must be transformed to a simple planar surface\n",
    "    if complex_surface.non_planar != False and complex_surface.non_planar != True:\n",
    "        raise MalissiaBaseError('the complex_surface non_planar_property must be given \"True\" or \"False\"')\n",
    "    if len(complex_surface.has_Part) <2 :\n",
    "        raise MalissiaBaseError('the complex_surface must have at least two parts')\n",
    "    if len(complex_surface.has_Part) >2 and complex_surface.non_planar == False:\n",
    "        raise MalissiaNotImplementedYet('the actual algorithm handle only a non deformed complex_surface of exactly two parts') \n",
    "    \n",
    "    if len(complex_surface.has_Part) ==2 and complex_surface.non_planar == False:\n",
    "        raise MalissiaBaseError('the complex_surface must have that represent a fold must not be planar') \n",
    "    \n",
    "    if len(complex_surface.has_Part) >2 and complex_surface.non_planar == True:\n",
    "    # calcualate the common nodes and shared sides\n",
    "        # check if all shared sides are parallel\n",
    "        if not shared_sides_are_parallel(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            #he geometry of the complex surface is not conforme because shared sides are not parallel ==> undo last interpetation\n",
    "            return [fold_anomaly_related_to_representation_constructor(knowledge_framework = knowledge_framework,\n",
    "                                       fold = fold, complex_surface =complex_surface, \n",
    "                                       complex_surface_specific_anomaly= shared_side_not_parallel_anomaly(knowledge_framework, complex_surface)),] \n",
    "                \n",
    "        # check if all shared sides in the complex_surface has the same length in the correct allignement\n",
    "        if not shared_sides_are_equal(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            # raise anomaly that indicates that this complex_surface must be extended ==> look at unmatched neighbor\n",
    "            return [fold_anomaly_related_to_representation_constructor(knowledge_framework = knowledge_framework,\n",
    "                                       fold = fold, complex_surface =complex_surface, \n",
    "                                       complex_surface_specific_anomaly=complex_surface_shared_sides_size_anomaly_constructor(knowledge_framework, complex_surface)),] \n",
    "            \n",
    "        \n",
    "        if not shared_sides_have_uniform_allignement(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            # raise anomaly that indicates that this complex_surface must be extended ==> look at unmatched neighbor\n",
    "            return [fold_anomaly_related_to_representation_constructor(knowledge_framework = knowledge_framework,\n",
    "                                       fold = fold, complex_surface =complex_surface, \n",
    "                                       complex_surface_specific_anomaly=complex_surface_shared_sides_size_anomaly_constructor(knowledge_framework, complex_surface)),] \n",
    "            \n",
    "        \n",
    "        #check neighbors number and arrangement in the complex_surface\n",
    "        if not check_complex_surface_neighbor_number_and_arrangement(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            # the geometry of the complex surface is not conforme because the number of neighbors is not adequate with\n",
    "                                                            ## the number of parts (n-1)*2 ==> undo last interpetation\n",
    "            return [fold_anomaly_related_to_representation_constructor(knowledge_framework = knowledge_framework,\n",
    "                                       fold = fold, complex_surface =complex_surface, \n",
    "                                       complex_surface_specific_anomaly=complex_surface_neighbor_number_anomaly(knowledge_framework, complex_surface)),] \n",
    "            \n",
    "         \n",
    "        # check chirality between all neighbors\n",
    "        for pair in extract_neighbors_in_complex_surface(complex_surface) :\n",
    "               if not check_chirality_projection(*pair, knowledge_framework, physical_space ):\n",
    "                   return [fold_anomaly_related_to_representation_constructor(knowledge_framework = knowledge_framework,\n",
    "                                       fold = fold, complex_surface =complex_surface, \n",
    "                                       complex_surface_specific_anomaly=polarity_anomaly_constructor(knowledge_framework, parts = pair)),] \n",
    "    \n",
    "    else:\n",
    "        # if all checks come verified the function will return a an empty list \n",
    "        print('passing all internal checks')\n",
    "        return []\n",
    "\n",
    "\n",
    "GeologicalKnowledgeFramework.register_internal_consistency_evaluator(\"Chevron_Fold\", \n",
    "                                                  method=evaluate_fold_internal_consistency)\n",
    "GeologicalKnowledgeFramework.register_explanation_consistency_evaluator(\"Chevron_Fold\", \n",
    "                                                  method=evaluate_fold_explanation_consistency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folding train evluators\n",
    "                          \n",
    "def folding_train_anomaly_related_to_representation_constructor(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                       folding_train, complex_surface, complex_surface_specific_anomaly):\n",
    "    \n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise MalissiaBaseError(\"Wrong condition for missing_explained_objects : knowledge framework\")\n",
    "    if (folding_train is None) or (complex_surface is None) or (complex_surface_specific_anomaly is None) :\n",
    "        raise MalissiaBaseError(\"Wrong condition for missing_explained_objects : one or multiple anomalic objects (fold, complex_surface, complex_surface_specific_anomaly) are None\")\n",
    "    \n",
    "    anomaly = knowledge_framework().FoldTrainRepresentationAnomaly(is_Related_To_Folding_Train = folding_train, is_Related_To_Complex_Surface = complex_surface, \n",
    "                                                                   is_Related_To_Anomaly = complex_surface_specific_anomaly)\n",
    "    return anomaly\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_folding_train_explanation_consistency(folding_train, knowledge_framework, **kargs):\n",
    "    # todo :  generate coverage anomalies (for points projecting outside of the explaining object)\n",
    "    print('will pass explanation check for folds')\n",
    "    # get representation\n",
    "    rep = folding_train.has_Representation\n",
    "    if (rep is None) or (len(rep) == 0):\n",
    "        return [ missing_representation_anomaly_constructor(\n",
    "            knowledge_framework= knowledge_framework,\n",
    "            object= folding_train)]\n",
    "    \n",
    "    if (len(rep) == 1) and (isinstance(rep[0], knowledge_framework().Complex_Surface)):\n",
    "        explained_objects = folding_train.explain\n",
    "        if len(explained_objects) < 1:\n",
    "            return [missing_explained_objects_anomaly_constructor(knowledge_framework, folding_train)]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    elif not isinstance(rep[0], knowledge_framework().Complex_Surface):\n",
    "        print('checking error : representations of the fold is not of the type complex_surf')\n",
    "        raise MalissiaNotImplementedYet(\"representations of the fold is not of the type complex_surface\")\n",
    "    else: \n",
    "        print('passing all explanation checks')\n",
    "        return []\n",
    "    \n",
    "\n",
    "def evaluate_folding_train_internal_consistency(folding_train, knowledge_framework, physical_space, max_dip= 4, **kargs):\n",
    "    \n",
    "    # get representation\n",
    "    rep = folding_train.has_Representation\n",
    "    if (rep is None) or (len(rep) == 0):\n",
    "        return [missing_representation_anomaly_constructor(\n",
    "            knowledge_framework= knowledge_framework,\n",
    "            object= folding_train)]\n",
    "    \n",
    "    complex_surface = rep[0]\n",
    "    # if complex surface has two parts and parallel ==> this surface must be transformed to a simple planar surface\n",
    "    if complex_surface.non_planar != False and complex_surface.non_planar != True:\n",
    "        raise MalissiaBaseError('the complex_surface non_planar_property must be given \"True\" or \"False\"')\n",
    "    if len(complex_surface.has_Part) <2 :\n",
    "        raise MalissiaBaseError('the complex_surface must have at least two parts')\n",
    "    if len(complex_surface.has_Part) >2 and complex_surface.non_planar == False:\n",
    "        raise MalissiaNotImplementedYet('the actual algorithm handle only a non deformed complex_surface of exactly two parts') \n",
    "    \n",
    "    if len(complex_surface.has_Part) ==2 and complex_surface.non_planar == False:\n",
    "        raise MalissiaBaseError('the complex_surface must have that represent a fold must not be planar') \n",
    "    \n",
    "    if len(complex_surface.has_Part) >2 and complex_surface.non_planar == True:\n",
    "    # calcualate the common nodes and shared sides\n",
    "        # check if all shared sides are parallel\n",
    "        if not shared_sides_are_parallel(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            #he geometry of the complex surface is not conforme because shared sides are not parallel ==> undo last interpetation\n",
    "            return [folding_train_anomaly_related_to_representation_constructor(knowledge_framework = knowledge_framework,\n",
    "                                       folding_train = folding_train, complex_surface =complex_surface, \n",
    "                                       complex_surface_specific_anomaly=shared_side_not_parallel_anomaly(knowledge_framework, complex_surface)),] \n",
    "                \n",
    "        # check if all shared sides in the complex_surface has the same length in the correct allignement\n",
    "        if not shared_sides_are_equal(complex_surface  = complex_surface,knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            # raise anomaly that indicates that this complex_surface must be extended ==> look at unmatched neighbor\n",
    "            return [folding_train_anomaly_related_to_representation_constructor(knowledge_framework = knowledge_framework,\n",
    "                                       folding_train = folding_train, complex_surface =complex_surface, \n",
    "                                       complex_surface_specific_anomaly=complex_surface_shared_sides_size_anomaly_constructor(knowledge_framework, complex_surface)),] \n",
    "            \n",
    "        \n",
    "        if not shared_sides_have_uniform_allignement(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            # raise anomaly that indicates that this complex_surface must be extended ==> look at unmatched neighbor\n",
    "            \n",
    "            return [folding_train_anomaly_related_to_representation_constructor(knowledge_framework = knowledge_framework,\n",
    "                                       folding_train = folding_train, complex_surface =complex_surface, \n",
    "                                       complex_surface_specific_anomaly=complex_surface_shared_sides_size_anomaly_constructor(knowledge_framework, complex_surface)),] \n",
    "            \n",
    "        #check neighbors number and arrangement in the complex_surface\n",
    "        if not check_complex_surface_neighbor_number_and_arrangement(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "            # the geometry of the complex surface is not conforme because the number of neighbors is not adequate with\n",
    "                                                            ## the number of parts (n-1)*2 ==> undo last interpetation\n",
    "            return [folding_train_anomaly_related_to_representation_constructor(knowledge_framework = knowledge_framework,\n",
    "                                       folding_train = folding_train, complex_surface =complex_surface, \n",
    "                                       complex_surface_specific_anomaly=complex_surface_neighbor_number_anomaly(knowledge_framework, complex_surface)),] \n",
    "        \n",
    "         \n",
    "        # check chirality between all neighbors\n",
    "        for pair in extract_neighbors_in_complex_surface(complex_surface) :\n",
    "               if not check_chirality_projection(*pair, knowledge_framework, physical_space ):\n",
    "                   return [folding_train_anomaly_related_to_representation_constructor(knowledge_framework = knowledge_framework,\n",
    "                                       folding_train = folding_train, complex_surface =complex_surface, \n",
    "                                       complex_surface_specific_anomaly=polarity_anomaly_constructor(knowledge_framework, parts = pair)),] \n",
    "        return []\n",
    "    else:\n",
    "        # if all checks come verified the function will return a an empty list \n",
    "        return []\n",
    "\n",
    "\n",
    "GeologicalKnowledgeFramework.register_internal_consistency_evaluator(\"Fold_Train\", \n",
    "                                                  method=evaluate_folding_train_internal_consistency)\n",
    "GeologicalKnowledgeFramework.register_explanation_consistency_evaluator(\"Fold_Train\", \n",
    "                                                  method=evaluate_folding_train_explanation_consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# compute and construct\n",
    "\n",
    "def list_coords_to_list_nodes(list_of_coord, knowledge_framewrok, coord_labels):\n",
    "    return [create_nodes_from_coords(knowledge_framewrok, coord_i, \n",
    "                                     coord_labels) for coord_i in list_of_coord]\n",
    "\n",
    "def list_of_planars_from_nodes(list_nodes, knowledge_framework):\n",
    "    list_of_planars = []\n",
    "    for node_i in list_nodes:\n",
    "        plan_i = create_planar_surface(knowledge_framework, node_i)\n",
    "        set_center(knowledge_framework, plan_i)\n",
    "        set_attitude(knowledge_framework, plan_i)\n",
    "        list_of_planars.append(plan_i)\n",
    "    return list_of_planars\n",
    "    \n",
    "def list_of_parts_from_nodes(list_of_planars, knowledge_framework):\n",
    "    return [create_surface_part(knowledge_framework, node_i) for node_i in list_of_planars]\n",
    "\n",
    "\n",
    "def complex_surface_parts_annexation(complex_surface, new_parts, \n",
    "                                     change_name_of_new_parts = False):\n",
    "    \"\"\"Method to annexe parts to an existing complex_surface . It returns the complex_surface after including the new parts.\n",
    "    Parameter:\n",
    "    - complex_surface:  the candidate complex_surface\n",
    "    - new_parts:  a list of parts to be included\n",
    "    - change_name_of_new_parts : a boolean by default is False, to force changing if yes the name of the new parts\"\"\"  \n",
    "    if change_name_of_new_parts is False: \n",
    "        for part in new_parts:\n",
    "            if part not in complex_surface.has_Part:\n",
    "                complex_surface.has_Part.append(part)\n",
    "        return complex_surface\n",
    "    else:\n",
    "        nb_of_existing_parts = len(complex_surface.has_Part)\n",
    "        for part in new_parts:\n",
    "            if part not in complex_surface.has_Part:\n",
    "                nb_of_existing_parts += 1\n",
    "                part.name =  'part_{}_of_{}'.format(nb_of_existing_parts, complex_surface.name)\n",
    "                complex_surface.has_Part.append(part)\n",
    "        return complex_surface    \n",
    "\n",
    "def find_equivalent_list(main_list, list_of_lists, tolerance=0.5):\n",
    "    if not isinstance(main_list, list):\n",
    "        raise MalissiaBaseError('main_list {} is not of type list'.format(main_list))\n",
    "    for sublist in list_of_lists:\n",
    "        if all(abs(a - b) <= tolerance for a, b in zip(main_list, sublist)):\n",
    "            return sublist\n",
    "    return None\n",
    "\n",
    "\n",
    "def are_sets_close(set1, set2, tolerance = 0.3):\n",
    "    # Check if sets have the same length\n",
    "    if len(set1) != len(set2):\n",
    "        return False\n",
    "\n",
    "    # Iterate through elements and compare with tolerance\n",
    "    for el1, el2 in zip(set1, set2):\n",
    "        if abs(el1 - el2) > tolerance:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def neighbor_constructor_from_two_planars(part1, part2, \n",
    "                                          knowledge_framework,  physical_space,\n",
    "                                          tolerance = 0.3, \n",
    "                                          complex_surface = None,\n",
    "                                            same_complex_surface_condition = True):\n",
    "    \n",
    "    \"\"\"Method to constrtuct neighbors of two parts in a complex_surface if they have exactly two shared nodes 'nodes with the same coordaintes'.\n",
    "     . It assign the neighbors if the conditions are met but it does not return anything .\n",
    "    Parameter:\n",
    "    - part1 : first plane \n",
    "    - part2 : second plane \n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework = the \n",
    "    - tolerance = a tolerance value to define the distance at which two nodes are considered as superposed 'the instruction to use  \n",
    "    - complex_surface = the complex_surface if it exists, or the method will generate new one\n",
    "    - same_complex_surface_condition = by default True, a boolean to ensure that the two parts to be neighbors must be part of the same complex_surface\"\"\"  \n",
    "    \n",
    "    if same_complex_surface_condition == True:\n",
    "        if complex_surface not in list(set(part2.is_Part_Of).intersection(part1.is_Part_Of)):\n",
    "            raise MalissiaBaseError('the {} and {} are not in the same complex_surface and \\\n",
    "                                    same_complex_surface_condition is enforced \"True\"'.format(part1, part2))\n",
    "    \n",
    "    \n",
    "    commons = {}\n",
    "    com_init = 1\n",
    "    for no_i in part1.has_Representation:\n",
    "        val_i = physical_space.get_object_coordinates(no_i,'nodes')[0]\n",
    "        for idx_ii, no_ii in enumerate(part2.has_Representation):\n",
    "            val_ii = physical_space.get_object_coordinates(no_ii,'nodes')[0]\n",
    "            if are_sets_close(set(val_i), set(val_ii), tolerance) :\n",
    "                commons['common_node{}'.format(com_init)] = [no_i, no_ii, idx_ii]\n",
    "                com_init += 1\n",
    "    \n",
    "    #common_nodes = [node_i for node_i in nodes1 if node_i in nodes2] # if an error here node_i should be also a list\n",
    "    common_nodes_nb = len(commons)\n",
    "    if common_nodes_nb == 3 or common_nodes_nb == 4:\n",
    "        print(part1, part2)\n",
    "        raise MalissiaBaseError('parts to be neighbors must not be superposed')\n",
    "    if common_nodes_nb == 2:\n",
    "        if part2 not in part1.has_Neighbor:\n",
    "            part1.has_Neighbor.append(part2)\n",
    "        if part1 not in part2.has_Neighbor: # this block is redundant as the relation must be symmetric\n",
    "            part2.has_Neighbor.append(part1)\n",
    "        \n",
    "\n",
    "        intersection_set = set(part2.has_Shared_Side).intersection(part1.has_Shared_Side)\n",
    "        if not intersection_set:\n",
    "            n1_coor = physical_space.get_object_coordinates( object= commons['common_node1'][0], \n",
    "                                                            kind= \"nodes\")[0]\n",
    "            n2_coor = physical_space.get_object_coordinates( object= commons['common_node2'][0], \n",
    "                                                            kind= \"nodes\")[0]\n",
    "            side_size = float(np.linalg.norm(np.array(n1_coor)-np.array(n2_coor)))\n",
    "\n",
    "            shared_side = knowledge_framework().Side(has_End_Points = [commons['common_node1'][0], \n",
    "                                                                       commons['common_node2'][0]], \n",
    "                                                                        size = [side_size])  \n",
    "            part1.has_Shared_Side.append(shared_side)\n",
    "            part2.has_Shared_Side.append(shared_side)\n",
    "        \n",
    "        \n",
    "\n",
    "        # change commonde nodes in the part2.rep and in the pertinent nodes by nodes of part1 \n",
    "        part2.has_Representation[commons['common_node1'][2]] = commons['common_node1'][0]\n",
    "        part2.has_Representation[commons['common_node2'][2]] = commons['common_node2'][0]\n",
    "        for n in range(0,4):\n",
    "            node_i = getattr(part2, 'has_Node{}'.format(n))\n",
    "            if commons['common_node1'][1] in node_i :\n",
    "                setattr(part2, 'has_Node{}'.format(n), [commons['common_node1'][0]])\n",
    "            elif commons['common_node2'][1] in node_i :\n",
    "                setattr(part2, 'has_Node{}'.format(n), [commons['common_node2'][0]])\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "    # if there is no neighbor, check for possible unmatched neighbors that share an unmatched side\n",
    "    else:\n",
    "        nodes1 = (physical_space.get_object_coordinates( object= part1, kind= \"nodes\")).tolist()\n",
    "        nodes2 = (physical_space.get_object_coordinates( object= part2, kind= \"nodes\")).tolist()\n",
    "        reference_sides = [[nodes1[0],nodes1[1]],\n",
    "                            [nodes1[1],nodes1[2]],\n",
    "                             [nodes1[2],nodes1[3]],\n",
    "                            [nodes1[3],nodes1[0]]]\n",
    "        part2_sides =  [[nodes2[0],nodes2[1]],\n",
    "                            [nodes2[1],nodes2[2]],\n",
    "                             [nodes2[2],nodes2[3]],\n",
    "                            [nodes2[3],nodes2[0]]]\n",
    "\n",
    "        for ref_side in reference_sides:\n",
    "            ref_vec = np.array(ref_side[0])-np.array(ref_side[1])\n",
    "            for side in part2_sides:\n",
    "                vec_side = np.array(side[0])-np.array(side[1])\n",
    "                parallel = are_vectors_parallel(vec_side, ref_vec)\n",
    "                if parallel: \n",
    "                    projec_start_side = compute_point_projection_onto_line(side[0], ref_side[0], ref_vec )\n",
    "                    projec_end_side = compute_point_projection_onto_line(side[1], ref_side[0], ref_vec )\n",
    "                    mag_projec_start_side = np.linalg.norm(np.array(side[0])-np.array(projec_start_side))\n",
    "                    mag_projec_end_side = np.linalg.norm(np.array(side[1])-np.array(projec_end_side))\n",
    "\n",
    "                    #print(projec_start_side, projec_end_side, mag_projec_start_side, mag_projec_end_side)\n",
    "\n",
    "                    if mag_projec_end_side == 0 and mag_projec_start_side == 0 : \n",
    "                        \n",
    "                        if part1 not in part2.has_Unmatched_Neighbor and  part2 not in part1.has_Unmatched_Neighbor:\n",
    "                            # assuming that if part1 is already in unmatched neighbor of part2 or the inverse, they already have unmatched shared sides\n",
    "                            part2.has_Unmatched_Neighbor.append(part1)\n",
    "                            part1.has_Unmatched_Neighbor.append(part2)\n",
    "                            \n",
    "                            share_sides_nb = len(knowledge_framework().Side.instances())\n",
    "                            point_nb =  len(knowledge_framework().Point.instances())\n",
    "\n",
    "                            point_0_part1_side = constructor_point(knowledge_framework, physical_space.coordinate_labels,\n",
    "                                                [float(x) for x in ref_side[0]], name=\"end_point_nb_{}\".format(point_nb+1))\n",
    "                            point_1_part1_side = constructor_point(knowledge_framework, physical_space.coordinate_labels,\n",
    "                                                [float(x) for x in ref_side[1]], name=\"end_point_nb_{}\".format(point_nb+2))\n",
    "                            share_sides_name1 = str('side_nb_{}'.format(share_sides_nb+1))\n",
    "                            side_size1 = float(np.linalg.norm(np.array(ref_side[0])-np.array(ref_side[1])))\n",
    "                            shared_side_1 = knowledge_framework().Side(name = share_sides_name1, has_End_Points = [point_0_part1_side,\n",
    "                                                                                                                point_1_part1_side],\n",
    "                                                                                                                size = [side_size1])  \n",
    "                            if shared_side_1 not in part1.has_Unmatched_Shared_Side :\n",
    "                                part1.has_Unmatched_Shared_Side.append(shared_side_1)\n",
    "\n",
    "                            point_0_part2_side = constructor_point(knowledge_framework, physical_space.coordinate_labels,\n",
    "                                                [float(x) for x in side[0]], name=\"end_point_nb_{}\".format(point_nb+3))\n",
    "                            point_1_part2_side = constructor_point(knowledge_framework, physical_space.coordinate_labels,\n",
    "                                                [float(x) for x in side[1]], name=\"end_point_nb_{}\".format(point_nb+4))\n",
    "                            share_sides_name2 = str('side_nb_{}'.format(share_sides_nb+2))\n",
    "                            side_size2 = float(np.linalg.norm(np.array(side[0])-np.array(side[1])))\n",
    "                            shared_side_2 = knowledge_framework().Side(name = share_sides_name2, has_End_Points = [point_0_part2_side,\n",
    "                                                                                                                point_1_part2_side],\n",
    "                                                                                                            size = [side_size2])  \n",
    "                            if  shared_side_2 not in  part2.has_Unmatched_Shared_Side:\n",
    "                                part2.has_Unmatched_Shared_Side.append(shared_side_2)\n",
    "\n",
    "                            shared_side_1.has_Corresponding_Part = [part2]\n",
    "                            shared_side_2.has_Corresponding_Part = [part1]\n",
    "                            break\n",
    "                        else:\n",
    "                            break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "    \n",
    "def neighbor_constructor_from_multiple_planars(complex_surface, \n",
    "                                                knowledge_framework,\n",
    "                                                physical_space,\n",
    "                                                tolerance = 0.1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Method to constrtuct neighbors of multiple parts in a complex_surface if they have exactly two shared nodes 'nodes with the same coordaintes'.\n",
    "     . It assign the neighbors if the conditions are met but it does not return anything.\n",
    "    Parameter:\n",
    "    - complex_surface:  the candidate complex_surface\n",
    "    - physical_space:  the physical_space\n",
    "    - knowledge_framework : the knowledge_framework\n",
    "    \"\"\"  \n",
    "    \n",
    "    list_of_parts = complex_surface.has_Part\n",
    "    pairs = compute_pairs_of_surfaces_in_complex_surface(list_of_parts)\n",
    "    for pair in pairs:\n",
    "        if pair[0]  in pair[1].has_Neighbor or pair[1] in pair[0].has_Neighbor :\n",
    "            pass\n",
    "        else:\n",
    "            neighbor_constructor_from_two_planars(pair[0], pair[1], \n",
    "                                                  knowledge_framework = knowledge_framework, \n",
    "                                                  physical_space = physical_space ,\n",
    "                                            tolerance = tolerance, complex_surface = complex_surface, \n",
    "                                            same_complex_surface_condition = True)\n",
    "\n",
    "def compute_pairs_of_surfaces_in_complex_surface(list_of_parts):\n",
    "    \"\"\"\n",
    "    Method to compute possible pairs combinations in a list of parts. It returns ta list of combinations.\n",
    "    Parameter:\n",
    "    - list_of_parts:  list of parts\n",
    "    \"\"\"  \n",
    "    return list(itertools.combinations(list_of_parts, 2))\n",
    "\n",
    "def compute_complex_surface_extreme_surfaces(knowledge_framework,\n",
    "                                          complex_surface, return_none = False):\n",
    "    \n",
    "    \"\"\"Method to detect the two extreme surfaces of a complex surface based on the number of neighbors of each part of it \n",
    "    (must be one neighbor for the each extremete face)\"\"\"\n",
    "    list_of_parts = complex_surface.has_Part\n",
    "    if knowledge_framework().Complex_Surface not in complex_surface.is_instance_of:\n",
    "        if return_none == True:\n",
    "            return None\n",
    "        raise MalissiaBaseError('the introduced object is not a complex surface')\n",
    "    elif len(list_of_parts) < 2 :\n",
    "        if return_none == True:\n",
    "            return None\n",
    "        raise MalissiaBaseError('the complex surface has less then two parts')\n",
    "    \n",
    "    elif len(list_of_parts) == 2:\n",
    "         if complex_surface.has_Part[0] in  complex_surface.has_Part[1].has_Neighbor or \\\n",
    "                complex_surface.has_Part[1] in  complex_surface.has_Part[0].has_Neighbor:\n",
    "             return [complex_surface.has_Part[0], complex_surface.has_Part[1]]\n",
    "         else:\n",
    "             print('Warning: the complex_surface {} has two parts but they are not extreme faces, check neighborhood properties'.format(complex_surface.name))\n",
    "             return None \n",
    "    else:\n",
    "        extreme_surfaces = [part for part in list_of_parts if \\\n",
    "                            len(set(part.has_Neighbor).intersection(set(list_of_parts))) == 1]\n",
    "        print(len(extreme_surfaces))\n",
    "        if len(extreme_surfaces) != 2 :\n",
    "            if return_none == True:\n",
    "                return None\n",
    "            raise MalissiaBaseError('the complex_surface has a problem with geometry \"multiple or less than two extreme surfaces\"')\n",
    "        else:\n",
    "            return extreme_surfaces\n",
    "        \n",
    "def compute_extremeties_from_two_extreme_surfaces(extreme_surfaces, physical_space):\n",
    "    \"\"\"\n",
    "    Method to compute 4 nodes that represent extremeties of an existing complex_surface. It returns a list containing two pairs of nodes.\n",
    "    Parameter:\n",
    "    - extreme_surfaces:  a list of exactly two etreme faces of a complex_surface\n",
    "    - physical_space:  the physical_space\n",
    "    \"\"\"  \n",
    "    nodes_of_part1 = (physical_space.get_object_coordinates( object= extreme_surfaces[0],\n",
    "                                                                kind= \"nodes\")).tolist()\n",
    "    part1_neighbor = extreme_surfaces[0].has_Neighbor[0]\n",
    "    nodes_of_part1_neighbor = (physical_space.get_object_coordinates( object= part1_neighbor, \n",
    "                                                                        kind= \"nodes\")).tolist()\n",
    "    # to use tolerance it is possible to use the distance between two points that they could be consdired as one\n",
    "    uncommon_nodes_nb1 = [node_i for node_i in nodes_of_part1 if node_i not in nodes_of_part1_neighbor]\n",
    "    if  len(uncommon_nodes_nb1) != 2:\n",
    "        raise MalissiaBaseError('part1 has more than 2 uncommon nodes with its neighbor')\n",
    "    else:\n",
    "        extremities_0_1 = uncommon_nodes_nb1\n",
    "\n",
    "    nodes_of_part2 = (physical_space.get_object_coordinates( object= extreme_surfaces[1],\n",
    "                                                                kind= \"nodes\")).tolist()\n",
    "    part2_neighbor = extreme_surfaces[1].has_Neighbor[0]\n",
    "    nodes_of_part2_neighbor = (physical_space.get_object_coordinates( object= part2_neighbor, \n",
    "                                                                        kind= \"nodes\")).tolist()\n",
    "    # to use tolerance it is possible to use the distance between two points that they could be consdired as one\n",
    "    uncommon_nodes_nb2 = [node_i for node_i in nodes_of_part2 if node_i not in nodes_of_part2_neighbor]\n",
    "\n",
    "    if  len(uncommon_nodes_nb2) != 2:\n",
    "        raise MalissiaBaseError('part2 has more than 2 uncommon nodes with its neighbor')\n",
    "    else:\n",
    "        extremities_2_3 = uncommon_nodes_nb2\n",
    "    return [extremities_0_1, extremities_2_3]\n",
    "        \n",
    "\n",
    "def complex_surface_two_planars_constructor(knowledge_framework, physical_space,\n",
    "                                          complex_surface, nodes1_temp, \n",
    "                                          nodes2_temp, \n",
    "                                          side_size = None,  force_non_planar_sign = None, \n",
    "                                          proposed_nodes1 = None,\n",
    "                                          proposed_nodes2 = None, \n",
    "                                          forced_p2_v3 = None, \n",
    "                                          forced_p2_v2 = None, \n",
    "                                          forced_p1_v3 = None, \n",
    "                                          forced_p1_v2 = None, ):\n",
    "    \"\"\"\n",
    "    Method to construct two parts if complex surface and assign them to it. It returns the constructed complex_surface.\n",
    "    Parameter:\n",
    "    - knowledge_framework : the knowledge_framework\n",
    "    - physical_space : the physical_space\n",
    "    - complex_surface:  the candidate complex_surface\n",
    "    - nodes1_temp:  temporary np.array of part1 nodes\n",
    "    - nodes2_temp:  temporary np.array of part2 nodes\n",
    "    - side_size : a side size of it exist\n",
    "    - force_non_planar_sign : boolean to force the sign of non_planar dataproperty of the complex_surface\n",
    "    \"\"\"      \n",
    "    nb_nodes = len(knowledge_framework().Point.instances())\n",
    "    coord_labels = physical_space.coordinate_labels\n",
    "\n",
    "    if proposed_nodes1 is None :\n",
    "        nodes1 = [\n",
    "            constructor_point(knowledge_framework, coord_labels,[float(x) for x in nodes1_temp[0]], name=\"Node_{}\".format(nb_nodes+1)),\n",
    "            constructor_point(knowledge_framework, coord_labels,[float(x) for x in nodes1_temp[1]], name=\"Node_{}\".format(nb_nodes+2)),\n",
    "            constructor_point(knowledge_framework, coord_labels,[float(x) for x in nodes1_temp[2]], name=\"Node_{}\".format(nb_nodes+3)),\n",
    "            constructor_point(knowledge_framework, coord_labels,[float(x) for x in nodes1_temp[3]], name=\"Node_{}\".format(nb_nodes+4))\n",
    "            ]\n",
    "    else: nodes1 = proposed_nodes1\n",
    "         \n",
    "    if proposed_nodes2 is None :\n",
    "        nodes2 = [\n",
    "            constructor_point(knowledge_framework, coord_labels,[float(x) for x in nodes2_temp[0]], name=\"Node_{}\".format(nb_nodes+5)),\n",
    "            constructor_point(knowledge_framework, coord_labels,[float(x) for x in nodes2_temp[1]], name=\"Node_{}\".format(nb_nodes+6)),\n",
    "            constructor_point(knowledge_framework, coord_labels,[float(x) for x in nodes2_temp[2]], name=\"Node_{}\".format(nb_nodes+7)),\n",
    "            constructor_point(knowledge_framework, coord_labels,[float(x) for x in nodes2_temp[3]], name=\"Node_{}\".format(nb_nodes+8))\n",
    "            ]\n",
    "    else: nodes2 = proposed_nodes2\n",
    "\n",
    "    nodes2[0] = nodes1[1]\n",
    "    nodes2[1] = nodes1[0]\n",
    "\n",
    "    nodes1[2] =  forced_p1_v2 if forced_p1_v2 is not  None else nodes1[2]\n",
    "    nodes1[3] =  forced_p1_v3 if forced_p1_v3 is not  None else nodes1[3]\n",
    "\n",
    "    nodes2[2] =  forced_p2_v2 if forced_p2_v2 is not  None else nodes2[2]\n",
    "    nodes2[3] =  forced_p2_v3 if forced_p2_v3 is not  None else nodes2[3]\n",
    "    \n",
    "     \n",
    "\n",
    "\n",
    "    nb_parts = len(knowledge_framework().Planar_Surface.instances())\n",
    "    \n",
    "    new_part1 = constructor_planar_surface_from_nodes(knowledge_framework, \n",
    "                                                        nodes1, name= 'part_nb{}'.format(nb_parts+1))\n",
    "\n",
    "    new_part2 = constructor_planar_surface_from_nodes(knowledge_framework, \n",
    "                                                        nodes2, name= 'part_nb{}'.format(nb_parts+2))\n",
    "\n",
    "    share_sides_nb = len(knowledge_framework().Side.instances())+1\n",
    "    point_nb =  len(knowledge_framework().Point.instances())+1\n",
    "\n",
    "    #end_point_0 = constructor_point(knowledge_framework, coord_labels, [float(x) for x in nodes2_temp[0]], name=\"end_point_nb_{}\".format(point_nb))\n",
    "    #end_point_1 = constructor_point(knowledge_framework, coord_labels, [float(x) for x in nodes2_temp[1]], name=\"end_point_nb_{}\".format(point_nb+1))\n",
    "    end_point_0 = new_part2.has_Node0[0]\n",
    "    end_point_1 = new_part2.has_Node1[0]\n",
    "    share_sides_name = str('shared_side_nb_{}'.format(share_sides_nb))\n",
    "    side_size = float(np.linalg.norm(np.array(nodes2_temp[0])-np.array(nodes2_temp[1]))) # if side_size is None else float(side_size)\n",
    "    shared_side = knowledge_framework().Side(name = share_sides_name, has_End_Points = [end_point_0, end_point_1], \n",
    "                                              size = [side_size])  \n",
    "    \n",
    "    new_part1.has_Shared_Side.append(shared_side)\n",
    "    new_part2.has_Shared_Side.append(shared_side)\n",
    "    \n",
    "    complex_surface.has_Part.extend([new_part1, new_part2])\n",
    "    new_part1.has_Neighbor = [new_part2]\n",
    "    new_part2.has_Neighbor = [new_part1] # this should be deleted because the relation is symmetrical\n",
    "    if type(force_non_planar_sign) is bool:\n",
    "        complex_surface.non_planar = force_non_planar_sign     \n",
    "    return complex_surface\n",
    "    \n",
    "def compute_center_nodes(nodes):\n",
    "\n",
    "    center_coord = np.mean(nodes,axis=0)\n",
    "    return center_coord\n",
    "\n",
    "def compute_complex_surface_two_planars(planar1, planar2, \n",
    "                                knowledge_framework, physical_space, \n",
    "                                complex_surface = None, complex_surface_name = None, \n",
    "                                force_projection_of_part_1_extremeties_on_axis = False, \n",
    "                                axis_1 = None, axis_1_origin = None,\n",
    "                                force_projection_of_part_2_extremeties_on_axis = False, \n",
    "                                axis_2 = None, axis_2_origin = None, \n",
    "                                tolerance_angle_parallel = 3, tolerance_angle_coplan = 3,\n",
    "                                forced_p2_v3 = None, \n",
    "                                          forced_p2_v2 = None, \n",
    "                                          forced_p1_v3 = None, \n",
    "                                          forced_p1_v2 = None, tolerance = 0.1):\n",
    "    \"\"\"\n",
    "    Method to compute a complex_surface from two planars. It returns a complex_surface. \n",
    "    It could use an existing complex_surface and add new parts to it.\n",
    "    if the surfaces are parellel, they must be on the same plane and have the same polarity of the method will raise an error and \n",
    "        assigning non_planar quality as false. If they meet these conditions the method generate a complex_surface with new parts.\n",
    "    If the surfaces are not parallel, the method will create a complex_surface by computing an intersection line, direction and \n",
    "        calculating nodes and assining non_planar quality as True.\n",
    "    if the first two parts are adjacent 'have 2 common nodes' the method will create a complex_surface with the candidate parts as parts of it.\n",
    "    The method could force the projection of the constructed nodes extremeties of one or both parts on an axis, for that force_projection_of_part_X_extremeties_on_axis\n",
    "        must be True, and an axis and its origin must be given\n",
    "\n",
    "    Parameter:\n",
    "    - planar1 : first candidate planar 'planar_surface'\n",
    "    - planar2 : second candidate planar 'planar_surface'\n",
    "    - tolerance_angle_parallel : tolerence degree to which the two planars are considered as parellel \n",
    "    - tolerance_angle_coplan : tolerence degree to which the two planars are considered as on the same plane\n",
    "    - physical_space : the physical space,\n",
    "    - knowledge_framework : the knowledge_framework, \n",
    "    - complex_surface : candidate complex_surface if it exists , \n",
    "    - complex_surface_name : proposed name of the complex_surface to construct, \n",
    "    - force_projection_of_part_1_extremeties_on_axis : boolean by default is False. If true it forces the projection of \n",
    "            the non-shared nodes of the constructed part1 on a given axis\n",
    "    - axis_1 : vector type array\n",
    "    - axis_1_origin : point type array\n",
    "    - force_projection_of_part_2_extremeties_on_axis : boolean by default is False. If true it forces the projection of \n",
    "            the non-shared nodes of the constructed part2 on a given axis\n",
    "    - axis_2 : vector type array\n",
    "    - axis_2_origin : point type arr\n",
    "\"\"\"  \n",
    "    \n",
    "    if complex_surface is not None:\n",
    "        complex_surface = complex_surface\n",
    "    else:\n",
    "        if complex_surface_name is not None :\n",
    "            complex_surface = knowledge_framework().Complex_Surface(name = complex_surface_name)\n",
    "        else:\n",
    "            complex_surface = knowledge_framework().Complex_Surface()\n",
    "    # change name of parts based on their number for part in complex_surf.has_Part....    \n",
    "    coord_labels = physical_space.coordinate_labels\n",
    "    # get candidate stratigraphic_parts paramters\n",
    "    n1 = np.array(physical_space.get_object_coordinates( object= planar1, kind= \"normal\")) \n",
    "    n2 = np.array(physical_space.get_object_coordinates( object= planar2, kind= \"normal\")) \n",
    "    c1 = np.array(physical_space.get_object_coordinates( object= planar1, kind= \"center\")) \n",
    "    c2 = np.array(physical_space.get_object_coordinates( object= planar2, kind= \"center\"))\n",
    "    # two cases could happen either they are parallel or not \n",
    "    parallelism = check_surface_parallelism(n1, n2,  tolerance = tolerance_angle_parallel)\n",
    "    # if the surfaces are parllel, they must be coplanar, have the same polarity and not totally overlapping to be able to build a complex surface\n",
    "    if parallelism is True: # this first block could be replaced by a try except block\n",
    "        \n",
    "        strati_part1_nodes = physical_space.get_object_coordinates( object= planar1, kind= \"nodes\")\n",
    "        strati_part2_nodes = physical_space.get_object_coordinates( object= planar2, kind= \"nodes\")\n",
    "        coplanar = check_if_two_surfaces_coplanar(point_on_surface1 = c1,  point_on_surface2 = c2,\n",
    "                                                    normal = n1, tolerance = tolerance_angle_coplan)\n",
    "        if coplanar is True:\n",
    "\n",
    "            inclusion1_in_2 = check_two_coplanar_surface_inclusion(candidate_larger_surface_nodes = strati_part2_nodes, \n",
    "                                                                    candidate_smaller_surface_nodes = strati_part1_nodes,\n",
    "                                                                    candidate_larger_surface_normal = n2)\n",
    "            inclusion2_in_1 = check_two_coplanar_surface_inclusion(candidate_larger_surface_nodes = strati_part1_nodes, \n",
    "                                                                    candidate_smaller_surface_nodes = strati_part2_nodes,\n",
    "                                                                    candidate_larger_surface_normal = n1)\n",
    "\n",
    "            if inclusion1_in_2 is True:\n",
    "                # return None\n",
    "                raise MalissiaNotImplementedYet ('{} is inluded totally in {}'.format(planar1.name, planar2.name))\n",
    "            elif inclusion2_in_1 is True:\n",
    "                # return None\n",
    "                raise MalissiaNotImplementedYet ('{} is inluded totally in {}'.format(planar2.name, planar1.name))\n",
    "            else:\n",
    "                common_nodes = common_two_nodes_two_planars(strati_part1_nodes, strati_part2_nodes, return_error = False)\n",
    "                if common_nodes is not None and len(common_nodes) == 2:\n",
    "                    complex_surface.has_Part.extend([planar1, planar2])\n",
    "                    neighbor_constructor_from_multiple_planars( complex_surface, knowledge_framework, physical_space)\n",
    "                    complex_surface.non_planar = False\n",
    "                    attribute_parts_nodes_to_complex_surface(complex_surface)\n",
    "                    return complex_surface\n",
    "                c1_to_c2_vec = c2-c1\n",
    "                separ_axis = np.cross(n1, c1_to_c2_vec)\n",
    "                side_size_1_tempo = physical_space.get_object_coordinates( object= planar1, kind= \"size\")\n",
    "                side_size_2_tempo = physical_space.get_object_coordinates( object= planar2, kind= \"size\")\n",
    "                ratio1 = side_size_1_tempo/(side_size_1_tempo + side_size_2_tempo)\n",
    "                ratio2 = side_size_2_tempo/(side_size_1_tempo + side_size_2_tempo)\n",
    "                dist_c1_c2 = np.linalg.norm(c2-c1)\n",
    "                point_inter = compute_endpoint(c1, c1_to_c2_vec, ratio1*dist_c1_c2)\n",
    "                side_size_common, start_point = compute_side_size_and_start_point(planar1,  \n",
    "                                                                    planar2, physical_space, \n",
    "                                                                        separ_axis, point_inter)\n",
    "                all_distances1 = [compute_distance_to_vector(n_i, point_inter, \n",
    "                                                                separ_axis) for n_i in strati_part1_nodes]\n",
    "                side_size1 = np.max(all_distances1)\n",
    "                all_distances2 = [compute_distance_to_vector(n_i, point_inter, \n",
    "                                                                separ_axis) for n_i in strati_part2_nodes]\n",
    "                side_size2 = np.max(all_distances2)\n",
    "                p1_v0 = start_point\n",
    "                p1_v1 = compute_endpoint(p1_v0, separ_axis, side_size_common)\n",
    "                p1_v2 = compute_endpoint(p1_v1, -c1_to_c2_vec, side_size1)\n",
    "                p1_v3 = compute_endpoint(p1_v2, -separ_axis, side_size_common)\n",
    "                nodes1_temp = [p1_v0, p1_v1, p1_v2, p1_v3]\n",
    "                p2_v0 = p1_v1\n",
    "                p2_v1 = start_point\n",
    "                p2_v2 = compute_endpoint(p2_v1, c1_to_c2_vec, side_size2)\n",
    "                p2_v3 = compute_endpoint(p2_v2, separ_axis, side_size_common)\n",
    "                nodes2_temp = [p2_v0, p2_v1, p2_v2, p2_v3]\n",
    "                new_complex_surface = complex_surface_two_planars_constructor(knowledge_framework, \n",
    "                                                                    physical_space, complex_surface, \n",
    "                                                            nodes1_temp, nodes2_temp, \n",
    "                                                            side_size= side_size2, force_non_planar_sign = False)\n",
    "                neighbor_constructor_from_multiple_planars(new_complex_surface, knowledge_framework, physical_space)\n",
    "                attribute_parts_nodes_to_complex_surface(new_complex_surface)\n",
    "                return new_complex_surface\n",
    "\n",
    "        \n",
    "        else:\n",
    "            projected_c1_on_plane2 = project_point_onto_plane(c1, strati_part2_nodes )\n",
    "            c1_to_c1_proj_vec = projected_c1_on_plane2-c1\n",
    "            dist_c1_proj = np.linalg.norm(c1_to_c1_proj_vec)\n",
    "            c1_to_c1_proj_vec /= dist_c1_proj\n",
    "            inverse_c1_to_c1_proj_vec = -c1_to_c1_proj_vec\n",
    "            \n",
    "            new_strati_part1_nodes = np.array([np.array(compute_endpoint(node_i, c1_to_c1_proj_vec, dist_c1_proj/2)) for node_i in strati_part1_nodes ])\n",
    "            new_strati_part2_nodes = np.array([np.array(compute_endpoint(node_i, inverse_c1_to_c1_proj_vec, dist_c1_proj/2)) for node_i in strati_part2_nodes ])\n",
    "            new_c1 = compute_center_nodes(new_strati_part1_nodes)\n",
    "            new_c2 = compute_center_nodes(new_strati_part2_nodes)\n",
    "            \n",
    "            c1_to_c2_vec = new_c2-new_c1\n",
    "            separ_axis = np.cross(n1, c1_to_c2_vec)\n",
    "            side_size_1_tempo = physical_space.get_object_coordinates( object= planar1, kind= \"size\")\n",
    "            side_size_2_tempo = physical_space.get_object_coordinates( object= planar2, kind= \"size\")\n",
    "            ratio1 = side_size_1_tempo/(side_size_1_tempo + side_size_2_tempo)\n",
    "            ratio2 = side_size_2_tempo/(side_size_1_tempo + side_size_2_tempo)\n",
    "            dist_c1_c2 = np.linalg.norm(new_c2-new_c1)\n",
    "            point_inter = compute_endpoint(new_c1, c1_to_c2_vec, ratio1*dist_c1_c2)\n",
    "            all_nodes =  [v for v in new_strati_part1_nodes ] + [v for v in new_strati_part2_nodes ]\n",
    "            \n",
    "            side_size_common, start_point = compute_side_size_and_start_point(planar1,  \n",
    "                                                                planar2, physical_space, \n",
    "                                                                    separ_axis, point_inter, all_surfaces_nodes= all_nodes)\n",
    "            all_distances1 = [compute_distance_to_vector(n_i, point_inter, \n",
    "                                                            separ_axis) for n_i in new_strati_part1_nodes]\n",
    "            side_size1 = np.max(all_distances1)\n",
    "            all_distances2 = [compute_distance_to_vector(n_i, point_inter, \n",
    "                                                            separ_axis) for n_i in new_strati_part2_nodes]\n",
    "            side_size2 = np.max(all_distances2)\n",
    "            p1_v0 = start_point\n",
    "            p1_v1 = compute_endpoint(p1_v0, separ_axis, side_size_common)\n",
    "            p1_v2 = compute_endpoint(p1_v1, -c1_to_c2_vec, side_size1)\n",
    "            p1_v3 = compute_endpoint(p1_v2, -separ_axis, side_size_common)\n",
    "            nodes1_temp = [p1_v0, p1_v1, p1_v2, p1_v3]\n",
    "            p2_v0 = p1_v1\n",
    "            p2_v1 = start_point\n",
    "            p2_v2 = compute_endpoint(p2_v1, c1_to_c2_vec, side_size2)\n",
    "            p2_v3 = compute_endpoint(p2_v2, separ_axis, side_size_common)\n",
    "            nodes2_temp = [p2_v0, p2_v1, p2_v2, p2_v3]\n",
    "            if force_projection_of_part_1_extremeties_on_axis == True:\n",
    "                if axis_1 is None or axis_2_origin is None :\n",
    "                    raise MalissiaBaseError('to project new_part1 extermeties on an axis, axis_1 and its origin must be given')\n",
    "                else:\n",
    "                    p1_v2 = np.array(compute_point_projection_onto_line(nodes1_temp[2],axis_1_origin, axis_1))\n",
    "                    p1_v3 = np.array(compute_point_projection_onto_line(nodes1_temp[3],axis_1_origin, axis_1))\n",
    "                    nodes1_temp =  [nodes1_temp[0], nodes1_temp[1], p1_v2, p1_v3]\n",
    "                    \n",
    "                    \n",
    "            if force_projection_of_part_2_extremeties_on_axis == True:\n",
    "                if axis_2 is None or axis_2_origin is None :\n",
    "                    raise MalissiaBaseError('to project new_part2 extermeties on an axis, axis_2 and its origin must be given')\n",
    "                else:\n",
    "                    p2_v2 = np.array(compute_point_projection_onto_line(nodes2_temp[2],axis_2_origin, axis_2))\n",
    "                    p2_v3 = np.array(compute_point_projection_onto_line(nodes2_temp[3],axis_2_origin, axis_2))\n",
    "                    nodes2_temp =  [nodes2_temp[0], nodes2_temp[1], p2_v2, p2_v3]\n",
    "\n",
    "\n",
    "            new_complex_surface = complex_surface_two_planars_constructor(knowledge_framework, physical_space, complex_surface, \n",
    "                                                        nodes1_temp, nodes2_temp, side_size= side_size_common, \n",
    "                                                        force_non_planar_sign = False,\n",
    "                                                        forced_p2_v3 = forced_p2_v3, \n",
    "                                          forced_p2_v2 = forced_p2_v2, \n",
    "                                          forced_p1_v3 = forced_p1_v3, \n",
    "                                          forced_p1_v2 = forced_p1_v2)\n",
    "            neighbor_constructor_from_multiple_planars( new_complex_surface, knowledge_framework, physical_space)\n",
    "            attribute_parts_nodes_to_complex_surface(new_complex_surface)\n",
    "            return new_complex_surface         \n",
    "            \n",
    "        \n",
    "\n",
    "    # in case the candidatesurfaces are not parallel the construction of \n",
    "        # a complex surface could be approach as fold construction\n",
    "    else:\n",
    "        # check chirality imprtant as the same quality as pola or para \n",
    "        # using methods of fold constuction compute an intersection point and a side \n",
    "            ## \"fold axis and limbs are only nomination of parammters that are used to compute but not used as folding paramters\"\n",
    "        part1_nodes = physical_space.get_object_coordinates( object= planar1, kind= \"nodes\")\n",
    "        part2_nodes = physical_space.get_object_coordinates( object= planar2, kind= \"nodes\")    \n",
    "        common_nodes = common_two_nodes_two_planars(part1_nodes, part2_nodes, return_error = False)\n",
    "        if common_nodes is not None and len(common_nodes) == 2:\n",
    "            complex_surface.has_Part.extend([planar1, planar2])\n",
    "            neighbor_constructor_from_multiple_planars( complex_surface, knowledge_framework, physical_space)\n",
    "            complex_surface.non_planar = True\n",
    "            attribute_parts_nodes_to_complex_surface(complex_surface)\n",
    "            return complex_surface    \n",
    "           \n",
    "        axis = compute_fold_axis(n1 = n1, c1 =  c1, n2 = n2, c2 = c2)\n",
    "        intersection_point = compute_intersection_point(plane1_center=c1, \n",
    "                                                    plane1_normal= n1,plane2_center=c2, \n",
    "                                                    plane2_normal= n2, fold_axis= axis)\n",
    "        ## get side_size and start point\n",
    "        side_size, start_point = compute_side_size_and_start_point(limb_part1 = planar1,  limb_part2 = planar2,\n",
    "                                   physical_space = physical_space,  fold_axis = axis, \n",
    "                                   intersection_point = intersection_point)\n",
    "        nodes1_temp = compute_shape_limb1(start_point, axis, side_size,  n1)\n",
    "        vertex0, vertex1 = nodes1_temp[1],  nodes1_temp[0]\n",
    "        nodes2_temp = compute_shape_limb2( vertex0, vertex1 , axis, side_size,  n2)\n",
    "\n",
    "        if force_projection_of_part_1_extremeties_on_axis == True:\n",
    "            if axis_1 is None or axis_2_origin is None :\n",
    "                raise MalissiaBaseError('to project new_part1 extermeties on an axis, axis_1 and its origin must be given')\n",
    "            else:\n",
    "                p1_v2 = np.array(compute_point_projection_onto_line(nodes1_temp[2],axis_1_origin, axis_1))\n",
    "                p1_v3 = np.array(compute_point_projection_onto_line(nodes1_temp[3],axis_1_origin, axis_1))\n",
    "                nodes1_temp =  [nodes1_temp[0], nodes1_temp[1], p1_v2, p1_v3]\n",
    "                \n",
    "                for i in range(0,4):\n",
    "                    name_prop = 'has_Node{}'.format(i)\n",
    "                    v = np.array(physical_space.get_object_coordinates(getattr(planar1, name_prop)[0], 'nodes')[0])\n",
    "                    diff = np.linalg.norm(p1_v2 - v)\n",
    "                    if diff <= tolerance:\n",
    "                        forced_p1_v2 = getattr(planar1, name_prop)[0]\n",
    "                        break\n",
    "\n",
    "                for i in range(0,4):\n",
    "                    name_prop = 'has_Node{}'.format(i)\n",
    "                    v = np.array(physical_space.get_object_coordinates(getattr(planar1, name_prop)[0], 'nodes')[0])\n",
    "                    diff = np.linalg.norm(p1_v3 - v)\n",
    "                    if diff <= tolerance:\n",
    "                        forced_p1_v3 = getattr(planar1, name_prop)[0]\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if force_projection_of_part_2_extremeties_on_axis == True:\n",
    "            if axis_2 is None or axis_2_origin is None :\n",
    "                raise MalissiaBaseError('to project new_part2 extermeties on an axis, axis_2 and its origin must be given')\n",
    "            else:\n",
    "                p2_v2 = np.array(compute_point_projection_onto_line(nodes2_temp[2],axis_2_origin, axis_2))\n",
    "                p2_v3 = np.array(compute_point_projection_onto_line(nodes2_temp[3],axis_2_origin, axis_2))\n",
    "                nodes2_temp =  [nodes2_temp[0], nodes2_temp[1], p2_v2, p2_v3]\n",
    "\n",
    "                \n",
    "                for i in range(0,4):\n",
    "                    name_prop = 'has_Node{}'.format(i)\n",
    "                    v = np.array(physical_space.get_object_coordinates(getattr(planar2, name_prop)[0], 'nodes')[0])\n",
    "                    diff = np.linalg.norm(p2_v2 - v)\n",
    "                    if diff <= tolerance:\n",
    "                        forced_p2_v2 = getattr(planar2, name_prop)[0]\n",
    "                        break\n",
    "\n",
    "                for i in range(0,4):\n",
    "                    name_prop = 'has_Node{}'.format(i)\n",
    "                    v = np.array(physical_space.get_object_coordinates(getattr(planar2, name_prop)[0], 'nodes')[0])\n",
    "                    diff = np.linalg.norm(p2_v3 - v)\n",
    "                    if diff <= tolerance:\n",
    "                        forced_p2_v3 = getattr(planar2, name_prop)[0]\n",
    "                        break\n",
    "\n",
    "\n",
    "        new_complex_surface =  complex_surface_two_planars_constructor(knowledge_framework, physical_space, complex_surface, \n",
    "                                                        nodes1_temp, nodes2_temp,  \n",
    "                                                        side_size= side_size, force_non_planar_sign= True,\n",
    "                                                        forced_p2_v3 = forced_p2_v3, \n",
    "                                          forced_p2_v2 = forced_p2_v2, \n",
    "                                          forced_p1_v3 = forced_p1_v3, \n",
    "                                          forced_p1_v2 = forced_p1_v2,)\n",
    "        neighbor_constructor_from_multiple_planars( new_complex_surface, knowledge_framework, physical_space)\n",
    "        attribute_parts_nodes_to_complex_surface(new_complex_surface)\n",
    "        return new_complex_surface\n",
    "\n",
    "\n",
    "def compute_complex_surface_from_complex_surface_and_planar(candidate_complex_surface, planar, \n",
    "                                                              knowledge_framework, physical_space,\n",
    "                                                              new_complex_surface_name = None, \n",
    "                                                              chosen_part_to_join = None, \n",
    "                                                              tolerance_angle_parallel = 3, \n",
    "                                                              tolerance_angle_coplan = 3 ):\n",
    "    \"\"\"\n",
    "    Method to compute a complex_surface from a candidate_complex_surface and a planar. It returns a new complex_surface. \n",
    "    At this stage, chosen_part_to_join must be given (one of the candidate_complex_surface extremete faces).\n",
    "\n",
    "\n",
    "    Parameter:\n",
    "    - candidate_complex_surface : the candidate_complex_surface \n",
    "    - planar : candidate planar \n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    - physical_space : the physical_space\n",
    "    - new_complex_surface_name : type str, optional variable holding a desired name for the new_complex_surface  \n",
    "    - chosen_part_to_join : a chosen part to join with the planar \n",
    "    - tolerance_angle_parallel : tolerence degree to which the two planars are considered as parellel \n",
    "    - tolerance_angle_coplan : tolerence degree to which the two planars are considered as on the same plane \n",
    "\"\"\"  \n",
    "    \n",
    "\n",
    "    if new_complex_surface_name is not None :\n",
    "        new_complex_surface = knowledge_framework().Complex_Surface( name = new_complex_surface_name,)\n",
    "    else:\n",
    "        nb_cplx = len(knowledge_framework().Complex_Surface.instances())\n",
    "        new_complex_surface_name = 'Cmpx_Su_{}'.format(nb_cplx +1)\n",
    "        new_complex_surface = knowledge_framework().Complex_Surface( name = new_complex_surface_name)\n",
    "    \n",
    "    if chosen_part_to_join is not None:\n",
    "        # check if the chosen_part_to_join has only one neighbor in the candidate complex_surface\n",
    "        neighbors_of_part_tojoin = chosen_part_to_join.has_Neighbor\n",
    "        neighbor_of_part_tojoin_in_cplx_surf = list(set(candidate_complex_surface.has_Part).intersection(neighbors_of_part_tojoin))\n",
    "        if len(neighbor_of_part_tojoin_in_cplx_surf) != 1:\n",
    "            raise MalissiaBaseError('the  {} used as chosen_part_to_join is an extreme face and must have exactly one neighbor \\\n",
    "                                    in the candidate complex_surface {} '.format(chosen_part_to_join.name, candidate_complex_surface.name ))\n",
    "        \n",
    "        shared_intersection = list(set(chosen_part_to_join.has_Shared_Side).intersection(neighbor_of_part_tojoin_in_cplx_surf[0].has_Shared_Side))\n",
    "        if len(shared_intersection) != 1:\n",
    "            raise MalissiaBaseError('the  {} used as chosen_part_to_join must have exactly one shared side with its neighbor in \\\n",
    "                                    in the candidate complex_surface {} '.format(chosen_part_to_join.name, candidate_complex_surface.name ))\n",
    "        shared_side = shared_intersection[0]\n",
    "        point_0 = physical_space.get_object_coordinates( object= shared_side.has_End_Points[0], kind= \"node\")[0] \n",
    "        point_1 = physical_space.get_object_coordinates( object= shared_side.has_End_Points[1], kind= \"node\")[0] \n",
    "        shared_side_vec = np.array(point_0) - np.array(point_1)\n",
    "\n",
    "        \n",
    "\n",
    "        new_complex_surface = compute_complex_surface_two_planars( planar1= planar, \n",
    "                                                   planar2= chosen_part_to_join,\n",
    "                                                   tolerance_angle_parallel = tolerance_angle_parallel, \n",
    "                                                    tolerance_angle_coplan = tolerance_angle_coplan, \n",
    "                                                    physical_space = physical_space, \n",
    "                                                    knowledge_framework = knowledge_framework, \n",
    "                                                    complex_surface = new_complex_surface,  \n",
    "                                                    force_projection_of_part_2_extremeties_on_axis= True, \n",
    "                                                    axis_2= shared_side_vec, axis_2_origin = point_0)\n",
    "        \n",
    "        for part in candidate_complex_surface.has_Part:\n",
    "            if part != chosen_part_to_join and part not in  new_complex_surface.has_Part :\n",
    "                new_complex_surface.has_Part.append(part)\n",
    "        if new_complex_surface.non_planar != True:\n",
    "            new_complex_surface.non_planar = True\n",
    "        neighbor_constructor_from_multiple_planars( new_complex_surface, knowledge_framework, physical_space)\n",
    "        attribute_parts_nodes_to_complex_surface(new_complex_surface)\n",
    "        return new_complex_surface\n",
    "\n",
    "    else:\n",
    "        extreme_faces = compute_complex_surface_extreme_surfaces(knowledge_framework, \n",
    "                                                                 candidate_complex_surface)\n",
    "        if len(extreme_faces)!=2:\n",
    "            raise MalissiaBaseError('the candidate complex_surface {} has {} extreme faces'.format(candidate_complex_surface.name,\n",
    "                                                                                                    len(extreme_faces))) \n",
    "        center_planar = physical_space.get_object_coordinates(planar, \"center\")\n",
    "        face1_center = physical_space.get_object_coordinates(extreme_faces[0], \"center\")\n",
    "        face2_center = physical_space.get_object_coordinates(extreme_faces[1], \"center\")\n",
    "\n",
    "        dist_face1_c = np.linalg.norm(np.array(center_planar)-np.array(face1_center))\n",
    "        dist_face2_c = np.linalg.norm(np.array(center_planar)-np.array(face2_center))\n",
    "        chosen_part_to_join = extreme_faces[0] if dist_face1_c <= dist_face2_c else extreme_faces[1]\n",
    "        return compute_complex_surface_from_complex_surface_and_planar(candidate_complex_surface, planar, \n",
    "                                                              knowledge_framework = knowledge_framework, \n",
    "                                                              physical_space = physical_space ,\n",
    "                                                              new_complex_surface_name = new_complex_surface_name, \n",
    "                                                              chosen_part_to_join = chosen_part_to_join, \n",
    "                                                              tolerance_angle_parallel = tolerance_angle_parallel, \n",
    "                                                              tolerance_angle_coplan = tolerance_angle_coplan )\n",
    "\n",
    "  \n",
    "def compute_complex_surface_from_two_complex_surfaces(complex_surface1, complex_surface2,  \n",
    "                                                              knowledge_framework, physical_space,\n",
    "                                                              chosen_part_to_join_1 = None, \n",
    "                                                              chosen_part_to_join_2 = None,\n",
    "                                                              new_complex_surface_name = None, \n",
    "                                                              tolerance_angle_parallel = 3, \n",
    "                                                              tolerance_angle_coplan = 3 ):\n",
    "    \"\"\"\n",
    "    Method to compute a complex_surface from two candidate_complex_surfaces. It returns a new complex_surface. \n",
    "    At this stage, chosen_part_to_join_1 and chosen_part_to_join_2 must be given (one of the candidate_complex_surface extremete faces).\n",
    "\n",
    "    Parameter:\n",
    "    - Complex_surface1 : the candidate_complex_surface1\n",
    "    - Complex_surface2 : the candidate_complex_surface2  \n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    - physical_space : the physical_space\n",
    "    - new_complex_surface_name : type str, optional variable holding a desired name for the new_complex_surface  \n",
    "    - chosen_part_to_join_1 : a chosen part to join from the candidate_complex_surface1\n",
    "    - chosen_part_to_join_2 : a chosen part to join from the candidate_complex_surface2\n",
    "    - tolerance_angle_parallel : tolerence degree to which the two planars are considered as parellel \n",
    "    - tolerance_angle_coplan : tolerence degree to which the two planars are considered as on the same plane \n",
    "    \"\"\"     \n",
    "    \n",
    "    if new_complex_surface_name is not None :\n",
    "        new_complex_surface = knowledge_framework().Complex_Surface( name = new_complex_surface_name)\n",
    "    else:\n",
    "        nb_cplx = len(knowledge_framework().Complex_Surface.instances())\n",
    "        new_complex_surface_name = 'Cmpx_Su_{}'.format(nb_cplx +1)\n",
    "        new_complex_surface = knowledge_framework().Complex_Surface( name = new_complex_surface_name)\n",
    "    \n",
    "    if chosen_part_to_join_1 is not None and chosen_part_to_join_2 is not None : \n",
    "        print(chosen_part_to_join_1, chosen_part_to_join_2)\n",
    "        # check if the chosen_part_to_join has only one neighbor in the candidate complex_surface1\n",
    "        neighbors_of_part_tojoin1 = chosen_part_to_join_1.has_Neighbor\n",
    "        neighbor_of_part_tojoin_in_cplx_surf1 = list(set(complex_surface1.has_Part).intersection(neighbors_of_part_tojoin1))\n",
    "        if len(neighbor_of_part_tojoin_in_cplx_surf1) != 1:\n",
    "            raise MalissiaBaseError('the  {} used as chosen_part_to_join is an extreme face and must have exactly one neighbor \\\n",
    "                                    in the candidate complex_surface {} '.format(chosen_part_to_join_1.name, complex_surface1.name ))\n",
    "        \n",
    "        shared_intersection1 = list(set(chosen_part_to_join_1.has_Shared_Side).intersection(neighbor_of_part_tojoin_in_cplx_surf1[0].has_Shared_Side))\n",
    "        if len(shared_intersection1) != 1:\n",
    "            raise MalissiaBaseError('the  {} used as chosen_part_to_join must have exactly one shared side with its neighbor in \\\n",
    "                                    in the candidate complex_surface {} '.format(chosen_part_to_join_1.name, complex_surface1.name ))\n",
    "        shared_side1 = shared_intersection1[0]\n",
    "        point1_0 = physical_space.get_object_coordinates( object= shared_side1.has_End_Points[0], kind= \"node\")[0] \n",
    "        point1_1 = physical_space.get_object_coordinates( object= shared_side1.has_End_Points[1], kind= \"node\")[0] \n",
    "        shared_side_vec1 = np.array(point1_0) - np.array(point1_1)\n",
    "\n",
    "        # check if the chosen_part_to_join has only one neighbor in the candidate complex_surface2\n",
    "        neighbors_of_part_tojoin2 = chosen_part_to_join_2.has_Neighbor\n",
    "        neighbor_of_part_tojoin_in_cplx_surf2 = list(set(complex_surface2.has_Part).intersection(neighbors_of_part_tojoin2))\n",
    "        if len(neighbor_of_part_tojoin_in_cplx_surf2) != 1:\n",
    "            raise MalissiaBaseError('the  {} used as chosen_part_to_join is an extreme face and must have exactly one neighbor \\\n",
    "                                    in the candidate complex_surface {} '.format(chosen_part_to_join_2.name, complex_surface2.name ))\n",
    "        \n",
    "        shared_intersection2 = list(set(chosen_part_to_join_2.has_Shared_Side).intersection(neighbor_of_part_tojoin_in_cplx_surf2[0].has_Shared_Side))\n",
    "        if len(shared_intersection2) != 1:\n",
    "            raise MalissiaBaseError('the  {} used as chosen_part_to_join must have exactly one shared side with its neighbor in \\\n",
    "                                    in the candidate complex_surface {} '.format(chosen_part_to_join_2.name, complex_surface2.name ))\n",
    "        shared_side2 = shared_intersection2[0]\n",
    "        point2_0 = physical_space.get_object_coordinates( object= shared_side2.has_End_Points[0], kind= \"node\")[0] \n",
    "        point2_1 = physical_space.get_object_coordinates( object= shared_side2.has_End_Points[1], kind= \"node\")[0] \n",
    "        shared_side_vec2 = np.array(point2_0) - np.array(point2_1)\n",
    "\n",
    "\n",
    "        new_complex_surface= compute_complex_surface_two_planars(chosen_part_to_join_1, chosen_part_to_join_2, \n",
    "                                    tolerance_angle_parallel = tolerance_angle_parallel, \n",
    "                                    tolerance_angle_coplan = tolerance_angle_coplan, \n",
    "                                    physical_space = physical_space,\n",
    "                                    knowledge_framework = knowledge_framework, \n",
    "                                    complex_surface = new_complex_surface,  \n",
    "                                    force_projection_of_part_1_extremeties_on_axis= True, \n",
    "                                    axis_1= shared_side_vec1, axis_1_origin= point1_0, \n",
    "                                    force_projection_of_part_2_extremeties_on_axis= True, \n",
    "                                    axis_2= shared_side_vec2 , axis_2_origin= point2_0 )\n",
    "        \n",
    "        for part in complex_surface1.has_Part:\n",
    "            if part !=  chosen_part_to_join_1 and part not in  new_complex_surface.has_Part :\n",
    "                new_complex_surface.has_Part.append(part)\n",
    "        for part in complex_surface2.has_Part:\n",
    "            if part != chosen_part_to_join_2 and part not in  new_complex_surface.has_Part :\n",
    "                new_complex_surface.has_Part.append(part)\n",
    "        if new_complex_surface.non_planar != True:\n",
    "            new_complex_surface.non_planar = True\n",
    "        neighbor_constructor_from_multiple_planars( new_complex_surface, knowledge_framework, physical_space)\n",
    "        attribute_parts_nodes_to_complex_surface(new_complex_surface)\n",
    "        return new_complex_surface\n",
    "    \n",
    "    elif  chosen_part_to_join_1 is  None and chosen_part_to_join_2 is  None :\n",
    "        extrem_faces_1 = compute_complex_surface_extreme_surfaces(knowledge_framework, complex_surface1)\n",
    "        if len(extrem_faces_1)!=2:\n",
    "            raise MalissiaBaseError('the candidate complex_surface {} has {} extreme faces'.format(complex_surface1.name,\n",
    "                                                                                                    len(extrem_faces_1))) \n",
    "        extrem_faces_2 = compute_complex_surface_extreme_surfaces(knowledge_framework, complex_surface2)\n",
    "        if len(extrem_faces_2)!=2:\n",
    "            raise MalissiaBaseError('the candidate complex_surface {} has {} extreme faces'.format(complex_surface2.name,\n",
    "                                                                                                    len(extrem_faces_2))) \n",
    "        min_distance =  float('inf')\n",
    "        for face_1 in extrem_faces_1:\n",
    "            face_1_center = physical_space.get_object_coordinates(face_1, \"center\")\n",
    "            for face_2 in extrem_faces_2:\n",
    "                face_2_center = physical_space.get_object_coordinates(face_2, \"center\")\n",
    "                distance = np.linalg.norm(np.array(face_1_center) - np.array(face_2_center))\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    closest_faces = [face_1, face_2]\n",
    "                    \n",
    "        return compute_complex_surface_from_two_complex_surfaces(complex_surface1, complex_surface2, \n",
    "                                                      chosen_part_to_join_1 = closest_faces[0], \n",
    "                                                      chosen_part_to_join_2 = closest_faces[1], \n",
    "                                                              knowledge_framework = knowledge_framework,\n",
    "                                                              physical_space = physical_space ,\n",
    "                                                              new_complex_surface_name = new_complex_surface_name, \n",
    "                                                              tolerance_angle_parallel = tolerance_angle_parallel, \n",
    "                                                              tolerance_angle_coplan = tolerance_angle_coplan )\n",
    "    else:\n",
    "        raise MalissiaNotImplementedYet('chosen_part_to_join_1 and chosen_part_to_join_2 must be given both or missing both but not one given and the other missing')\n",
    "    \n",
    "\n",
    "def are_points_coplanar(points):\n",
    "    # Check if the list has at least three points\n",
    "    if len(points) < 3:\n",
    "        return False\n",
    "\n",
    "    # Extract the first three points to define the plane\n",
    "    p1, p2, p3 = points[:3]\n",
    "\n",
    "    # Calculate the normal vector of the plane formed by the first three points\n",
    "    normal_vector = np.cross(p2 - p1, p3 - p1)\n",
    "\n",
    "    # Check if the remaining points lie on the same plane\n",
    "    for point in points[3:]:\n",
    "        # Calculate the vector from p1 to the current point\n",
    "        vector_to_point = point - p1\n",
    "\n",
    "        # Check if the vector is orthogonal to the normal vector\n",
    "        if not np.allclose(np.dot(vector_to_point, normal_vector), 0):\n",
    "            return False\n",
    "\n",
    "    return True        \n",
    "\n",
    "def part_explained_by_part( explainable_features):\n",
    "    \"\"\"Method to ensure that a parat is not already explained by another part or itslef in features to explain\"\"\"\n",
    "\n",
    "    feature0 = explainable_features[0]\n",
    "    feature1 = explainable_features[1]\n",
    "    if feature0 is feature1:\n",
    "        print(' Warning: objects to explain are {}, thus the same object. An object intrinsically explains itslef'.format(explainable_features))\n",
    "        return True, feature0\n",
    "\n",
    "    if feature0 in feature1.explain or feature0 in feature1.has_Part:\n",
    "        print(' {} is already explained and part of {}'.format(feature0, feature1))\n",
    "        if feature0 not in feature1.explain:\n",
    "            feature1.explain.append(feature0)\n",
    "        if feature0 not in feature1.has_Part:\n",
    "            feature1.has_Part.append(feature0)\n",
    "        return True, feature1\n",
    "    if feature1 in feature0.explain or feature1 in feature0.has_Part:\n",
    "        print(' {} is already explained and part of {}'.format(feature1, feature0))\n",
    "        if feature1 not in feature0.explain:\n",
    "            feature0.explain.append(feature1)\n",
    "        if feature1 not in feature0.has_Part:\n",
    "            feature0.has_Part.append(feature1)\n",
    "        return True, feature0\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def rep_in_rep( physical_space, rep_feature_1, rep_feature_2, feature1, feature2, use_case  ):\n",
    "    \"\"\"Method to ensure that a parat is not already explained by another part through their representation or itslef in features to explain\"\"\"\n",
    "\n",
    "    if rep_feature_1 is rep_feature_2:\n",
    "        print(' Warning: objects to explain are {}{}, with these same representations {}{}, which means they are the same object. An object intrinsically explains itslef'.format(feature1,\n",
    "                                                                                                    feature2, rep_feature_1, rep_feature_2))\n",
    "        return True, feature1 \n",
    "    \n",
    "    if use_case == \"complx_rep_in_complx_rep\":\n",
    "        answ, obj =  complx_rep_in_complx_rep( physical_space, \n",
    "                                        rep_feature_1, rep_feature_2, \n",
    "                                        feature1, feature2)\n",
    "        if answ: return answ, obj\n",
    "        # inverse the rep and objects\n",
    "        if not answ: return complx_rep_in_complx_rep(physical_space, \n",
    "                                        rep_feature_1 = rep_feature_2, rep_feature_2 =rep_feature_1, \n",
    "                                        feature1 = feature2, feature2 = feature1)\n",
    "    if use_case == \"part_in_complx_rep\":\n",
    "        return part_in_complx_rep(physical_space, rep_feature_1, rep_feature_2, feature1, feature2, )\n",
    "    \n",
    "    if use_case == \"part_in_part\":\n",
    "        answ, obj =  part_rep_in_part_rep( physical_space, \n",
    "                                        rep_feature_1, rep_feature_2, \n",
    "                                        feature1, feature2)\n",
    "        if answ: return answ, obj\n",
    "        # inverse the rep and objects\n",
    "        if not answ: return part_rep_in_part_rep(physical_space, \n",
    "                                        rep_feature_1 = rep_feature_2, rep_feature_2 =rep_feature_1, \n",
    "                                        feature1 = feature2, feature2 = feature1)\n",
    "    \n",
    "\n",
    "\n",
    "     \n",
    "    \n",
    "\n",
    "def complx_rep_in_complx_rep(physical_space, rep_feature_1, rep_feature_2, feature1, feature2):    \n",
    "    # for now the method only search for a total inclusion of one surface in another\n",
    "    ## todo: add methods to check if /\\ covers a part of \\/\\/ or \\/\\/ sahre parts with \\/\\/\\ \n",
    "\n",
    "    if all([part_i in rep_feature_2.has_Part for part_i in  rep_feature_1.has_Part]):\n",
    "        print(' {} is already explained and part of {} through their representation'.format(feature2,\n",
    "                                                                                    feature1))\n",
    "        return True, feature2\n",
    "    \n",
    "    rep1_tot_in_rep2 = [] \n",
    "    for part in rep_feature_1.has_Part:\n",
    "        part_nodes = physical_space.get_object_coordinates(part, \"nodes\")\n",
    "        part1_in_part2 = False\n",
    "        for part_2 in rep_feature_2.has_Part:\n",
    "            part_2_nodes = physical_space.get_object_coordinates(part_2, \"nodes\")\n",
    "            part_2_normal = physical_space.get_object_coordinates(part_2, \"normal\")\n",
    "            all_points = np.concatenate((part_nodes, part_2_nodes))\n",
    "            if are_points_coplanar(all_points)   and   check_two_coplanar_surface_inclusion(part_2_nodes, \n",
    "                                                part_nodes, part_2_normal ):\n",
    "                part1_in_part2 = True\n",
    "                rep1_tot_in_rep2.append(True)\n",
    "                break\n",
    "        if part1_in_part2 is False:\n",
    "            rep1_tot_in_rep2.appendd(False)\n",
    "            break\n",
    "    if all(rep1_tot_in_rep2):\n",
    "        if feature1 not in feature2.explain :\n",
    "            feature2.explain.append(feature1)\n",
    "        if feature1 not in feature2.has_Part :\n",
    "            feature2.has_Part.append(feature1)\n",
    "        print('warning {} is already explained and part of {} through their representation'.format(feature2,\n",
    "                                                                                    feature1))    \n",
    "        return True, feature2\n",
    "    else:\n",
    "        return False, None\n",
    "    \n",
    "\n",
    "def part_in_complx_rep(physical_space, part_rep, complex_rep, strati_part, strati_complx):\n",
    "\n",
    "    if part_rep in complex_rep.has_Part:\n",
    "        if strati_part not in strati_complx.has_Part:\n",
    "            strati_complx.has_Part.append(strati_part)\n",
    "        if strati_part not in strati_complx.explain :\n",
    "            strati_complx.explain.append(strati_part)\n",
    "        print('Warning :  {} is already explained and part of {} through their representation'.format(strati_part,\n",
    "                                                                                    strati_complx))\n",
    "        return True, strati_complx\n",
    "    \n",
    "    part_nodes = physical_space.get_object_coordinates(part_rep, \"nodes\")\n",
    "    for part_2 in complex_rep.has_Part:\n",
    "        part_2_nodes = physical_space.get_object_coordinates(part_2, \"nodes\")\n",
    "        part_2_normal = physical_space.get_object_coordinates(part_2, \"normal\")\n",
    "        all_points = np.concatenate((part_nodes, part_2_nodes))\n",
    "        if are_points_coplanar(all_points)   and   check_two_coplanar_surface_inclusion(part_2_nodes, part_nodes, part_2_normal ):\n",
    "            if strati_part not in strati_complx.has_Part:\n",
    "                strati_complx.has_Part.append(strati_part)\n",
    "            if strati_part not in strati_complx.explain :\n",
    "                strati_complx.explain.append(strati_part)\n",
    "            print('Warning :  {} is already explained and part of {} through their representation'.format(strati_part,\n",
    "                                                                                    strati_complx))\n",
    "            return True, strati_complx\n",
    "    return False, None\n",
    "    \n",
    "\n",
    "def part_rep_in_part_rep(physical_space, rep_feature_1, rep_feature_2, feature1, feature2, ):\n",
    "    part_nodes = physical_space.get_object_coordinates(rep_feature_1, \"nodes\")\n",
    "    part_2_nodes = physical_space.get_object_coordinates(rep_feature_2, \"nodes\")\n",
    "    part_2_normal = physical_space.get_object_coordinates(rep_feature_2, \"normal\")\n",
    "    all_points = np.concatenate((part_nodes, part_2_nodes))\n",
    "    if are_points_coplanar(all_points)   and   check_two_coplanar_surface_inclusion(part_2_nodes, part_nodes, part_2_normal ):\n",
    "        if feature1 not in feature2.explain :\n",
    "            feature2.explain.append(feature1)\n",
    "        print('Warning :  {} is already explained and part of {} through their representation'.format(feature1,\n",
    "                                                                                    feature2))\n",
    "        return True, feature2\n",
    "    return False, None\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "def construct_complex_stratigraphic_part(knowledge_framework,  explainable_features, physical_space,\n",
    "                                         name_object = None, name_rep = None):\n",
    "    \"\"\"\n",
    "    Method to construct a stratigraphic_part \n",
    "    \"\"\" \n",
    "    \n",
    "    if len(explainable_features[0].has_Representation) == 0 or len (explainable_features[1].has_Representation) ==0 :\n",
    "        print(\"Warning: explainable_features are missing representation. To be implemented.\")\n",
    "        return None\n",
    "    rep_feature_1 = explainable_features[0].has_Representation[0]\n",
    "    rep_feature_2 = explainable_features[1].has_Representation[0]\n",
    "    feature1 = explainable_features[0]\n",
    "    feature2 = explainable_features[1]\n",
    "    \n",
    "    redundant_explaiantion, explaining_object = part_explained_by_part( explainable_features)\n",
    "    if redundant_explaiantion: return explaining_object\n",
    "    \n",
    "\n",
    "    if knowledge_framework().Complex_Surface in rep_feature_1.is_a and  knowledge_framework().Complex_Surface in rep_feature_2.is_a:\n",
    "        redundant_explaiantion, explaining_object = rep_in_rep( physical_space, rep_feature_1, rep_feature_2, \n",
    "                                                                         feature1, feature2, use_case = \"complx_rep_in_complx_rep\" )\n",
    "        if redundant_explaiantion: return explaining_object \n",
    "        complex_surface = compute_complex_surface_from_two_complex_surfaces(rep_feature_1, \n",
    "                                                                            rep_feature_2,\n",
    "                                                                            knowledge_framework = knowledge_framework, \n",
    "                                                                            physical_space= physical_space \n",
    "                                                                            , new_complex_surface_name= name_rep)\n",
    "        \n",
    "    elif knowledge_framework().Planar_Surface in rep_feature_1.is_a and  knowledge_framework().Complex_Surface in rep_feature_2.is_a:\n",
    "        redundant_explaiantion, explaining_object = rep_in_rep( physical_space, rep_feature_1, rep_feature_2, \n",
    "                                                                         feature1, feature2, use_case = \"part_in_complx_rep\" )\n",
    "        if redundant_explaiantion: return explaining_object \n",
    "        complex_surface = compute_complex_surface_from_complex_surface_and_planar( rep_feature_2, \n",
    "                                                                                  rep_feature_1, \n",
    "                                                                                  knowledge_framework= knowledge_framework, \n",
    "                                                                                  physical_space=physical_space, \n",
    "                                                                                    new_complex_surface_name = name_rep)\n",
    "         \n",
    "    elif knowledge_framework().Complex_Surface in rep_feature_1.is_a and  knowledge_framework().Planar_Surface in rep_feature_2.is_a:\n",
    "        redundant_explaiantion, explaining_object = rep_in_rep( physical_space, rep_feature_2, rep_feature_1, \n",
    "                                                                         feature2, feature1, use_case = \"part_in_complx_rep\" )\n",
    "        if redundant_explaiantion: return explaining_object \n",
    "        complex_surface = compute_complex_surface_from_complex_surface_and_planar( rep_feature_1, \n",
    "                                                                                  rep_feature_2, \n",
    "                                                                                  knowledge_framework= knowledge_framework, \n",
    "                                                                                  physical_space=physical_space\n",
    "                                                                                   , new_complex_surface_name= name_rep)\n",
    " \n",
    "\n",
    "    elif knowledge_framework().Planar_Surface in rep_feature_1.is_a and  knowledge_framework().Planar_Surface in rep_feature_2.is_a :\n",
    "        redundant_explaiantion, explaining_object = rep_in_rep( physical_space, rep_feature_2, rep_feature_1, \n",
    "                                                                         feature2, feature1, use_case = \"part_in_part\" )\n",
    "        if redundant_explaiantion: return explaining_object \n",
    "\n",
    "        complex_surface = compute_complex_surface_two_planars(rep_feature_1, rep_feature_2, physical_space= physical_space,\n",
    "                                                              knowledge_framework= knowledge_framework  \n",
    "                                                            , complex_surface_name= name_rep )\n",
    "        \n",
    "\n",
    "    else:\n",
    "        raise MalissiaNotImplementedYet('explainable features representations are not Planar Surface or Complex_Surface') \n",
    "    \n",
    "\n",
    "    new_complex_surface = compute_larger_complex_surface(knowledge_framework, physical_space, complex_surface)\n",
    "    stratigraphic_part =  create_surface_part(knowledge_framework= knowledge_framework, \n",
    "                                             surface_representation= new_complex_surface, name = name_object, \n",
    "                                             explain = explainable_features)\n",
    "    \n",
    "    stratigraphic_part.has_Part = explainable_features\n",
    "    stratigraphic_part.has_Part.extend(explainable_features[0].has_Part)\n",
    "    stratigraphic_part.has_Part.extend(explainable_features[1].has_Part)\n",
    "\n",
    "    # temporary instruction to assign a rough center for the complex_surface, for representation, to be discussed\n",
    "    compute_center_complex_surface(knowledge_framework,physical_space, new_complex_surface)\n",
    "    if new_complex_surface.non_planar == True:  \n",
    "        stratigraphic_part.deformed = True\n",
    "    return stratigraphic_part\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, physical_space):\n",
    "    \"\"\"\n",
    "    Method to get shared sides in one complex surface. It returns list of shared sides. \n",
    "\n",
    "    Parameter:\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    \"\"\" \n",
    "\n",
    "    if len(complex_surface.has_Part)<2:\n",
    "        raise MalissiaBaseError('for a complex_surface to have a shared side in it, it must have at least two surface parts')\n",
    "    \n",
    "    neighbor_constructor_from_multiple_planars(complex_surface, knowledge_framework, physical_space)\n",
    "    list_of_shared_sides_in_complex_surf = []\n",
    "    for part in complex_surface.has_Part :\n",
    "        possible_nei = set(part.has_Neighbor).intersection(complex_surface.has_Part)\n",
    "        if possible_nei:\n",
    "            for neigh in list(possible_nei):\n",
    "                shared_intersection = set(neigh.has_Shared_Side).intersection(part.has_Shared_Side)\n",
    "                if shared_intersection is not None and list(shared_intersection)[0] not in list_of_shared_sides_in_complex_surf:\n",
    "                    list_of_shared_sides_in_complex_surf.append(list(shared_intersection)[0])\n",
    "    return list_of_shared_sides_in_complex_surf\n",
    "\n",
    "\n",
    "def get_unmatched_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, physical_space):\n",
    "    \"\"\"\n",
    "    Method to get unmatched shared sides in one complex surface. It returns list of unmatched shared sides. \n",
    "\n",
    "    Parameter:\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    \"\"\" \n",
    "\n",
    "    if len(complex_surface.has_Part)<2:\n",
    "        raise MalissiaBaseError('for a complex_surface to have a shared side in it, it must have at least two surface parts')\n",
    "    \n",
    "    neighbor_constructor_from_multiple_planars(complex_surface = complex_surface, knowledge_framework = knowledge_framework, physical_space = physical_space)\n",
    "    list_of_unmatched_shared_sides_in_complex_surf = []\n",
    "    for part in complex_surface.has_Part :\n",
    "        for unm_side in part.has_Unmatched_Shared_Side:\n",
    "            intersection_list = list(set(unm_side.has_Corresponding_Part).intersection(set(complex_surface.has_Part)))\n",
    "            if len(intersection_list) == 1 and unm_side not in list_of_unmatched_shared_sides_in_complex_surf: \n",
    "                list_of_unmatched_shared_sides_in_complex_surf.append(unm_side)\n",
    "\n",
    "    return list_of_unmatched_shared_sides_in_complex_surf\n",
    "\n",
    "\n",
    "def compute_normal_vector(nodes):\n",
    "    \"\"\"\n",
    "    Method to compute a normal vector from 4 nodes. \n",
    "\n",
    "    Parameter:\n",
    "    - nodes\n",
    "    \"\"\" \n",
    "    # Ensure we have four nodes\n",
    "    if len(nodes) != 4:\n",
    "        raise ValueError(\"A surface should be defined by four nodes.\")\n",
    "\n",
    "    # Define two vectors lying in the surface\n",
    "    vector1 = np.array(nodes[1]) - np.array(nodes[0])\n",
    "    vector2 = np.array(nodes[3]) - np.array(nodes[0])\n",
    "\n",
    "    # Compute the cross product to get the normal vector\n",
    "    normal_vector = np.cross(vector1, vector2)\n",
    "\n",
    "    # Normalize the normal vector to make it a unit vector\n",
    "    normal_vector /= np.linalg.norm(normal_vector)\n",
    "\n",
    "    return normal_vector\n",
    "\n",
    "\n",
    "def compute_planar_from_planar_complex_surface(complex_surface, knowledge_framework, physical_space):\n",
    "    \"\"\"\n",
    "    Method to compute a planar surface from a complex surface that is planar. \n",
    "        It returns a planar surface.\n",
    "\n",
    "    Parameter:\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    \"\"\" \n",
    "    # make sur that the complex_surface is planar\n",
    "    if complex_surface.non_planar != False:\n",
    "        raise MalissiaBaseError('the complex_surface {} does not have False value for the quality non_planar'.format(complex_surface))\n",
    "    if len(complex_surface.has_Part) != 2:\n",
    "        raise MalissiaNotImplementedYet('mehods to transform a complex_surface with n_parts > 2 parts into one palanr are not yet implemented')\n",
    "    shared_sides = get_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, physical_space)\n",
    "    if len(shared_sides) != 1:\n",
    "        raise MalissiaBaseError('the {} parts have {} shared sides, but it should be exactly 1'.format(complex_surface, len(shared_sides)))\n",
    "    \n",
    "    # other checking methods could be added like for normals parallelism and polarity\n",
    "    original_normal = physical_space.get_object_coordinates(complex_surface.has_Part[0], kind = \"normal\")\n",
    "\n",
    "    \n",
    "    shared = shared_sides[0]\n",
    "    size_shared = shared.size[0]\n",
    "    shared_points_list = [(physical_space.get_object_coordinates(shared.has_End_Points[0], \n",
    "                                                                kind = \"node\")[0]), \n",
    "                              (physical_space.get_object_coordinates(shared.has_End_Points[1], \n",
    "                                                                    kind = \"node\")[0])] \n",
    "    shared_points_tuple = [tuple(list_i)for list_i in shared_points_list]\n",
    "    ref_shared_point  = np.array(shared_points_list[0])\n",
    "\n",
    "    part1_nodes = physical_space.get_object_coordinates(complex_surface.has_Part[0], kind = \"nodes\")\n",
    "    part1_nodes_tuple = [tuple(list_i)for list_i in part1_nodes]\n",
    "    part1_extremeties = list(set(part1_nodes_tuple) - set(shared_points_tuple))\n",
    "    part1_size = np.linalg.norm(compute_point_projection_onto_line(np.array(part1_extremeties[0]),\n",
    "                                        np.array(shared_points_list[0]), \n",
    "                                     np.array(shared_points_list[0])-np.array(shared_points_list[1]))- part1_extremeties[0])\n",
    "    part1_extremeties_1 = np.array(part1_extremeties[0])\n",
    "    part1_extremeties_2 = np.array(part1_extremeties[1])\n",
    "    distance_to_point1_1 = np.linalg.norm(ref_shared_point - part1_extremeties_1)\n",
    "    distance_to_point1_2 = np.linalg.norm(ref_shared_point - part1_extremeties_2)\n",
    "\n",
    "    # Determine the most distant and closest points\n",
    "    most_distant_1 =  part1_extremeties_1 if distance_to_point1_1 > distance_to_point1_2 else part1_extremeties_2\n",
    "    closest_point_1 = part1_extremeties_1 if distance_to_point1_1 < distance_to_point1_2 else part1_extremeties_2\n",
    "    \n",
    "\n",
    "\n",
    "    part2_nodes = physical_space.get_object_coordinates(complex_surface.has_Part[1], kind = \"nodes\")\n",
    "    part2_nodes_tuple = [tuple(list_i)for list_i in part2_nodes]\n",
    "    part2_extremeties = list(set(part2_nodes_tuple) - set(shared_points_tuple))\n",
    "\n",
    "    part2_size = np.linalg.norm(compute_point_projection_onto_line(np.array(part2_extremeties[0]),\n",
    "                                        np.array(shared_points_list[0]), \n",
    "                                     np.array(shared_points_list[0])-np.array(shared_points_list[1]))- part2_extremeties[0])\n",
    "    \n",
    "\n",
    "    part2_extremeties_1 = np.array(part2_extremeties[0])\n",
    "    part2_extremeties_2 = np.array(part2_extremeties[1])\n",
    "    distance_to_point2_1 = np.linalg.norm(ref_shared_point - part1_extremeties_1)\n",
    "    distance_to_point2_2 = np.linalg.norm(ref_shared_point - part1_extremeties_2)\n",
    "\n",
    "    # Determine the most distant and closest points\n",
    "    most_distant_2 =  part2_extremeties_1 if distance_to_point2_1 > distance_to_point2_2 else part2_extremeties_2\n",
    "    closest_point_2 = part2_extremeties_1 if distance_to_point2_1 < distance_to_point2_2 else part2_extremeties_2\n",
    "\n",
    "    proposed_nodes = np.array([most_distant_2, closest_point_2, closest_point_1, most_distant_1])\n",
    "    proposed_nodes_normal = compute_normal_vector(proposed_nodes)\n",
    "    if are_parallel_and_same_direction(proposed_nodes_normal, original_normal):\n",
    "        nodes_tempo = proposed_nodes\n",
    "    else:\n",
    "        nodes_tempo = np.array([most_distant_1, closest_point_1, closest_point_2, most_distant_2])\n",
    "        proposed_nodes_normalx = compute_normal_vector(nodes_tempo)\n",
    "        # recheck to make sure\n",
    "        if not are_parallel_and_same_direction(proposed_nodes_normalx, original_normal): \n",
    "            raise MalissiaBaseError('can not compute complex_surface from exsiting nodes {}, check their order'.format(nodes_tempo))\n",
    "     \n",
    "    coor_lab = physical_space.coordinate_labels\n",
    "    nb_points = len(knowledge_framework().Point.instances())\n",
    "    new_planar_nodes = [\n",
    "        constructor_point(knowledge_framework,coor_lab ,\n",
    "                          [float(x) for x in nodes_tempo[0]], \n",
    "                          name= \"point_nb_{}_N0\".format(nb_points+1)),\n",
    "        constructor_point(knowledge_framework,coor_lab ,\n",
    "                          [float(x) for x in nodes_tempo[1]], \n",
    "                          name= \"point_nb_{}_N1\".format(nb_points+2)),\n",
    "        constructor_point(knowledge_framework,coor_lab ,\n",
    "                          [float(x) for x in nodes_tempo[2]], \n",
    "                          name= \"point_nb_{}_N2\".format(nb_points+3)),\n",
    "        constructor_point(knowledge_framework,coor_lab ,\n",
    "                          [float(x) for x in nodes_tempo[3]], \n",
    "                          name= \"point_nb_{}_N3\".format(nb_points+4)),                  \n",
    "        ]\n",
    "    return constructor_planar_surface_from_nodes(knowledge_framework, new_planar_nodes)\n",
    "\n",
    "\n",
    "def compute_center_complex_surface(knowledge_framework, physical_space, complex_surface, \n",
    "                                   extreme_faces = None, force_computing_for_fold = False,\n",
    "                                     assign_center_to_complex_surface = True,):\n",
    "    \"\"\"\n",
    "    Method to compute and attributting a center of complex surface. \n",
    "        It returns the midpoint between the centers of the two extreme faces. \n",
    "\n",
    "    Parameter:\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    \"\"\" \n",
    "    if force_computing_for_fold == True:\n",
    "       faces = extreme_faces\n",
    "       assign_center_to_complex_surface = False\n",
    "    else: \n",
    "        if len(complex_surface.has_Part )< 2 :\n",
    "            raise MalissiaBaseError('the complex surface should have > 2 parts') \n",
    "        else: \n",
    "            faces =  complex_surface.has_Part\n",
    "    center = np.mean([np.array(physical_space.get_object_coordinates(face_i, \"center\")) for face_i in faces \\\n",
    "                       if physical_space.get_object_coordinates(face_i, \"center\") is not None ], axis = 0)\n",
    "\n",
    "    if assign_center_to_complex_surface:\n",
    "        center_formated =  constructor_point(knowledge_framework, physical_space.coordinate_labels, [float(n_i) for n_i in center])\n",
    "        complex_surface.has_Center = [center_formated]\n",
    "    return center\n",
    "    \n",
    "\n",
    "def compute_size_complex_surface_for_representation(knowledge_framework, physical_space, complex_surface, extreme_faces = None):\n",
    "    \"\"\"\n",
    "    Method to compute a rough size of a complex_surface or a fold for representation and projection of explained objects.\n",
    "     It returns the distance between the farthest nodes of the extreme nodes of the complex_surface.\n",
    "\n",
    "    Parameter:\n",
    "    - extreme_faces : extreme faces of the complex_surface if already identified\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    \"\"\" \n",
    "    extreme_faces = compute_complex_surface_extreme_surfaces(knowledge_framework, \n",
    "                            complex_surface, return_none= True) if extreme_faces is None else extreme_faces\n",
    "    if extreme_faces is None: # temporary solution to avoid representation broking\n",
    "        return complex_surface.has_Part[0].size[0]\n",
    "    if len(extreme_faces) != 2:\n",
    "        raise MalissiaBaseError('problm with calculating extreme faces')\n",
    "    extremeties = compute_extremeties_from_two_extreme_surfaces(extreme_faces, physical_space)\n",
    "    if len(extremeties) != 2:\n",
    "        raise MalissiaBaseError('problm with calculating extremeties')\n",
    "    node1 = np.array([\n",
    "        extremeties[0][0]])\n",
    "    nodes = [extremeties[1][0],\n",
    "        extremeties[1][1],]\n",
    "    farthest = np.array(find_farthest_node_from_nodes(nodes, node1))\n",
    "    size =  float(np.round(np.linalg.norm(node1 - farthest), 2))\n",
    "    return size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updated complex_surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_type(knowledge_framework, anomaly):\n",
    "    anomaly_type = list(set(anomaly.is_a).intersection(set(knowledge_framework().Anomaly.subclasses())))\n",
    "    if len(anomaly_type) != 1:\n",
    "        raise MalissiaBaseError('the introduced anomaly has more then one type: {}'.format(anomaly.name, anomaly_type))\n",
    "    else: return anomaly_type[0]\n",
    "    \n",
    "\n",
    "def updated_complex_surface_constructor_after_anomaly(knowledge_framework, physical_space, anomaly, anomaly_type ):\n",
    "    if anomaly_type != knowledge_framework().ComplexSurfaceSizeSharedSidesAnomaly:\n",
    "        raise MalissiaNotImplementedYet('constructor for anomaly type {} not implemented yet'.format(anomaly_type.name))\n",
    "    complex_surface = anomaly.is_Related_To[0]\n",
    "    return compute_larger_complex_surface(knowledge_framework, physical_space, complex_surface)\n",
    "\n",
    "\n",
    "\n",
    "def compute_larger_complex_surface(knowledge_framework, physical_space, complex_surface):\n",
    "    if shared_sides_are_equal(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                            physical_space =  physical_space) == True \\\n",
    "                         and  shared_sides_have_uniform_allignement(complex_surface  = complex_surface, \n",
    "                             knowledge_framework  = knowledge_framework, physical_space =  physical_space) == True:\n",
    "        return complex_surface\n",
    "    \n",
    "\n",
    "    shared_sides = get_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, physical_space)\n",
    "    unmatched_sides = get_unmatched_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, physical_space)\n",
    "    sides = shared_sides + unmatched_sides\n",
    "    if len(sides) < 1 :\n",
    "        raise MalissiaBaseError ('Warning: there is no shared sides or unmatched sides with complex_surface {}'.format(complex_surface.name))\n",
    "    \n",
    "\n",
    "    init_size = -1\n",
    "    reference_side = None\n",
    "    for side_i in sides:\n",
    "        if side_i.size[0] > init_size:\n",
    "            reference_side = side_i\n",
    "            init_size = side_i.size[0]\n",
    "\n",
    "    p_0_side = physical_space.get_object_coordinates(reference_side.has_End_Points[0], \"node\")[0]\n",
    "    p_1_side = physical_space.get_object_coordinates(reference_side.has_End_Points[1], \"node\")[0]\n",
    "    reference_vec = np.array(p_0_side)-np.array(p_1_side)\n",
    "    projected_sides_nodes = []\n",
    "\n",
    "    for side_i in sides :\n",
    "        side_i_p0 =  np.array(physical_space.get_object_coordinates(side_i.has_End_Points[0], \"node\")[0])\n",
    "        side_i_p1 = np.array(physical_space.get_object_coordinates(side_i.has_End_Points[1], \"node\")[0])\n",
    "\n",
    "        projected_sides_nodes.append(compute_point_projection_onto_line(side_i_p0, p_0_side, reference_vec))\n",
    "        projected_sides_nodes.append(compute_point_projection_onto_line(side_i_p1, p_0_side, reference_vec))\n",
    "\n",
    "    info = compute_projection_info (projected_sides_nodes,  reference_vec)\n",
    "    node_of_extension_1 = info['start_point'] \n",
    "    node_of_extension_2 = info['end_point'] \n",
    "    \n",
    "\n",
    "    new_complex_surface = compute_larger_complex_surface_from_complex_surface_laterally(complex_surface, \n",
    "                                                                    node_of_extension_1, knowledge_framework, physical_space)\n",
    "\n",
    "    if shared_sides_are_equal(complex_surface  = new_complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                         physical_space =  physical_space)== True \\\n",
    "                and  shared_sides_have_uniform_allignement (complex_surface  = new_complex_surface,\n",
    "                                                            knowledge_framework  = knowledge_framework, \n",
    "                                                           physical_space =  physical_space) == True:\n",
    "        return new_complex_surface\n",
    "    \n",
    "    else:\n",
    "        return compute_larger_complex_surface_from_complex_surface_laterally(new_complex_surface, node_of_extension_2 \n",
    "                                                                             ,knowledge_framework, physical_space)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def common_two_nodes_two_planars(nodes1, nodes2, return_error = True):\n",
    "    \"\"\"\n",
    "    Method to extract common two nodes of two planars if exsist.\n",
    "    It return a list of two shared nodes if they are found, else it returns an error. In case return_error == False, \n",
    "        the method will return a none type if nothing is found. \n",
    "\n",
    "    Parameter:\n",
    "    - nodes1, nodes2 : lists of nodes of the two planars\n",
    "    - return_error : boolean by default is True. If False, the method returns None object\n",
    "    \"\"\" \n",
    "    if type(nodes1) != list:\n",
    "        nodes1 = nodes1.tolist()\n",
    "    if type(nodes2) != list:\n",
    "        nodes2 = nodes2.tolist()\n",
    "\n",
    "    common_nodes = [node_i for node_i in nodes1 if node_i in nodes2]\n",
    "    if len(common_nodes) == 2:\n",
    "        return common_nodes \n",
    "    else:\n",
    "        if return_error == True :\n",
    "            raise MalissiaBaseError('there is {} common nodes '.format(len(common_nodes)))\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "def project_point_onto_plane(point, vertices):\n",
    "    \"\"\"\n",
    "    Method to project a point on a plane defined by its vertices. It returns an np.array [x,y,z] of the projected point.\n",
    "\n",
    "    Parameter:\n",
    "    - point : point to be projected\n",
    "    - vertices : vertices of the surface hold by the plane on which the point will be projected\n",
    "    \"\"\" \n",
    "    # Find two vectors within the plane\n",
    "    vector1 = vertices[1] - vertices[0]\n",
    "    vector2 = vertices[3] - vertices[0]\n",
    "\n",
    "    # Calculate the normal vector to the plane\n",
    "    normal_vector = np.cross(vector1, vector2)\n",
    "\n",
    "    # Calculate the projection of the point onto the plane\n",
    "    projection = point - np.dot(point - vertices[0], normal_vector) / np.dot(normal_vector, normal_vector) * normal_vector\n",
    "\n",
    "    return projection\n",
    "\n",
    "\n",
    "def multiple_parts_constructor_for_complex_surface(knowledge_framework, new_complex_surface,\n",
    "                                                                 physical_space, list__of_parts_nodes):\n",
    "    \"\"\"\n",
    "    Method to construct multiple parts in a complex surface. It returns a list of constructed parts\n",
    "    \n",
    "\n",
    "    Parameter:\n",
    "    - knowledge_framework : the knowledge_framework\n",
    "    - new_complex_surface : the new complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - list__of_parts_nodes : list of parts nodes that will be construced\n",
    "    \"\"\" \n",
    "    constructed_parts = []\n",
    "    \n",
    "    if len(list__of_parts_nodes) == 0 :\n",
    "        return new_complex_surface\n",
    "    for nodes_i in list__of_parts_nodes:\n",
    "        nb_nodes = len(knowledge_framework().Point.instances())\n",
    "        nb_parts = len(knowledge_framework().Planar_Surface.instances())\n",
    "        name_of_part = 'part_{}'.format(nb_parts+1)\n",
    "        nodes= [constructor_point(knowledge_framework, physical_space.coordinate_labels, nodes_i[0], name=\"N_nb_{}\".format(nb_nodes+1)),\n",
    "                constructor_point(knowledge_framework, physical_space.coordinate_labels, nodes_i[1], name=\"N_nb_{}\".format(nb_nodes+2)),\n",
    "                constructor_point(knowledge_framework, physical_space.coordinate_labels, nodes_i[2], name=\"N_nb_{}\".format(nb_nodes+3)),\n",
    "                constructor_point(knowledge_framework, physical_space.coordinate_labels, nodes_i[3], name=\"N_nb_{}\".format(nb_nodes+4))]\n",
    "        x_part = constructor_planar_surface_from_nodes(knowledge_framework, nodes, name= name_of_part)\n",
    "        new_complex_surface.has_Part.append(x_part)\n",
    "        constructed_parts.append(x_part)\n",
    "    neighbor_constructor_from_multiple_planars(new_complex_surface, knowledge_framework, physical_space )\n",
    "    return constructed_parts\n",
    "    \n",
    "def find_farthest_node_from_nodes(nodes, proposed_node_of_extension):\n",
    "    \"\"\"\n",
    "    Method to find the farthest point amongest a list of nodes from a referene point 'proposed_node_of_extension'. \n",
    "        it returns an np.array [x,y,z] of the farthest node.\n",
    "    \n",
    "    Parameter:\n",
    "    - nodes : nodes\n",
    "    - proposed_node_of_extension : proposed_node_of_extension\n",
    "    \"\"\" \n",
    "    return max(nodes, key=lambda node_i: np.linalg.norm(np.array(node_i) - np.array(proposed_node_of_extension)))\n",
    "\n",
    "\n",
    "def is_point_on_line_projection(point, line_start, line_end, tolerance=0.1):\n",
    "    \"\"\"\n",
    "    Method to find if a point projection is on a line segment. It returns a True or False Value.\n",
    "    \n",
    "    Parameter:\n",
    "    - point : point to be projected and tested\n",
    "    - line_start, line_end : points that define the line segment\n",
    "    - tolerance : a small value to account for numerical imprecision\n",
    "    \"\"\" \n",
    "    point = np.array(point)\n",
    "    line_start = np.array(line_start)\n",
    "    line_end = np.array(line_end)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Calculate the projection of the point onto the line\n",
    "    projection = line_start + np.dot(point - line_start, \n",
    "                                     line_end - line_start) / np.linalg.norm(line_end \\\n",
    "                                                                             - line_start)**2 * (line_end - line_start)\n",
    "    if np.round(np.linalg.norm(projection - line_start),1) <= tolerance :\n",
    "        return True\n",
    "    if np.round(np.linalg.norm(projection - line_end),1) <= tolerance :\n",
    "        return True\n",
    "    # Check if the projection lies on the line segment with tolerance\n",
    "    on_segment = 0 <= np.dot(projection - line_start, line_end - line_start) \\\n",
    "                            <= np.linalg.norm(line_end - line_start)**2 + tolerance\n",
    "    \n",
    "    return on_segment\n",
    "\n",
    "def vectors_size_is_equal_for_surface_upgrade(oldv1_mag, oldv2_mag, newv1_mag, newv2_mag):\n",
    "    oldv1_mag = np.round(oldv1_mag, 3)\n",
    "    oldv2_mag = np.round(oldv2_mag, 3)\n",
    "    newv1_mag = np.round(newv1_mag, 3)\n",
    "    newv2_mag = np.round(newv2_mag, 3)\n",
    "    return newv1_mag == oldv1_mag or newv2_mag == oldv2_mag\n",
    "\n",
    "def check_vectors_size_for_surface_upgrade(oldv1_mag, oldv2_mag, newv1_mag, newv2_mag):\n",
    "    \"\"\"\n",
    "    Method to check that new vectors 'two sides' of a new surface are greater then the old ones. \n",
    "    It returns True or false value. three cases are accepted and give a True value:\n",
    "        if the two vectors are greater then the old ones\n",
    "        one of the vectors is greater is the other one is at least equal to the old one\n",
    "        one vector is greater in the other vector is null. \n",
    "\n",
    "    Parameter:\n",
    "    - oldv1_mag : old_vector 1 magnitude\n",
    "    - oldv2_mag : old_vector 2 magnitude\n",
    "    - newv1_mag : new_vector 1 magnitude\n",
    "    - newv2_mag : new_vector 2 magnitude \n",
    "    \"\"\" \n",
    "    oldv1_mag = np.round(oldv1_mag, 3)\n",
    "    oldv2_mag = np.round(oldv2_mag, 3)\n",
    "    newv1_mag = np.round(newv1_mag, 3)\n",
    "    newv2_mag = np.round(newv2_mag, 3)\n",
    "    # Check if both new vectors are greater than or equal to the old vectors\n",
    "    condition1 = newv1_mag >= oldv1_mag  and newv2_mag >= oldv2_mag\n",
    "    condition2 = newv1_mag == oldv1_mag and newv2_mag == oldv2_mag\n",
    "\n",
    "    # Check if one new vector has a magnitude of 0 while the other is greater than the old one\n",
    "    condition3 = newv1_mag == 0 and newv2_mag > oldv2_mag or newv2_mag == 0 and newv1_mag > oldv1_mag\n",
    "    if (condition1 == True and condition2 == False) or condition3 == True:\n",
    "        return True\n",
    "    #temporary fix (todo must treat topology of nodes)\n",
    "    ratio1 = np.round(newv1_mag/newv2_mag, 1)\n",
    "    ratio2 = np.round(newv2_mag/newv1_mag, 1)\n",
    "\n",
    "    condition4 = ratio1 == 0 and newv2_mag > oldv2_mag or ratio2 == 0 and newv1_mag > oldv1_mag\n",
    "    if  condition4:\n",
    "        return True\n",
    "    # end of the fix \n",
    "\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def check_point_location_for_surface_upgrade(vector_of_projection, vector_0_1, vector_0_3):\n",
    "    \"\"\"\n",
    "    Method to check if the point of extension is located in the inner side of the surface to be upgraded or outside.\n",
    "        It returns It returns True or false value. \n",
    "\n",
    "    Parameter:\n",
    "    - vector_of_projection : the vector of projection point of extension -point to fix\n",
    "    - vector_0_1 : vector from nodes 0 to 1 of the surface to be extended 0 is the node to fix\n",
    "    - vector_0_3 : vector from nodes 0 to 3 of the surface to be extended 0 is the node to fix\n",
    "    \"\"\" \n",
    "    condition1 = np.round(np.dot(vector_of_projection, vector_0_1), 3) >= 0\n",
    "    condition2 = np.round(np.dot(vector_of_projection, vector_0_3), 3) >= 0\n",
    "    return condition1 and condition2\n",
    "    \n",
    "def compute_larger_surface_by_extension(node0, node1, node2, node3,\n",
    "                                         node_to_fix, proposed_node_of_extension,\n",
    "                                        force_axis_direction = None):\n",
    "    \"\"\"\n",
    "    Method to extend a surface based on a point that must be located outside the existing surface and the vector from \n",
    "        the node to fix to the node of extension must point to the inner part of the surface 'between the two vectors that define\n",
    "        the surface and start at the point_to_fix' \n",
    "    if needed, the extension could be forced on a given axis, in this case, an axis in force_axis_direction must be given\n",
    "    Parameters:\n",
    "    - node0, node1, node2, node3 : four nodes of the surface to be extended.\n",
    "    - node_to_fix : the node to fix \n",
    "    - proposed_node_of_extension : the proposed_node_of_extension\n",
    "    - force_axis_direction = by default is None, or it could takes the type of an np.array[x,y,z] of a vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    if force_axis_direction is not None:\n",
    "        force_axis_direction /= np.linalg.norm(force_axis_direction)\n",
    "    nodes = np.array([node0, node1, node2, node3])\n",
    "    for i, node in enumerate(nodes):\n",
    "        if np.array_equal(node, node_to_fix): # Check if the current node matches the node_to_fix\n",
    "            # If so, set new_node_x and new_node_y accordingly\n",
    "            modified_node_3 = nodes[(i - 1) % 4]  # Previous node\n",
    "            modified_node_1 = nodes[(i + 1) % 4]  # Next node\n",
    "            modified_node_0 = node\n",
    "            modified_node_2 = nodes[(i + 2) % 4] \n",
    "\n",
    "            vector_0_1 = modified_node_1 - modified_node_0 \n",
    "            side_0_1 = np.linalg.norm(vector_0_1)\n",
    "            vector_0_1/= side_0_1\n",
    "            vector_0_3 = modified_node_3 - modified_node_0 \n",
    "            side_0_3 = np.linalg.norm(vector_0_3)\n",
    "            vector_0_3/= side_0_3\n",
    "\n",
    "            # check the vector of extension is not pointing out outside the surface            \n",
    "            vector_of_extension = np.array(proposed_node_of_extension)- np.array(node_to_fix)\n",
    "            check_orientation_condtion = check_point_location_for_surface_upgrade( vector_of_extension, vector_0_1, vector_0_3)\n",
    "            if check_orientation_condtion == False:\n",
    "                raise  MalissiaBaseError(' the vector from point_to_fix to the projected point is pointing outside the surface')\n",
    "            # check that the point of extension is on the same plane as the surface                                                                  \n",
    "            vector_of_extension_normalized = vector_of_extension / np.linalg.norm(vector_of_extension)\n",
    "            cross_p_surface_axis = (np.cross(vector_0_1, vector_0_3)) / np.linalg.norm(np.cross(vector_0_1, vector_0_3)) \n",
    "            cross_p_axis_1_proj = (np.cross(vector_0_1, vector_of_extension_normalized)) / np.linalg.norm(np.cross(vector_0_1, vector_of_extension_normalized))\n",
    "            if all(cross_p_surface_axis != cross_p_axis_1_proj) and np.round(np.linalg.norm(np.cross(vector_0_1, vector_of_extension_normalized)),3) !=0:\n",
    "\n",
    "                raise  MalissiaBaseError(' the projected point is not on the same plane that bears the candidate surface')\n",
    "            \n",
    "            if force_axis_direction is None :\n",
    "                projection_on_vector_0_1 = compute_point_projection_onto_line(proposed_node_of_extension,\n",
    "                                                                            modified_node_0,  vector_0_1)\n",
    "                vec_to_projection_on_vector_0_1 = projection_on_vector_0_1 - modified_node_0\n",
    "                vec_to_projection_on_vector_0_1_mag = np.linalg.norm(vec_to_projection_on_vector_0_1)\n",
    "                if vec_to_projection_on_vector_0_1_mag != 0:\n",
    "                    vec_to_projection_on_vector_0_1/=vec_to_projection_on_vector_0_1_mag\n",
    "\n",
    "                projection_on_vector_0_3 = compute_point_projection_onto_line(proposed_node_of_extension,\n",
    "                                                                            modified_node_0,  vector_0_3)\n",
    "                vec_to_projection_on_vector_0_3 = projection_on_vector_0_3 - modified_node_0\n",
    "                vec_to_projection_on_vector_0_3_mag = np.linalg.norm(vec_to_projection_on_vector_0_3)\n",
    "                if vec_to_projection_on_vector_0_3_mag != 0:\n",
    "                    vec_to_projection_on_vector_0_3 /= vec_to_projection_on_vector_0_3_mag\n",
    "\n",
    "            elif np.round(np.dot(force_axis_direction, vector_0_1), 2) == 1:\n",
    "                projection_on_vector_0_1 = compute_point_projection_onto_line(proposed_node_of_extension,\n",
    "                                                            modified_node_0,  vector_0_1)\n",
    "                vec_to_projection_on_vector_0_1 = projection_on_vector_0_1 - modified_node_0\n",
    "                vec_to_projection_on_vector_0_1_mag = np.linalg.norm(vec_to_projection_on_vector_0_1)\n",
    "                vec_to_projection_on_vector_0_1/=vec_to_projection_on_vector_0_1_mag\n",
    "                projection_on_vector_0_3 = modified_node_0\n",
    "                vec_to_projection_on_vector_0_3 = np.array([0.,0.,0.])\n",
    "                vec_to_projection_on_vector_0_3_mag = 0.\n",
    "\n",
    "            elif np.round(np.dot(force_axis_direction, vector_0_3), 2) == 1:\n",
    "                projection_on_vector_0_3 = compute_point_projection_onto_line(proposed_node_of_extension,\n",
    "                                                            modified_node_0,  vector_0_3)\n",
    "                vec_to_projection_on_vector_0_3 = projection_on_vector_0_3 - modified_node_0\n",
    "                vec_to_projection_on_vector_0_3_mag = np.linalg.norm(vec_to_projection_on_vector_0_3)\n",
    "                vec_to_projection_on_vector_0_3/=vec_to_projection_on_vector_0_3_mag\n",
    "\n",
    "                projection_on_vector_0_1 = modified_node_0\n",
    "                vec_to_projection_on_vector_0_1 = np.array([0.,0.,0.])\n",
    "                vec_to_projection_on_vector_0_1_mag = 0.  \n",
    "\n",
    "            elif  np.round(np.dot(force_axis_direction, vector_0_3), 3) == -1 or np.round(np.dot(force_axis_direction, vector_0_1), 3) == -1:\n",
    "                raise MalissiaBaseError('the forced vector starting from the point_to_fix is pointing outside the surface \"must be in the opposite direction\" ')    \n",
    "            else:\n",
    "                raise MalissiaBaseError(' forced vector starting from the point_to_fix must be parallel  at least to one side of the surface')\n",
    "            \n",
    "            check_size_condtions = check_vectors_size_for_surface_upgrade(side_0_1, side_0_3, \n",
    "                                                 vec_to_projection_on_vector_0_1_mag, \n",
    "                                                 vec_to_projection_on_vector_0_3_mag)\n",
    "            \n",
    "            if check_size_condtions == False:\n",
    "                raise MalissiaBaseError('the proposed extension must be larger than the existing surface, at least on one side, and must not truncate the existing one')\n",
    "            else:\n",
    "                if np.round(vec_to_projection_on_vector_0_1_mag,3) == 0:\n",
    "                    new_node_0 = [float(i) for i in modified_node_0]\n",
    "                    new_node_1 = [float(i) for i in modified_node_1] \n",
    "                    new_node_2 = compute_endpoint(new_node_1, vec_to_projection_on_vector_0_3, \n",
    "                                              vec_to_projection_on_vector_0_3_mag)\n",
    "                    new_node_2 = [float(i) for i in new_node_2]  \n",
    "                    new_node_3 = [float(i) for i in projection_on_vector_0_3] \n",
    "                    return np.array([new_node_0, new_node_1, new_node_2, new_node_3])\n",
    "\n",
    "                if np.round(vec_to_projection_on_vector_0_3_mag,3)  == 0:\n",
    "                    new_node_0 = [float(i) for i in modified_node_0]\n",
    "                    new_node_1 = [float(i) for i in projection_on_vector_0_1] \n",
    "                    new_node_3 = [float(i) for i in modified_node_3] \n",
    "                    new_node_2 = compute_endpoint(new_node_3, vec_to_projection_on_vector_0_1, \n",
    "                                              vec_to_projection_on_vector_0_1_mag)\n",
    "                    new_node_2 = [float(i) for i in new_node_2]\n",
    "\n",
    "                    return np.array([new_node_0, new_node_1, new_node_2, new_node_3])\n",
    "                else:\n",
    "                    new_node_0 = [float(i) for i in modified_node_0]\n",
    "                    new_node_1 = [float(i) for i in projection_on_vector_0_1] \n",
    "                    new_node_2 = [float(i) for i in proposed_node_of_extension] \n",
    "                    new_node_3 =  [float(i) for i in projection_on_vector_0_3] \n",
    "                    return np.array([new_node_0, new_node_1, new_node_2, new_node_3])\n",
    "                   \n",
    "            break  # Exit the loop if a match is found\n",
    "    else:\n",
    "        raise MalissiaBaseError(\"node_to_fix not in the surface nodes\")\n",
    "    \n",
    "\n",
    "\n",
    "def compute_larger_complex_surface_from_complex_surface_laterally(complex_surface, node_of_extension,\n",
    "                                                                   knowledge_framework, physical_space, tolerance = 0.5,\n",
    "                                                                   new_complex_surface = None, ):\n",
    "    \n",
    "\n",
    "    neighbor_constructor_from_multiple_planars(complex_surface, knowledge_framework, physical_space)\n",
    "    parts = complex_surface.has_Part\n",
    "    if len(parts) < 2:\n",
    "        raise MalissiaBaseError('complex_surface {} has {} parts, maybe try method: compute_larger_surface_by_extension'.fromat(complex_surface.name, \n",
    "                                                                                                                                len(parts)))\n",
    "    node_of_extension = np.array(node_of_extension)\n",
    "    shared_sides = get_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, physical_space)\n",
    "    unmathced_sides = get_unmatched_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, physical_space)\n",
    "    sides = shared_sides + unmathced_sides \n",
    "    if len(sides) < 1 :\n",
    "        print('Warning: complex_surface {} parts have no shared or unmatched sides'.format(complex_surface.name))\n",
    "        return None\n",
    "    \n",
    "    if new_complex_surface is None:\n",
    "        nb_of_cplx_sur = len(knowledge_framework().Complex_Surface.instances())\n",
    "        new_complex_surface_name = str('cplx_sur_nb_{}'.format(nb_of_cplx_sur+1))\n",
    "        new_complex_surface = knowledge_framework().Complex_Surface(name = new_complex_surface_name, non_planar = True)\n",
    "\n",
    "    unchanged_parts = []\n",
    "    extended_parts = []\n",
    "    new_parts_nodes = []\n",
    "    for side_i in shared_sides:\n",
    "        start_point = physical_space.get_object_coordinates(side_i.has_End_Points[0], \"node\")[0]\n",
    "        end_point = physical_space.get_object_coordinates(side_i.has_End_Points[1], \"node\")[0]\n",
    "        vec_shared_side = np.array(start_point)-np.array(end_point)\n",
    "        projected_node_on_vec = compute_point_projection_onto_line(node_of_extension, start_point, vec_shared_side)\n",
    "        node_to_fix = find_farthest_node_from_nodes([start_point, end_point], projected_node_on_vec)\n",
    "        closest_point = start_point if set(end_point) == set(node_to_fix) else end_point\n",
    "        \n",
    "        # temporary fix (todo : work on topolgy of common nodes)\n",
    "        if np.linalg.norm(np.array(closest_point) - np.array(projected_node_on_vec)) <= tolerance:\n",
    "            for part in side_i.is_Shared_Side_Of:\n",
    "                if part not in unchanged_parts:\n",
    "                    unchanged_parts.append(part)\n",
    "        # end of the fix           \n",
    "        if set(closest_point) == set(projected_node_on_vec):\n",
    "            for part in side_i.is_Shared_Side_Of:\n",
    "                if part not in unchanged_parts:\n",
    "                    unchanged_parts.append(part)\n",
    "        if not is_point_on_line_projection(projected_node_on_vec, start_point, end_point):\n",
    "            for part_i in side_i.is_Shared_Side_Of:\n",
    "                if part_i not in extended_parts :\n",
    "                    part_i_nodes = physical_space.get_object_coordinates(part_i, \"nodes\")\n",
    "                    new_part_nodes = compute_larger_surface_by_extension(*part_i_nodes, node_to_fix, projected_node_on_vec)\n",
    "                    extended_parts.append(part_i)\n",
    "                    new_parts_nodes.append(new_part_nodes)\n",
    "\n",
    "    for unma_side_i in unmathced_sides:\n",
    "        start_point = physical_space.get_object_coordinates(unma_side_i.has_End_Points[0], \"node\")[0]\n",
    "        end_point = physical_space.get_object_coordinates(unma_side_i.has_End_Points[1], \"node\")[0]\n",
    "        vec_shared_side = np.array(start_point)-np.array(end_point)\n",
    "        projected_node_on_vec = compute_point_projection_onto_line(node_of_extension, start_point, vec_shared_side)\n",
    "        node_to_fix = find_farthest_node_from_nodes([start_point, end_point], projected_node_on_vec)\n",
    "        closest_point = start_point if set(end_point) == set(node_to_fix) else end_point\n",
    "\n",
    "        # temporary fix (todo : work on topolgy of common nodes)\n",
    "        if np.linalg.norm(np.array(closest_point) - np.array(projected_node_on_vec)) <= tolerance:\n",
    "            for part in side_i.is_Shared_Side_Of:\n",
    "                if part not in unchanged_parts:\n",
    "                    unchanged_parts.append(part)\n",
    "        # end of the fix      \n",
    "                                  \n",
    "        if set(closest_point) == set(projected_node_on_vec):\n",
    "            for part in side_i.is_Unmatched_Shared_Side_Of:\n",
    "                if part not in unchanged_parts:\n",
    "                    unchanged_parts.append(part)\n",
    "        if not is_point_on_line_projection(projected_node_on_vec, start_point, end_point):\n",
    "            for part_i in unma_side_i.is_Unmatched_Shared_Side_Of:\n",
    "                if part_i not in extended_parts :\n",
    "                    part_i_nodes = physical_space.get_object_coordinates(part_i, \"nodes\")\n",
    "                    new_part_nodes = compute_larger_surface_by_extension(*part_i_nodes, node_to_fix, projected_node_on_vec)\n",
    "                    extended_parts.append(part_i)\n",
    "                    new_parts_nodes.append(new_part_nodes)\n",
    "    \n",
    "    for part_i in parts:\n",
    "        if part_i not in extended_parts and part_i not in unchanged_parts:\n",
    "            unchanged_parts.append(part_i)\n",
    "    multiple_parts_constructor_for_complex_surface(knowledge_framework, new_complex_surface,physical_space, new_parts_nodes)\n",
    "    new_complex_surface.has_Part.extend(unchanged_parts)\n",
    "\n",
    "    neighbor_constructor_from_multiple_planars(complex_surface=  new_complex_surface, \n",
    "                                               knowledge_framework= knowledge_framework, physical_space= physical_space )\n",
    "    attribute_parts_nodes_to_complex_surface(new_complex_surface)\n",
    "\n",
    "    return new_complex_surface\n",
    "\n",
    "\n",
    "\n",
    "# methods to be updated (porblem with neighbors successions)\n",
    "def compute_larger_surface_by_extension_of_two_non_planars_complex_surface(knowledge_framework,\n",
    "                                                                                 physical_space, \n",
    "                                                                                 first_face_to_extend, \n",
    "                                                                                 second_face_to_extend, \n",
    "                                                           node_to_fix, proposed_node_of_extension, \n",
    "                                                           new_complex_surface = None, force_axis = None,\n",
    "                                                             force_projection_of_point_on_part1_plane = True, \n",
    "                                                              force_construction_of_first_part = True ):\n",
    "    \"\"\"\n",
    "    Method to extend create a ne complex surface by the extension of an exsiting non_planar complex_surface that has two parts.\n",
    "        The extension coild ne forced on an axis. The point of extension could be projected at the plane of the first face to extend if needed.\n",
    "        The method could construct both new parts, or only one and leave the first part. It returns a new_complex_surface.\n",
    "    Parameter:\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - first_face_to_extend : first_face_to_extend\n",
    "    - second_face_to_extend : second_face_to_extend\n",
    "    - node_to_fix : node_to_fix\n",
    "    - proposed_node_of_extension : proposed_node_of_extension \n",
    "    - new_complex_surface : new_complex_surface\n",
    "    - force_axis : by default is None, or it could takes the type of an np.array[x,y,z] of a vector.\n",
    "    - coord_labels : coord_labels\n",
    "    - force_projection_of_point_on_part1_plane : boolean by default True. If True it forces the projection of the node of extension on the first plane to extend\n",
    "    - force_construction_of_first_part : boolean by default True. If True, it forces the construction of new part_1 and not using the existing part\n",
    "    \"\"\" \n",
    "    if new_complex_surface == None : \n",
    "        nb = len(knowledge_framework().Complex_Surface.instances())\n",
    "        complex_surface_name = str('cplx_sur_nb_{}_by_ext_{}_and_{}_'.format(nb, \n",
    "                            first_face_to_extend.name, second_face_to_extend.name))\n",
    "        new_complex_surface = knowledge_framework().Complex_Surface( name = complex_surface_name)\n",
    "    \n",
    "    \n",
    "    coord_labels = physical_space.coordinate_labels\n",
    "    # get candidate stratigraphic_parts paramters\n",
    "    n1 = np.array(physical_space.get_object_coordinates( object= first_face_to_extend, kind= \"normal\")) \n",
    "    n2 = np.array(physical_space.get_object_coordinates( object= second_face_to_extend, kind= \"normal\")) \n",
    "    c1 = np.array(physical_space.get_object_coordinates( object= first_face_to_extend, kind= \"center\")) \n",
    "    c2 = np.array(physical_space.get_object_coordinates( object= second_face_to_extend, kind= \"center\")) \n",
    "\n",
    "    nodes1 = np.array(physical_space.get_object_coordinates( object= first_face_to_extend, kind= \"nodes\")) \n",
    "    nodes2 = np.array(physical_space.get_object_coordinates( object= second_face_to_extend, kind= \"nodes\")) \n",
    "\n",
    "    common_two_nodes = common_two_nodes_two_planars(nodes1, nodes2)\n",
    "    axis = np.array(common_two_nodes[0])-np.array(common_two_nodes[1])\n",
    "\n",
    "    proposed_node_of_extension = project_point_onto_plane(proposed_node_of_extension, nodes1) if force_projection_of_point_on_part1_plane == True else proposed_node_of_extension\n",
    "\n",
    "    center1_proj_on_axis = np.array(compute_point_projection_onto_line(c1, np.array(common_two_nodes[0]), axis))\n",
    "    forbidden_axis_for_part1 = center1_proj_on_axis-c1\n",
    "    forbidden_axis_for_part1/= np.linalg.norm(forbidden_axis_for_part1) \n",
    "    center2_proj_on_axis = np.array(compute_point_projection_onto_line(c2, np.array(common_two_nodes[0]), axis))\n",
    "    forbidden_axis_for_part2 = center2_proj_on_axis-c2\n",
    "    forbidden_axis_for_part2/= np.linalg.norm(forbidden_axis_for_part2)\n",
    "\n",
    "    extention_axis_part1 = np.array(proposed_node_of_extension) - np.array(node_to_fix) \n",
    "    extention_axis_part1/= np.linalg.norm(extention_axis_part1)\n",
    "    if force_axis is not None:\n",
    "        force_axis /= np.linalg.norm(force_axis)\n",
    "    \n",
    "    if all(extention_axis_part1 == forbidden_axis_for_part1) or all(force_axis == forbidden_axis_for_part1):\n",
    "        raise MalissiaBaseError('the proposed surface cannot be extend in the direction toward the intersection of the surfaces')\n",
    "    \n",
    "    new_nodes_part1_temp = compute_larger_surface_by_extension(*nodes1,node_to_fix, proposed_node_of_extension,\n",
    "                                        force_axis_direction = force_axis)     \n",
    "    n = 0 \n",
    "    n_part_cpmlx_sur = len(new_complex_surface.has_Part)+1\n",
    "    new_nodes_part1 = []\n",
    "    for n_i in new_nodes_part1_temp:\n",
    "        name= 'N{}_{}'.format(n, n_part_cpmlx_sur)\n",
    "        n +=1\n",
    "        n_part_cpmlx_sur +=1\n",
    "        n_i =  [float(i) for i in n_i]\n",
    "        new_nodes_part1.append(constructor_point(knowledge_framework, coord_labels, n_i, name))\n",
    "   \n",
    "    projection_proposed_node_of_extension_2 = project_point_onto_plane(proposed_node_of_extension, nodes2)\n",
    "\n",
    "    new_nodes_part2_temp = compute_larger_surface_by_extension(*nodes2, \n",
    "                                                               node_to_fix, \n",
    "                                                               projection_proposed_node_of_extension_2,\n",
    "                                        force_axis_direction = force_axis) \n",
    "    n = 0 \n",
    "    n_part_cpmlx_sur += 1\n",
    "    new_nodes_part2 = []\n",
    "    for n_ii in new_nodes_part2_temp:\n",
    "        name= 'N{}_{}'.format(n, n_part_cpmlx_sur)\n",
    "        n +=1\n",
    "        n_part_cpmlx_sur +=1\n",
    "        n_ii =  [float(ii) for ii in n_ii]\n",
    "        new_nodes_part2.append(constructor_point(knowledge_framework, coord_labels, n_ii, name))\n",
    "\n",
    "    common_nodes = common_two_nodes_two_planars(new_nodes_part1_temp, new_nodes_part2_temp)\n",
    "    share_sides_nb = len(knowledge_framework().Side.instances())+1\n",
    "    point_nb =  len(knowledge_framework().Point.instances())+1\n",
    "    end_point_0 = constructor_point(knowledge_framework, coord_labels,\n",
    "                        [float(x) for x in common_nodes[0]], name=\"end_point_nb_{}\".format(point_nb))\n",
    "    end_point_1 = constructor_point(knowledge_framework, coord_labels,\n",
    "                        [float(x) for x in common_nodes[0]], name=\"end_point_nb_{}\".format(point_nb+1))\n",
    "    share_sides_name = str('shared_side_nb_{}_between_{}_and_{}'.format(share_sides_nb, end_point_0.name, end_point_1.name))\n",
    "    side_size = float(np.linalg.norm(np.array(common_nodes[0])-np.array(common_nodes[1])))\n",
    "    shared_side = knowledge_framework().Side(name = share_sides_name, has_End_Points = [end_point_0, end_point_1], \n",
    "                                             size = [side_size])    \n",
    "    \n",
    "    \n",
    "    if force_construction_of_first_part == True:\n",
    "        two_parts = multiple_parts_constructor_for_complex_surface(knowledge_framework, new_complex_surface,\n",
    "                                                                 physical_space, [new_nodes_part1, new_nodes_part2])\n",
    "        \n",
    "        shared_side.name = str('shared_side_nb_{}_between_{}_and_{}'.format(share_sides_nb, two_parts[0].name, two_parts[1].name))\n",
    "        two_parts[0].has_Shared_Side.append(shared_side)\n",
    "        two_parts[1].has_Shared_Side.append(shared_side)\n",
    "        attribute_parts_nodes_to_complex_surface(new_complex_surface)\n",
    "        return new_complex_surface\n",
    "    if force_construction_of_first_part == False:\n",
    "\n",
    "        shared_side.name = str('shared_side_nb_{}_between_{}_and_{}'.format(share_sides_nb,\n",
    "                                                                             part2.name, first_face_to_extend.name))\n",
    "\n",
    "        part2 =    multiple_parts_constructor_for_complex_surface(knowledge_framework, new_complex_surface,\n",
    "                                                                 physical_space, [new_nodes_part2, ])[0]\n",
    "        first_face_to_extend.has_Shared_Side.append(shared_side)\n",
    "        part2.has_Shared_Side.append(shared_side)\n",
    "        attribute_parts_nodes_to_complex_surface(new_complex_surface)\n",
    "        return new_complex_surface\n",
    "\n",
    "def get_one_extreme_face(complex_surface):\n",
    "    \"\"\"Method to get one extreme face of a complex_surface that have a problem with geometry\n",
    "    \"\"\"\n",
    "    if len(complex_surface.has_Part) < 2 :\n",
    "        raise MalissiaBaseError('Wrong condition for one extreme face extraction : the complex_surface {} has {} part'.format(complex_surface.name,\n",
    "                                                                                                                             len(complex_surface.has_Part) ))\n",
    "    extreme_faces = [part_i for part_i in complex_surface.has_Part if (len(part_i.has_Neighbor) ==1 and part_i.has_Neighbor[0] in complex_surface.has_Part\n",
    "                                                                        and len(part_i.has_Unmatched_Neighbor) == 0)]\n",
    "    if len(extreme_faces) < 1 or len(extreme_faces) > 2 : \n",
    "        print('Warning, complex_surface {} has {} extreme faces, None is returned'.format(complex_surface.name, len(extreme_faces)))\n",
    "        return None\n",
    "    else:\n",
    "        return extreme_faces[0]\n",
    "                     \n",
    "def compute_larger_surface_by_extension_of_complex_surface(knowledge_framework,physical_space,\n",
    "                                                           candidate_complex_surface, \n",
    "                                                           proposed_node_of_extension, \n",
    "                                                           new_complex_surface = None, \n",
    "                                                           new_complex_surface_name = None,\n",
    "                                                           extend_only_laterally = True,\n",
    "                                                           force_projection = False, \n",
    "                                                            extend_one_extreme_face = False,\n",
    "                                                            forced_extreme_face = None,\n",
    "                                                            force_axis_direction_for_one_face_extention = None, \n",
    "                                                              use_face_as_new_first_part = False,\n",
    "                                                                forced_new_first_part = None, \n",
    "                                                                extreme_part_associated_with_forced_new_part = None\n",
    "                                                                ):\n",
    "    \n",
    "    \"\"\"Method to construct a new complex surface by the extension of an existing complex_surface with more than two sides.\n",
    "    - the candidate_complex_surface must be deformed.\n",
    "    - the method has the argument extend_only_laterally true by default as it is primarly to extend deformed surfaces laterally\n",
    "    - the proposed point of extension could be on one face of exntesion and respecting the condition of the extension of one planar surface\n",
    "       or anywhere on the space and it will be projected on the shared axis of the chosen face and its neighbor, for this second option,\n",
    "       the force_projection must be True and exten.\n",
    "    - the method could be used to extend only one surface by extension using a point anywhere i the space, for this extend_one_extreme_face must be True and extend_only_laterally False\n",
    "       and a forced face must be given in forced_extreme_face. If the point allow only extending in the opposite direction toward the shared axis of the extreme \n",
    "       face and its neighbor, an axis of forcing must be given in force_axis_direction_for_one_face_extention and that is orthogonal \n",
    "       on the shared axis.\n",
    "    - the method could be used to extend only laterally the candidate complex_surface and preserving a forced face that is extended elswhere.\n",
    "       For this option, use_face_as_new_first_part must be True, extend_only_laterally False, extreme_part_associated_with_forced_new_part and forced_new_first_part must be given.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # add coodiantes labels if none in args \n",
    "    coord_labels = physical_space.coordinate_labels\n",
    "    # check if the complex surface have more than two parts\n",
    "    if len(candidate_complex_surface.has_Part) < 2:\n",
    "        raise MalissiaBaseError('the {} has less than two parts'.format(candidate_complex_surface))\n",
    "    \n",
    "    # create the new complex surface if it is not passed\n",
    "    if new_complex_surface is None:\n",
    "        if new_complex_surface_name is not None:\n",
    "            new_complex_surface = knowledge_framework().Complex_Surface(name = str(new_complex_surface_name), non_planar = True)\n",
    "        else:\n",
    "            nb_of_cplx_sur = len(knowledge_framework().Complex_Surface.instances())\n",
    "            new_complex_surface_name = str('cplx_sur_nb_{}'.format(nb_of_cplx_sur + 1))\n",
    "            new_complex_surface = knowledge_framework().Complex_Surface(name = new_complex_surface_name, non_planar = True)\n",
    "\n",
    "\n",
    "    # get candidate faces total extension of the complex surface but only laterally\n",
    "    if extend_only_laterally == True:\n",
    "        if force_projection != True:\n",
    "            # get the two extreme faces of the complex surface, for that the surface must be non_planar\n",
    "            #two_extreme_complex_surface_parts = compute_complex_surface_extreme_surfaces(knowledge_framework,\n",
    "            #                                                                            candidate_complex_surface) \n",
    "            # check if the proposed point is at least on the plane of one of the extreme faces\n",
    "            two_extreme_complex_surface_parts = [candidate_complex_surface.has_Part[0], candidate_complex_surface.has_Part[-1]] # small fix until resolving problem with extreme faces and nb of neighbors\n",
    "            extreme_face = None\n",
    "            for part in two_extreme_complex_surface_parts:\n",
    "                vertices = np.array(physical_space.get_object_coordinates( object= part, kind= \"nodes\")) \n",
    "                dist_from_plane = np.linalg.norm(np.array(project_point_onto_plane(proposed_node_of_extension, \n",
    "                                                                    vertices))-np.array(proposed_node_of_extension))\n",
    "                if np.linalg.norm(dist_from_plane) == 0:\n",
    "                    extreme_face = part\n",
    "                    break\n",
    "            if extreme_face == None:\n",
    "                raise MalissiaBaseError('the proposed node of extension {} is not on the same plane of any extreme face of the {}'.format(proposed_node_of_extension, candidate_complex_surface))\n",
    "\n",
    "        else:\n",
    "            extreme_face = get_one_extreme_face(candidate_complex_surface)\n",
    "            vertices = np.array(physical_space.get_object_coordinates( object= extreme_face, kind= \"nodes\"))\n",
    "            second_part = extreme_face.has_Neighbor[0]\n",
    "            nodes2 = physical_space.get_object_coordinates( object= second_part, kind= \"nodes\")\n",
    "            common_nodes = common_two_nodes_two_planars(vertices, nodes2)\n",
    "            node_to_fix = find_farthest_node_from_nodes(common_nodes, proposed_node_of_extension)\n",
    "            direction_of_forced_axis = common_nodes[0] if node_to_fix == common_nodes[1] else common_nodes[1]\n",
    "            force_axis = np.array(direction_of_forced_axis) - np.array(node_to_fix)\n",
    "            proposed_node_of_extension = compute_point_projection_onto_line(proposed_node_of_extension,\n",
    "                                                                            node_to_fix, force_axis)\n",
    "        \n",
    "        # define initial first and second parts that will be extended\n",
    "        first_part = extreme_face\n",
    "        second_part = first_part.has_Neighbor[0]\n",
    "        first_part.extended  = False\n",
    "\n",
    "    # get candidate faces for extension of one extreme face\n",
    "    elif extend_only_laterally != True and extend_one_extreme_face == True:\n",
    "        if forced_extreme_face is None:\n",
    "            raise MalissiaBaseError('an extreme face of the complex_surface must be given')\n",
    "        else:\n",
    "            extreme_face = forced_extreme_face\n",
    "            vertices = np.array(physical_space.get_object_coordinates( object= extreme_face, kind= \"nodes\"))\n",
    "            second_part = extreme_face.has_Neighbor[0]\n",
    "            nodes2 = physical_space.get_object_coordinates( object= second_part, kind= \"nodes\")\n",
    "            common_nodes = common_two_nodes_two_planars(vertices, nodes2)\n",
    "            node_to_fix = find_farthest_node_from_nodes(common_nodes, proposed_node_of_extension)\n",
    "            new_nodes_part1_temp = compute_larger_surface_by_extension(*vertices,\n",
    "                                         node_to_fix, proposed_node_of_extension,\n",
    "                                        force_axis_direction = force_axis_direction_for_one_face_extention)# to be tested\n",
    "            n = 0 \n",
    "            n_part_cpmlx_sur = len(new_complex_surface.has_Part)+1\n",
    "            new_nodes_part1 = []\n",
    "            for n_i in new_nodes_part1_temp:\n",
    "                name= 'N{}_{}'.format(n, n_part_cpmlx_sur)\n",
    "                n +=1\n",
    "                n_part_cpmlx_sur +=1\n",
    "                n_i =  [float(i) for i in n_i]\n",
    "                new_nodes_part1.append(constructor_point(knowledge_framework, coord_labels, n_i, name))\n",
    "\n",
    "            new_first_part = multiple_parts_constructor_for_complex_surface(knowledge_framework, new_complex_surface,\n",
    "                                                                 physical_space, [new_nodes_part1])[0]\n",
    "            if is_point_on_line_projection(proposed_node_of_extension, common_nodes[0], common_nodes[1]):\n",
    "                for part in candidate_complex_surface.has_Part:\n",
    "                    if part != extreme_face:\n",
    "                        new_complex_surface.has_Part.append(part) #to be checked\n",
    "                return new_complex_surface\n",
    "            else:\n",
    "                axis = np.array(common_nodes[0]) - np.array(common_nodes[1])\n",
    "                proposed_node_of_extension = compute_point_projection_onto_line(proposed_node_of_extension, \n",
    "                                                                              common_nodes[0], axis)\n",
    "                # define initial first and second parts that will be extended\n",
    "                first_part = new_first_part\n",
    "                second_part = first_part.has_Neighbor[0]\n",
    "                first_part.extended  = True\n",
    "                second_part.extended  = False\n",
    "                \n",
    "    # get candidate faces to extend a surface laterally and use an existing already extended first part\n",
    "    elif extend_only_laterally != True and use_face_as_new_first_part == True :\n",
    "        if forced_new_first_part is None or extreme_part_associated_with_forced_new_part is None:\n",
    "            raise MalissiaBaseError('the forced extended face and the old part from the candidate \\\n",
    "                                     surface must be given in forced_new_first_part = \"?\" \\\n",
    "                                    and extreme_part_associated_with_forced_new_part = \"?\"')\n",
    "        else:\n",
    "            new_complex_surface.has_Part.append(forced_new_first_part)\n",
    "            first_part = extreme_part_associated_with_forced_new_part\n",
    "            second_part = first_part.has_Neighbor[0]\n",
    "            first_part.extended  = True\n",
    "            second_part.extended  = False\n",
    "\n",
    "    \n",
    "    if extend_only_laterally != True and extend_one_extreme_face != True and use_face_as_new_first_part != True :\n",
    "        raise MalissiaBaseError('one option  extend_only_laterally or extend_one_extreme_face or  use_face_as_new_first_part must be chosen, or built new method ')\n",
    "\n",
    "    # apply extension on each part based on the previous identified faces to extend, \n",
    "    ## for that all parts must be successive neighbors (this should be ensured as if complex surface is not checked neighbors are not set)\n",
    "    for part_i in range(len(candidate_complex_surface.has_Part)):\n",
    "        if first_part.extended == True and second_part.extended == True :\n",
    "            break\n",
    "        else:\n",
    "            nodes1 = physical_space.get_object_coordinates( object= first_part, kind= \"nodes\")\n",
    "            nodes2 = physical_space.get_object_coordinates( object= second_part, kind= \"nodes\")\n",
    "            common_nodes = common_two_nodes_two_planars(nodes1, nodes2)\n",
    "            projection_point_of_extension_on_part1 = project_point_onto_plane(proposed_node_of_extension, nodes1)# add condition for first projection\n",
    "            node_to_fix = find_farthest_node_from_nodes(common_nodes, projection_point_of_extension_on_part1)\n",
    "            direction_of_forced_axis = common_nodes[0] if node_to_fix == common_nodes[1] else common_nodes[1]\n",
    "            force_axis = np.array(direction_of_forced_axis) - np.array(node_to_fix)\n",
    "            sign = not first_part.extended  \n",
    "            new_complex_surface = compute_larger_surface_by_extension_of_two_non_planars_complex_surface(knowledge_framework,\n",
    "                                    physical_space,first_part ,second_part,\n",
    "                                    node_to_fix, projection_point_of_extension_on_part1, \n",
    "                                    new_complex_surface, force_axis = force_axis,\n",
    "                                    force_projection_of_point_on_part1_plane = True, \n",
    "                                    force_construction_of_first_part = sign)\n",
    "            first_part.extended = True\n",
    "            second_part.extended = True\n",
    "            \n",
    "            if len(second_part.has_Neighbor) > 2:\n",
    "                raise MalissiaBaseError('the part {} has more than two neighbors, which is not accepted in this case'.format(first_part))\n",
    "            elif len(second_part.has_Neighbor) == 2:\n",
    "                first_part = second_part\n",
    "                second_part = first_part.has_Neighbor[0] if first_part.has_Neighbor[0].extended != True else first_part.has_Neighbor[1]\n",
    "            else:\n",
    "                first_part = second_part\n",
    "                second_part = first_part.has_Neighbor[0]\n",
    "\n",
    "    # reset extension parameters to false of the candidate and new complex surfaces parts for next applications\n",
    "    for part_i in new_complex_surface.has_Part:# adding checking of all surfaces have been extended than reset\n",
    "        part_i.extended = False\n",
    "    for part_i in candidate_complex_surface.has_Part:\n",
    "        part_i.extended = False\n",
    "    attribute_parts_nodes_to_complex_surface(new_complex_surface)\n",
    "    return new_complex_surface\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "def attribute_parts_nodes_to_complex_surface(complex_surface):\n",
    "    nestded_nodes = []\n",
    "    for part in complex_surface.has_Part:\n",
    "        nestded_nodes.extend(part.has_Representation)\n",
    "    complex_surface.has_Representation = nestded_nodes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check complex surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# complex surface checking and evaluation\n",
    "\n",
    "def check_surface_parallelism(normal1, normal2,  tolerance = 3):\n",
    "    \"\"\"Method to check if two surfaces are parallel or not. It returns True or False.\n",
    "    Parameter:\n",
    "    - normal1:  the normal of the first surface\n",
    "    - normal2:  the normal of the second surface\n",
    "    - tolerance : degree of tolerance\"\"\"    \n",
    "    dot_prod = np.dot(normal1,normal2) / (np.linalg.norm(normal1) * np.linalg.norm(normal2))\n",
    "    if  not -1<= dot_prod <= 1:\n",
    "        dot_prod = np.round(dot_prod,0)\n",
    "    angle_deg = np.rad2deg(np.arccos(dot_prod))\n",
    "    if    0 +tolerance >= angle_deg >= 0-tolerance or  180+tolerance >= angle_deg >= 180-tolerance:\n",
    "        return True\n",
    "    else:\n",
    "        return False    \n",
    "\n",
    "def check_same_polarity_of_parallel_surfaces(normal1, normal2):\n",
    "    \"\"\"Method to check if two surfaces have the same polarity when their normals are parallel. It returns True or False.\n",
    "    Parameter:\n",
    "    - normal1:  the normal of the first surface\n",
    "    - normal2:  the normal of the second surface\n",
    "    \"\"\"  \n",
    "    if np.dot(normal1, normal2) > 0:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def check_if_two_surfaces_coplanar(point_on_surface1,  point_on_surface2, normal, tolerance = 3 ):\n",
    "    \"\"\"Method to check if two parallel surfaces are on the same plane. It returns True or False.\n",
    "    Parameter:\n",
    "    - point_on_surface1:  a point on the first surface ex: center or node\n",
    "    - point_on_surface2:  a point on the second surface ex: center or node\n",
    "    - normal:  a normal of one surface\n",
    "    - tolerance : degree of tolerance\"\"\"  \n",
    "    vec = np.array((point_on_surface1))- np.array((point_on_surface2))\n",
    "    # first check \n",
    "    if  all([x ==0  for x in vec]):\n",
    "        return False\n",
    "    if 90 +tolerance >= (np.rad2deg(np.arccos(np.dot(normal, vec)))) >= 90-tolerance:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_point_inside_surface(point, surface_nodes, normal_vector):\n",
    "    \"\"\"Method to check if a point is insied a surface. It returns True or False.\n",
    "    The point must be on the same plane as the surface\n",
    "    Parameter:\n",
    "    - point:  the point to be checked\n",
    "    - surface_nodes:  four nodes of the surface\n",
    "    - normal_vector : normal vector of the surface\"\"\"  \n",
    "    # Check if the point is on the same side of all edges\n",
    "    for i in range(4):\n",
    "        next_node = surface_nodes[(i + 1) % 4]\n",
    "        edge_vector = next_node - surface_nodes[i]\n",
    "        point_to_node = point - surface_nodes[i]\n",
    "        cross_product = np.cross(edge_vector, point_to_node)\n",
    "\n",
    "        if np.dot(cross_product, normal_vector) < 0:\n",
    "            return False  # The point is on the opposite side of this edge\n",
    "    return True  # The point is inside the polygon\n",
    "\n",
    "def check_two_coplanar_surface_inclusion(candidate_larger_surface_nodes,  candidate_smaller_surface_nodes,\n",
    "                                         candidate_larger_surface_normal ):\n",
    "    \"\"\"Method to check if two parallel surfaces are totally overlapping. It returns True or False.\n",
    "    Parameter:\n",
    "    - candidate_larger_surface_nodes:  nodes of the surface possibally including a smaller surface\n",
    "    - candidate_smaller_surface_nodes:  nodes of the surface possibally included in a larger surface\n",
    "    - normal:  the normal of candidate_larger_surface_nodes\n",
    "    \"\"\"  \n",
    "    if all([check_point_inside_surface(node_i, \n",
    "                                               candidate_larger_surface_nodes, \n",
    "                                               candidate_larger_surface_normal) for node_i in \n",
    "                                               candidate_smaller_surface_nodes]):\n",
    "        return True\n",
    "    else:\n",
    "        return False  \n",
    "\n",
    " \n",
    "\n",
    "def are_vectors_parallel(v, w):\n",
    "    \"\"\"Method to check if vectors are parllel. It returns True or False.\n",
    "    Parameter:\n",
    "    - v:  first vector \n",
    "    - w:  second vector\n",
    "    \"\"\"  \n",
    "\n",
    "    # Check if vectors are parallel\n",
    "    if np.allclose(np.cross(v, w), np.zeros_like(v)):\n",
    "            return True  # Vectors are parallel and in the same direction\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_complex_surface_neighbor_number_and_arrangement(complex_surface, knowledge_framework, physical_space ):\n",
    "    \"\"\"\n",
    "    Method to check if the number and arrangement of neighbors in a complex_surface by calculating actual numer of neighbors and the expected number.\n",
    "        It returns a boolean value. \n",
    "\n",
    "    Parameter:\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    \"\"\" \n",
    "    \n",
    "    parts = complex_surface.has_Part\n",
    "    neighbor_constructor_from_multiple_planars(complex_surface, knowledge_framework, physical_space)\n",
    "    # add if more then two has one neighbor ==> false\n",
    "    nb_parts = len(parts)\n",
    "    if nb_parts < 2 :\n",
    "        raise MalissiaBaseError('the complex_surface _must have at least two parts')\n",
    "    if nb_parts == 2:\n",
    "        if parts[0] in parts[1].has_Neighbor or parts[1] in parts[0].has_Neighbor:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    expected_total_of_neighbors = (nb_parts-1)*2\n",
    "    actual_total_of_neighbors = 0\n",
    "    # actual_total_of_neighbors = sum(neigh in complex_surface.has_Part for part in parts for neigh in part.has_Neighbor)\n",
    "    for part in parts:\n",
    "        for neigh in part.has_Neighbor:\n",
    "            if neigh in complex_surface.has_Part:\n",
    "                actual_total_of_neighbors +=1\n",
    "    #print(nb_parts, expected_total_of_neighbors, actual_total_of_neighbors)\n",
    "    return expected_total_of_neighbors == actual_total_of_neighbors\n",
    "    \n",
    "\n",
    "\n",
    "def shared_sides_are_parallel(complex_surface, knowledge_framework, physical_space, tolerance = 0.1):\n",
    "\n",
    "    \"\"\"\n",
    "    Method to check if shared sides are parallel. It returns True or False. \n",
    "\n",
    "    Parameter:\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    \"\"\" \n",
    "\n",
    "    list_of_shared_sides_in_complex_surf= get_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, physical_space)\n",
    "    unmatched_side = get_unmatched_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, physical_space)\n",
    "    sides = list_of_shared_sides_in_complex_surf + unmatched_side\n",
    "    reference_side = sides[0]\n",
    "    start_point, end_point = reference_side.has_End_Points[0], reference_side.has_End_Points[1]\n",
    "    coor_start = physical_space.get_object_coordinates( object= start_point, kind= \"node\")[0]\n",
    "    coor_end = physical_space.get_object_coordinates( object= end_point, kind= \"node\")[0]\n",
    "    reference_side_vec = np.array(coor_end) - np.array(coor_start)\n",
    "    reference_side_vec /= np.linalg.norm(reference_side_vec)\n",
    "    print('sides', sides)\n",
    "    for side in sides:\n",
    "        start_point, end_point = side.has_End_Points[0], side.has_End_Points[1]\n",
    "        coor_start = physical_space.get_object_coordinates( object= start_point, kind= \"node\")[0]\n",
    "        coor_end = physical_space.get_object_coordinates( object= end_point, kind= \"node\")[0]\n",
    "        side_i_vec = np.array(coor_end) - np.array(coor_start) \n",
    "        side_i_vec /= np.linalg.norm(side_i_vec)\n",
    "        if not np.linalg.norm(np.cross(reference_side_vec,side_i_vec )) <= 0 +tolerance  :\n",
    "            print(side, reference_side)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def shared_sides_are_equal(complex_surface, knowledge_framework, physical_space):\n",
    "    \"\"\"\n",
    "    Method to check if shared sides are equal. It returns True or False. \n",
    "\n",
    "    Parameter:\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    \"\"\" \n",
    "    if not shared_sides_are_parallel(complex_surface  = complex_surface, knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space):\n",
    "        raise MalissiaBaseError('to test if the shared sides are equal they must be parallel, which seems not to be the case')\n",
    "    list_of_shared_sides_in_complex_surf= get_shared_sides_in_one_complex_surface(complex_surface  = complex_surface,\n",
    "                                                                                   knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space)\n",
    "    unmatched_side = get_unmatched_shared_sides_in_one_complex_surface(complex_surface  = complex_surface,\n",
    "                                                                                   knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space)\n",
    "    sides = list_of_shared_sides_in_complex_surf + unmatched_side\n",
    "    initial_size = sides[0].size[0]\n",
    "    # check if all size are not equal \n",
    "    if not all([np.round(np.abs(initial_size - side.size[0]),1) == 0. for side in sides ]):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def shared_sides_have_uniform_allignement(complex_surface, knowledge_framework, physical_space):\n",
    "    \"\"\"\n",
    "    Method to check if shared sides have uniform allignement. It returns True or False. \n",
    "\n",
    "    Parameter:\n",
    "    - complex_surface : the complex_surface\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framework \n",
    "    \"\"\" \n",
    "    if not shared_sides_are_equal(complex_surface, knowledge_framework, physical_space):\n",
    "        raise MalissiaBaseError('to test if the shared have uniform allignement, they must be equal, which seems not to be the case')\n",
    "    list_of_shared_sides_in_complex_surf= get_shared_sides_in_one_complex_surface(complex_surface  = complex_surface,\n",
    "                                                                                   knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space)\n",
    "    unmatched_side = get_unmatched_shared_sides_in_one_complex_surface(complex_surface  = complex_surface,\n",
    "                                                                                   knowledge_framework  = knowledge_framework, \n",
    "                                                                                   physical_space =  physical_space)\n",
    "    sides = list_of_shared_sides_in_complex_surf + unmatched_side\n",
    "    reference_side = sides[0]\n",
    "    start_point, end_point = reference_side.has_End_Points[0], reference_side.has_End_Points[1]\n",
    "    ref_coor_start = physical_space.get_object_coordinates( object= start_point, kind= \"node\")[0]\n",
    "    ref_coor_end = physical_space.get_object_coordinates( object= end_point, kind= \"node\")[0]\n",
    "    reference_side_vec = np.array(ref_coor_end) - np.array(ref_coor_start)\n",
    "    for side in sides :\n",
    "        start_point, end_point = side.has_End_Points[0], reference_side.has_End_Points[1]\n",
    "        coor_start = physical_space.get_object_coordinates( object= start_point, kind= \"node\")[0]\n",
    "        coor_end = physical_space.get_object_coordinates( object= end_point, kind= \"node\")[0]\n",
    "        projected_end = compute_point_projection_onto_line(coor_end, ref_coor_end, reference_side_vec )\n",
    "        projected_start = compute_point_projection_onto_line(coor_start, ref_coor_end, reference_side_vec )\n",
    "        condition_1 =  is_point_on_line_projection(projected_end, ref_coor_end, ref_coor_start)\n",
    "        condition_2 =  is_point_on_line_projection(projected_start, ref_coor_end, ref_coor_start)\n",
    "        if not condition_1 or not condition_2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def are_vectors_perpendicular(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    return np.isclose(dot_product, 0)\n",
    "    \n",
    "def check_chirality_projection(part1, part2, knowledge_framework, physical_space, \n",
    "                              same_complex_surface_condition= False):\n",
    "    \"\"\"Method to check the chirality of two representation of stratigraphic parts. Parts should be neighbors.\n",
    "    The method computes the normals and orientations directly if surfaces are parllel, or the projection of the \n",
    "    surfaces on a common plane if the normals are not parallel. Based on the comparaison of the projected normals the poalirty is defined.\n",
    "    \n",
    "    Parameters:\n",
    "    - part1 : planar surface representation of the first part\n",
    "    - part2 : planar surface representation of the second part\n",
    "    - physical_space : the physical_space\n",
    "    - knowledge_framework : the knowledge_framewrok\n",
    "    - same_complex_surface_condition : boolean by default False to allow forcing the condition of belononging to the \n",
    "                                        same complex_surface for part1 and part2\n",
    "    \"\"\"\n",
    "\n",
    "    neighbor_constructor_from_two_planars(part1, part2, knowledge_framework = knowledge_framework, \n",
    "                                          physical_space =physical_space, \n",
    "                                          same_complex_surface_condition= same_complex_surface_condition)\n",
    "    n1 = physical_space.get_object_coordinates(part1, \"normal\")\n",
    "    n2 = physical_space.get_object_coordinates(part2, \"normal\")\n",
    "    n1 = np.array(n1)\n",
    "    n2 = np.array(n2)\n",
    "\n",
    "    if are_vectors_parallel(n1, n2):\n",
    "        return are_parallel_and_same_direction(n1, n2)\n",
    "\n",
    "    c1 = physical_space.get_object_coordinates(part1, \"center\")\n",
    "    c2 = physical_space.get_object_coordinates(part2, \"center\")\n",
    "    c1 = np.array(c1)\n",
    "    c2 = np.array(c2)\n",
    "    nodes1 = physical_space.get_object_coordinates(part1, \"nodes\")\n",
    "    nodes1 = np.array(nodes1)\n",
    "    nodes2 = physical_space.get_object_coordinates(part2, \"nodes\")\n",
    "    nodes2 = np.array(nodes2)\n",
    "\n",
    "    if are_vectors_perpendicular(n1, n2):\n",
    "        extreme_nodes = compute_extremeties_from_two_extreme_surfaces([part1, part2],physical_space)\n",
    "        extremeties_part1 = np.array([np.array(n_i) for n_i in extreme_nodes[0]])\n",
    "        extremeties_part2 =  np.array([np.array(n_i) for n_i in extreme_nodes[1]])\n",
    "\n",
    "        farthest_p2node_from_n0_p1 = find_farthest_node_from_nodes(extremeties_part2, extremeties_part1[0])\n",
    "        closest_p2node_from_n0_p1 = extremeties_part2[0] if set(farthest_p2node_from_n0_p1) == set(extremeties_part2[1]) \\\n",
    "                                    else extremeties_part2[1]\n",
    "        new_surface_of_projection_nodes = np.array([\n",
    "                                                farthest_p2node_from_n0_p1,\n",
    "                                                closest_p2node_from_n0_p1,\n",
    "                                                extremeties_part1[0],\n",
    "                                                extremeties_part1[1] \n",
    "                                                                        ])\n",
    "        projection_of_p1_nodes_on_new_surf = [project_point_onto_plane(node_i, \n",
    "                                            new_surface_of_projection_nodes) for node_i in nodes1]\n",
    "        project_p1_normal = compute_normal_vector(projection_of_p1_nodes_on_new_surf)\n",
    "        projection_of_p2_nodes_on_new_surf = [project_point_onto_plane(node_i, \n",
    "                                            new_surface_of_projection_nodes) for node_i in nodes2]\n",
    "        project_p2_normal = compute_normal_vector(projection_of_p2_nodes_on_new_surf)\n",
    "        return are_parallel_and_same_direction(project_p1_normal, project_p2_normal)\n",
    "    else:\n",
    "        projected_p1_on_p2 = [project_point_onto_plane(node_i, \n",
    "                                            nodes2) for node_i in nodes1]\n",
    "        project_p1_on_p2_normal = compute_normal_vector(projected_p1_on_p2)\n",
    "        if not are_vectors_parallel(n2, project_p1_on_p2_normal):\n",
    "            raise MalissiaBaseError('problem with projection of {} on {}'.format(part1.name, part2,name))\n",
    "        return  are_parallel_and_same_direction(n2, project_p1_on_p2_normal)\n",
    "\n",
    "def check_chirality_vectors(part1, part2, knowledge_framework, physical_space):\n",
    "    n1 = physical_space.get_object_coordinates(part1, \"normal\")\n",
    "    n2 = physical_space.get_object_coordinates(part2, \"normal\")\n",
    "    n1 = np.array(n1)\n",
    "    n2 = np.array(n2)\n",
    "\n",
    "    if are_vectors_parallel(n1, n2):\n",
    "        return are_parallel_and_same_direction(n1, n2)\n",
    "\n",
    "    c1 = physical_space.get_object_coordinates(part1, \"center\")\n",
    "    c2 = physical_space.get_object_coordinates(part2, \"center\")\n",
    "    c1 = np.array(c1)\n",
    "    c2 = np.array(c2)\n",
    "    nodes1 = physical_space.get_object_coordinates(part1, \"nodes\")\n",
    "    nodes1 = np.array(nodes1)\n",
    "    nodes2 = physical_space.get_object_coordinates(part2, \"nodes\")\n",
    "    nodes2 = np.array(nodes2)\n",
    "    axis = compute_fold_axis(n1, c1, n2, c2)\n",
    "    inter_point = compute_intersection_point(c1, n1, c2, n2,axis  )\n",
    "    v1 = c1-inter_point\n",
    "    v2 = c2-inter_point\n",
    "    cross_v1_n1 = np.cross(v1, n1)\n",
    "    cross_v2_n2 = np.cross(v2, n2)\n",
    "    return not are_parallel_and_same_direction(cross_v1_n1, cross_v2_n2)\n",
    " \n",
    "    \n",
    "\n",
    "#discontinuous_stratigraphy_anomaly_constructor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distinguish two kind of operations here:\n",
    "* representation\n",
    "* visualisation\n",
    "\n",
    "A representation is a formal description of how something appears in a given representation space, but it doesn't have to be visualised.<br>\n",
    "A visualisation takes care of the rendering of a representation with a given support (image, screen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation should also be made a bit more abstract.<br>\n",
    "1. There is a variety of object that can be rendered in a representation space (typically, different kinds of a dataset components)\n",
    "2. Several kinds of representation spaces could be envisionned (e.g., spatial 1D,2D,3D, or temporal, or just an abstract text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "from scipy import linalg\n",
    "\n",
    "class RepresentationSpace(object):\n",
    "    \"\"\"A general framework for Representating geological objects\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_framework = None):\n",
    "        \"\"\"Initialise base class attributes\"\"\"\n",
    "        self.knowledge_framework = knowledge_framework if knowledge_framework is not None else GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "        self._datasets= []\n",
    "        self._interpreted_objects = []\n",
    "        \n",
    "    def attach_dataset(self, dataset, use_extension= True, padding= None):\n",
    "        \"\"\"Attach a dataset to the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - dataset: a Dataset object to be attached\n",
    "        - use_extension: if True, uses the extension of the dataset, else keeps the current ones\n",
    "        - padding: if use_extension is True, the given paddign will be used to keep a space around the dataset\"\"\"\n",
    "        if dataset == None: return\n",
    "        if dataset in self._datasets: return\n",
    "        self._datasets += [dataset]\n",
    "        \n",
    "        if use_extension:\n",
    "            self.set_extension_from_data(padding= padding)\n",
    "        \n",
    "        dataset.setup_representation_space(physical_space= self)\n",
    "    \n",
    "    def get_datasets(self):\n",
    "        \"\"\"dataset getter\"\"\"\n",
    "        return self._datasets\n",
    "        \n",
    "    def attach_interpreted_object(self, interpreted_object, update_extension= True, padding= None):\n",
    "        \"\"\"Attach an interpreted object to the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - interpreted_object: an interpreted object  to be attached\n",
    "        - update_extension: if True, uses the extension of the interpreted object, else keeps the current ones\n",
    "        - padding: if use_extension is True, the given padding will be used to keep a space around the interpreted object\"\"\"\n",
    "        if interpreted_object == None: return\n",
    "        if interpreted_object in self._interpreted_objects: return\n",
    "        self._interpreted_objects += [interpreted_object]\n",
    "        \n",
    "        if update_extension:\n",
    "            self.update_extension_from_interpreted_object([interpreted_object], padding=padding)\n",
    "            \n",
    "    def get_interpreted_objects(self):\n",
    "        \"\"\"interpreted objects getter\"\"\"\n",
    "        return self._interpreted_objects\n",
    "        \n",
    "class TemporalRepresentationSpace(RepresentationSpace):\n",
    "    \"\"\"A `RepresentationSpace` representing temporal apsects of represented objects.\"\"\"\n",
    "    \n",
    "class PhysicalRepresentationSpace(RepresentationSpace):\n",
    "    \"\"\"A type of `RepresentationSpace` representing physical aspects of the represented objects.\"\"\"\n",
    "    \n",
    "    __default_coordinate_labels = [\"X\",\"Y\",\"Z\"]\n",
    "    \n",
    "    def __init__(self, dimension: int=None, coordinate_labels: str|list= None, dataset= None, **kargs):\n",
    "        \"\"\"Initialisation of the representation space.\n",
    "        \n",
    "        Parameters:\n",
    "        - dimension (int): specify the number of dimensions of the representation space, typically 1D, 2D, or 3D (i.e., 1, 2, or 3),\n",
    "        NB: larger dimension spaces are not supported. At least either the `dimension` parameter or `coordinate_label` parameter should be given.\n",
    "        - coordinate_labels(str|list(str)): gives the label(s) of the coordinates. If given, the number of dimensions is deduced from the size of the list\n",
    "        and `dimensions`is ignored, otherwise, the labels are taken from the `__default_coordinate_labels` based on the number of `dimension`s. \n",
    "        At least either the `dimension` parameter or `coordinate_label` parameter should be given.\n",
    "        - dataset: a Dataset object containing the data to be attached to this representation space.\n",
    "        Note that the RepresentationSpace can be created first and then updated automatically when creating the dataset attached to this space.\n",
    "        - **kargs:\n",
    "            - use_extension: if True, uses the extension of the dataset, else keeps the current ones\n",
    "            - padding: if use_extension is True, the given paddign will be used to keep a space around the dataset\n",
    "        \"\"\"       \n",
    "        super().__init__() \n",
    "        \n",
    "        assert not (coordinate_labels is None and dimension is None), \"At least one of the parameters should be specified\"\n",
    "        if coordinate_labels is None:\n",
    "            assert isinstance(dimension, int),\"dimension parameter must be an integer\"\n",
    "            assert dimension in [1,2,3], \"The specified number of dimensions ({:d}) is not supported, should be 1, 2 or 3.\".format(dimension)\n",
    "            self.dimension= dimension\n",
    "            self.coordinate_labels= PhysicalRepresentationSpace.__default_coordinate_labels[:self.dimension]\n",
    "        elif isinstance(coordinate_labels,str):\n",
    "            self.dimension= 1\n",
    "            self.coordinate_labels=  [coordinate_labels]\n",
    "        elif isinstance(coordinate_labels, list):\n",
    "            self.dimension= len(coordinate_labels)\n",
    "            self.coordinate_labels= coordinate_labels\n",
    "        else:\n",
    "            raise(\"Unsupported initialisation of representation space: dimension({}) and coordinate_label ({}).\\n At least one of the parameters shoudl be specified.\".format(dimension, coordinate_labels))\n",
    "\n",
    "        self.__default_padding= 0.2\n",
    "        self.set_extension(init=True)\n",
    "        self.attach_dataset(dataset,**kargs)\n",
    "        self.set_name()\n",
    "\n",
    "    def set_name(self):\n",
    "        \"\"\"\n",
    "        Set the 'name' attribute to the given value.\n",
    "        Parameters:\n",
    "        - self\n",
    "        \"\"\"\n",
    "        if not hasattr(self, '_name'):\n",
    "            gc.collect()\n",
    "            # Get a list of all objects tracked by the garbage collector\n",
    "            all_objects = gc.get_objects()\n",
    "            count_of_instances = len([obj for obj in all_objects if isinstance(obj, PhysicalRepresentationSpace)])\n",
    "            name = str('PhysicalRepresentationSpace_nb_{}'.format(count_of_instances+1))\n",
    "            self._name = name\n",
    "\n",
    "    def get_name(self):\n",
    "        \"\"\"\n",
    "        Get the current value of the 'name' attribute.\n",
    "\n",
    "        Returns:\n",
    "        - str: The current value of the 'name' attribute.\n",
    "        \"\"\"\n",
    "        return self._name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_extension(self):\n",
    "        \"\"\"returns the extensions of the space\"\"\"\n",
    "        return self.extension\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"returns the size of the space in each dimension\"\"\"\n",
    "        ext = np.array(self.extension)\n",
    "        return ext[:,1] - ext[:,0]\n",
    "    \n",
    "    def get_center(self):\n",
    "        \"\"\"returns the center of the space\"\"\"\n",
    "        return np.mean(self.extension, axis= 1)\n",
    "    \n",
    "    def generate_random_location(self, n= 1):\n",
    "        \"\"\"Generates n random location uniformly distributed within the space extensions\"\"\"\n",
    "        return np.random.default_rng().uniform(low= self.extensions[:0], high= self.extensions[:1], size= n)\n",
    "    \n",
    "    def generate_random_dip(self, n= 1):\n",
    "        \"\"\"Generates n random dip values that are compatible with this space\"\"\"\n",
    "        return np.random.default_rng().uniform(0, 90, n)\n",
    "\n",
    "    def generate_random_dip_dir(self, n= 1):\n",
    "        \"\"\"Generates n random dip_dir values that are compatible with this space\n",
    "        \n",
    "        Note: The values are hardcoded to be in XZ cross section to simplify the example\"\"\"\n",
    "        return np.random.default_rng().choice([90,270], n)#np.random.default_rng().uniform(0, 360, n)\n",
    "    \n",
    "    def generate_random_polarity(self, n= 1):\n",
    "        \"\"\"Generates n random polarity values (-1 or 1)\"\"\"\n",
    "        return np.random.default_rng().choice([-1,1], n)\n",
    "    \n",
    "    def generate_random_size(self, n=1, min_perc= 0.05, max_perc= 0.99):\n",
    "        \"\"\"Generates n random size values\"\"\"\n",
    "        sizes = self.get_size()\n",
    "        max_sizes = max(sizes)\n",
    "        return np.random.default_rng().uniform(min_perc*max_sizes, max_perc*max_sizes, n)\n",
    "        \n",
    "    def set_extension(self, extension:list= None, padding:float= None, init:bool= False):\n",
    "        \"\"\"Setter for the extension (min,max) of the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - extension: a list containing a pair of min and max value for each dimension of the space.\n",
    "        If None, the default will be set, i.e., [[0,1]] * dimension\n",
    "        - padding: a space that is left around the dataset, either a value compatible with the coordinates, or a list of values of same dimensions.\n",
    "        If None, by default the padding is 5% of the dataset range.\n",
    "        - init: (default False) if True the extension will be set to the given one, or default if extension is None;\n",
    "        if False, the extension will be updated with the given one\n",
    "        \"\"\"\n",
    "        if extension is None:\n",
    "            if init or (self.extension is None):\n",
    "                self.extension = np.array([[0,1]] * self.dimension)\n",
    "                self.actual_extension = None\n",
    "                self.initialised_extension = False\n",
    "            return\n",
    "        extension= np.array(extension)\n",
    "        assert extension.shape[0] == self.dimension, \"The specified extension ({}) do not match the space dimensions ({})\".format(extension, self.dimension)\n",
    "        assert extension.shape[1] == 2, \"The specified extension should provide both lower andupper bounds for each dimension, given: {}\".format(extension)\n",
    "        \n",
    "        # if reset is False then update extension\n",
    "        if (init is False) and (self.initialised_extension is True):    \n",
    "            for dim_i in range(self.dimension):\n",
    "                extension[dim_i,0] = min(extension[dim_i,0], self.actual_extension[dim_i,0])\n",
    "                extension[dim_i,1] = max(extension[dim_i,1], self.actual_extension[dim_i,1])\n",
    "        \n",
    "        # padding\n",
    "        if padding is None:\n",
    "            padding= self.__default_padding\n",
    "        else:\n",
    "            try: # check if padding as a dimension\n",
    "                len(padding)\n",
    "            # if not, then use it a a scaling \n",
    "            except TypeError: #just checking it is a number\n",
    "                assert type(padding) == int or type(padding) == float, \"padding should be given as a number (int or float), here: \"+type(padding)\n",
    "                # keep the padding as is in this case\n",
    "            else:# else check its dimensions are ok\n",
    "                assert len(padding) == self.dimension, \"the dimensions of the specified padding (len({})->) should match the space dimension ({})\".format(padding, len(padding), self.dimension)\n",
    "                padding= np.array(padding)\n",
    "                \n",
    "        self.center= np.mean(extension, axis= 1)\n",
    "        diff= extension.T - self.center\n",
    "        diff= diff.T\n",
    "        \n",
    "        # padding is multiplied by the width along each axis and added to the extension\n",
    "        # check if each dimension is shrinked (ie. min == max -> diff == 0), takes the average extension of the other dimensions,\n",
    "        # unless all are shrinked, in which case [[-1,1]] * self.dimension is set\n",
    "        if np.all(diff == 0):\n",
    "            diff = np.array([[-1,1]] * self.dimension)\n",
    "        else:\n",
    "            zero_diff = np.where(~diff.any(axis=1))\n",
    "            non_zero_diff = np.where(diff.any(axis=1))\n",
    "            mean_diff = np.mean(diff[non_zero_diff], axis=0)\n",
    "            diff[zero_diff] = mean_diff\n",
    "        \n",
    "        self.actual_extension = extension \n",
    "        self.extension= np.repeat([self.center],2,axis=0).T + (1+2*padding)*diff\n",
    "        self.initialised_extension = True\n",
    "        \n",
    "    def set_extension_from_data(self, padding= None, init= False):\n",
    "        \"\"\"Sets the extension of the space from the attached dataset\n",
    "        \n",
    "        If no dataset is attached yet, then default extension are used instead (min:0,max:1).\n",
    "        \n",
    "        Parameters:\n",
    "        - padding: a space that is left around the dataset, either a value compatible with the coordinates, or a list of values of same dimensions.\n",
    "        If None, by default the padding is 5% of the dataset range.\n",
    "        \"\"\"\n",
    "        non_empty_dataset = [data_i for data_i in self._datasets if data_i.extension is not None]\n",
    "        if len(non_empty_dataset) == 0:\n",
    "            self.set_extension(init= init)\n",
    "            return\n",
    "                \n",
    "        extension = np.array(non_empty_dataset[0].extension)\n",
    "        for data_i in non_empty_dataset[1:]:\n",
    "            for dim_i in range(self.dimension):\n",
    "                extension[dim_i,0] = min(extension[dim_i,0], data_i.extension[dim_i])\n",
    "                extension[dim_i,1] = max(extension[dim_i,1], data_i.extension[dim_i])\n",
    "        \"\"\":todo: use projected coordinates instead of source coordinates, might fail if 3D data projected on a map\"\"\"\n",
    "        \n",
    "        self.set_extension(extension, padding=padding, init= init)\n",
    "        \n",
    "    def update_extension_from_interpreted_object(self, objects= None, padding= None, init= False):\n",
    "        \"\"\"Sets the extension of the space from the attached objects\n",
    "        \n",
    "        Parameters:\n",
    "        - padding: a space that is left around the dataset, either a value compatible with the coordinates, or a list of values of same dimensions.\n",
    "        If None, by default the padding is 5% of the dataset range.\n",
    "        \"\"\"\n",
    "        objects = self._interpreted_objects if objects is None else objects\n",
    "        objects = [objects] if not isinstance(objects, list) else objects\n",
    "            \n",
    "        if len(objects) == 0:\n",
    "            self.set_extension(init= init)\n",
    "            return\n",
    "                \n",
    "        first_obj = objects[0]\n",
    "        coord = self.get_object_coordinates(first_obj, kind= \"nodes\")\n",
    "        extension = np.array([np.min(coord, axis=0),np.max(coord, axis=0)]).T\n",
    "        for obj_i in objects[1:]:\n",
    "            for dim_i in range(self.dimension):\n",
    "                obj_i_coord = self.get_object_coordinates(obj_i, kind= \"nodes\")\n",
    "                extension_i = np.array([np.min(obj_i_coord, axis=0),np.max(obj_i_coord, axis=0)]).T\n",
    "                extension[dim_i,0] = min(extension[dim_i,0], extension_i[dim_i,0])\n",
    "                extension[dim_i,1] = max(extension[dim_i,1], extension_i[dim_i,1])\n",
    "        \"\"\":todo: use projected coordinates instead of source coordinates, might fail if 3D data projected on a map\"\"\"\n",
    "        \n",
    "        self.set_extension(extension, padding=padding, init= init)\n",
    "        \n",
    "    def filter_qualities(self,**qualities):\n",
    "        \"\"\"removes the named arguments corresponding to this space coordinates from qualities.\n",
    "        \n",
    "        Return:\n",
    "        - a copy with the passed parameters except for the ones correpsonding to the space coordinates\"\"\"\n",
    "        qualities =  {key:val for key, val in qualities.items() if key not in self.coordinate_labels}\n",
    "        return qualities\n",
    "    \n",
    "    def prepare_coordinate_qualities(self, **qualities):\n",
    "        \"\"\"transforms the coordinates passed in qualities into an appropriate format\n",
    "        \n",
    "        Return:\n",
    "        - a dict with quality keys and values for setting coordinates\"\"\"\n",
    "        coordinates = {key:val for key, val in qualities.items() if key in self.coordinate_labels}\n",
    "        coord_qualities = {}\n",
    "        for i,key in enumerate(self.coordinate_labels):\n",
    "            if key in coordinates:\n",
    "                val = coordinates[key]\n",
    "                coord_qualities[\"coord{}\".format(i+1)] = val if isinstance(val,list) else [val]\n",
    "                coord_qualities[\"coord{}_label\".format(i+1)] = [key]\n",
    "        return coord_qualities\n",
    "    \n",
    "    def label_map(self, object):\n",
    "        \"\"\"creates of mapping of coordinate labels\"\"\"\n",
    "        coord_label_params = [\"coord{}_label\".format(i) for i in range(1,4) if hasattr(object, \"coord{}_label\".format(i))]\n",
    "        non_empty_coord_label_param = [param for param in coord_label_params if len(getattr(object,param)) > 0]\n",
    "        label_map = {getattr(object,param)[0]:param for param in non_empty_coord_label_param if getattr(object,param)[0] in self.coordinate_labels}\n",
    "        return label_map\n",
    "        \n",
    "    def set_object_coordinates(self, object, **kargs):\n",
    "        \"\"\"Sets the coordinates corresponding to this space into the given object\n",
    "        \n",
    "        Parameters:\n",
    "        - object: the object whose coordinates needs to be set\n",
    "        - kargs: keyword argugments corresponding to the name and values of the coordinates.\n",
    "        They must match this space coordinate names, extra names will be ignored\"\"\"\n",
    "        \n",
    "        # find the object representation and check it is a point\n",
    "        #otherwise another setter must be used\n",
    "        if (object.has_Representation is None) or (len(object.has_Representation) < 1):\n",
    "            raise MalissiaBaseError(\"can't set the object coordinates because it doesn't have a geometrical representation\")\n",
    "        rep = object.has_Representation[0]\n",
    "        if not self.knowledge_framework.isinstance(rep, self.knowledge_framework().Point):\n",
    "            raise MalissiaBaseError(\"can't set the object coordinates because its geometrical representation is not a Point\")            \n",
    "        \n",
    "        # check if the object has coord{i}_label set, else set it\n",
    "        if mogi.has_qualities(rep, [\"coord{}_label\".format(i) for i in range(1,len(self.coordinate_labels)+1)]):\n",
    "            label_map = self.label_map(rep)\n",
    "            coordinate_values = {label_map[key].split(\"_\")[0]:(kargs[key] if isinstance([kargs[key]],list) else [kargs[key]])\n",
    "                                  for key in self.coordinate_labels if key in kargs}\n",
    "        else:\n",
    "            coordinate_values = self.prepare_coordinate_qualities(**kargs)\n",
    "        for param_name, val in coordinate_values.items():\n",
    "            setattr(rep, param_name, val)\n",
    "          \n",
    "    def _get_coord(self, object):\n",
    "        label_map = self.label_map(object)\n",
    "        coord = np.full_like(self.coordinate_labels, np.nan, dtype= float)\n",
    "        for i, key in enumerate(self.coordinate_labels):\n",
    "            if key in label_map:\n",
    "                coord_param = label_map[key].split(\"_\")[0]\n",
    "                coord[i] = getattr(object,coord_param)[0]\n",
    "        return coord \n",
    "    \n",
    "    def _get_coord_from_list(self, object_list):\n",
    "        coord = np.full((len(object_list),len(self.coordinate_labels)), np.nan, dtype= float)\n",
    "        for i, rep_i in enumerate(object_list):\n",
    "            label_map = self.label_map(rep_i)\n",
    "            for j, key in enumerate(self.coordinate_labels):\n",
    "                if key in label_map:\n",
    "                    coord_param = label_map[key].split(\"_\")[0]\n",
    "                    coord[i,j] = getattr(rep_i,coord_param)[0]\n",
    "        return coord\n",
    "    \n",
    "    def get_object_coordinates(self, object, kind= \"center\"):\n",
    "        \"\"\"Gets the coordinates corresponding to this space from the given object\"\"\"\n",
    "        \n",
    "        if object is None:\n",
    "            raise MalissiaBaseError(\"can't get the object coordinates because it is None\")\n",
    "            \n",
    "        # if the object is a Representation_Concept use it directly else check if it has a representation\n",
    "        if self.knowledge_framework.isinstance(object, self.knowledge_framework().Representation_Concept):\n",
    "            rep = object\n",
    "            n_rep = 1\n",
    "        else:\n",
    "            rep = object.has_Representation\n",
    "            n_rep = 0 if rep is None else len(rep)\n",
    "            if (rep is None) or (n_rep == 0):\n",
    "                raise MalissiaBaseError(\"can't get the object coordinates because it doesn't have a geometrical representation\")\n",
    "            if n_rep == 1:\n",
    "                rep = rep[0]\n",
    "                \n",
    "        if kind == \"center\":\n",
    "            if n_rep == 1:\n",
    "                if (rep.has_Center is None) or (len(rep.has_Center) == 0):\n",
    "                    raise MalissiaBaseError(\"can't get the object center because its center is not defined\")\n",
    "                center = rep.has_Center[0]\n",
    "            else:\n",
    "                centers = [rep_i.has_Center[0] for rep_i in rep if (rep_i.has_Center is not None) and (len(rep_i.has_Center)>0)]\n",
    "                if len(centers) >1:\n",
    "                    print(\"Warning: found more than one center, using center[0]\")\n",
    "                if (len(centers) == 0):\n",
    "                    raise MalissiaBaseError(\"can't get the object center because its center is not defined\")\n",
    "                center = centers[0]\n",
    "            if not self.knowledge_framework.isinstance(center, self.knowledge_framework().Point):\n",
    "                raise MalissiaBaseError(\"can't get the object center because its center is not a Point\") \n",
    "        \n",
    "            return self._get_coord(center)\n",
    "        \n",
    "        if kind == \"vect\":\n",
    "            if n_rep == 1:\n",
    "                vector = rep\n",
    "                if not self.knowledge_framework.isinstance(vector, self.knowledge_framework().Vector):\n",
    "                    raise MalissiaBaseError(\"can't get the object vector coordinate because is not a Vector\") \n",
    "            else:\n",
    "                vectors = [rep_i.has_Center[0] for rep_i in rep if (rep_i.has_Center is not None) and (len(rep_i.has_Center)>0)]\n",
    "                if len(vectors) >1:\n",
    "                    print(\"Warning: found more than one vector-> using vector[0]\")\n",
    "                vector = vectors[0]\n",
    "        \n",
    "            return self._get_coord(vector)\n",
    "        \n",
    "        elif (kind == \"node\") or (kind == \"nodes\"):\n",
    "            if n_rep == 1:\n",
    "                # check it has at least one node or center\n",
    "                if not (\n",
    "                        ((rep.has_Node1 is not None) and (len(rep.has_Node1) > 0)) \n",
    "                        or\n",
    "                        ((rep.has_Center is not None) and (len(rep.has_Center) > 0)) \n",
    "                    ):\n",
    "                    raise MalissiaBaseError(\"can't get the object node coordinates because it has no Nodes nor Center\")\n",
    "                # if so the nodes should be listed in the rep\n",
    "                nodes = rep.has_Representation\n",
    "            else:\n",
    "                nodes = [rep_i.has_Representation for rep_i in rep if (rep_i.has_Node1 is not None) and (len(rep_i.has_Node1)>0) and (rep_i.has_Representation is not None)]\n",
    "\n",
    "            if (nodes is None) or (len(nodes) == 0):\n",
    "                raise MalissiaBaseError(\"can't get the object nodes because its representation is not defined\")\n",
    "            nodes = [node_i for node_i in nodes if self.knowledge_framework.isinstance(node_i, self.knowledge_framework().Point)]\n",
    "            if len(nodes) == 0:\n",
    "                raise MalissiaBaseError(\"can't get the object coordinates because its geometrical representation are not a Point\") \n",
    "            \n",
    "            return self._get_coord_from_list(nodes)\n",
    "        \n",
    "        if kind == \"normal\":\n",
    "            \n",
    "            if n_rep == 1:\n",
    "                if (rep.has_Normal is None) or (len(rep.has_Normal) == 0):\n",
    "                    raise MalissiaBaseError(\"can't get the object normal because its normal is not defined\")\n",
    "                normal = rep.has_Normal[0]\n",
    "            else:\n",
    "                normals = [rep_i.has_Normal[0] for rep_i in rep if (rep_i.has_Normal is not None) and (len(rep_i.has_Normal)>0)]\n",
    "                if len(normals) >1:\n",
    "                    print(\"Warning: found more than one normal, using normal[0]\")\n",
    "                if (len(normals) == 0):\n",
    "                    raise MalissiaBaseError(\"can't get the object normal because its normal is not defined\")\n",
    "                normal = normals[0]\n",
    "            if not self.knowledge_framework.isinstance(normal, self.knowledge_framework().Vector):\n",
    "                raise MalissiaBaseError(\"can't get the object normal because its normal is not a Vector\") \n",
    "        \n",
    "            return self._get_coord(normal)\n",
    "        \n",
    "        if kind == \"size\":\n",
    "            \n",
    "            if n_rep == 1:\n",
    "                if (rep.size is None) or (len(rep.size) == 0):\n",
    "                    raise MalissiaBaseError(\"can't get the object size because its size is not defined\")\n",
    "                size = rep.size[0]\n",
    "            else:\n",
    "                sizes = [rep_i.size[0] for rep_i in rep if (rep_i.size is not None) and (len(rep_i.size)>0)]\n",
    "                if len(sizes) >1:\n",
    "                    print(\"Warning: found more than one size, using sizes[0]\")\n",
    "                if (len(sizes) == 0):\n",
    "                    raise MalissiaBaseError(\"can't get the object size because its size is not defined\")\n",
    "                size = sizes[0]\n",
    "            return float(size)\n",
    "        \n",
    "        else:\n",
    "            raise MalissiaBaseError(\"Type of coordinates not available:\"+kind)\n",
    "    \n",
    "    def coordinates_to_dict(self, coords):\n",
    "        \"\"\" transforms a coordinate array into an appropriate dict with labels\"\"\"\n",
    "        return {label:value for label,value in zip(self.coordinate_labels, coords)}\n",
    "    \n",
    "    def get_normal_vector(self, feature):\n",
    "        dip = feature.dip\n",
    "        dip_dir = feature.dip_dir\n",
    "        return self.compute_normal_from_dip_dir(dip, dip_dir)\n",
    "        \n",
    "    def compute_normal_from_dip_dir(self, dip, dip_dir, polarity= 1):\n",
    "        dip_rad = np.deg2rad(dip)\n",
    "        dip_dir_rad = np.deg2rad(dip_dir)\n",
    "        z = np.cos(dip_rad)\n",
    "        h = np.sin(dip_rad)\n",
    "        y = h * np.cos(dip_dir_rad)\n",
    "        x = h * np.sin(dip_dir_rad)\n",
    "        return polarity * np.array([x,y,z])\n",
    "    \n",
    "    def compute_dip_dir_from_normal(self, normal):\n",
    "        polarity = 1 if normal[2] == 0 else np.sign(normal[2]) \n",
    "        x,y,z = polarity * np.array(normal) / np.linalg.norm(normal)\n",
    "        if np.abs(z) == 1:\n",
    "            return 0,0,z\n",
    "        dip = np.rad2deg(np.arccos(z))\n",
    "        dip_dir = np.rad2deg(np.arctan2(x,y)) % 360\n",
    "        return dip, dip_dir, polarity\n",
    "    \n",
    "    def compute_line_attitude_from_two_points(self, p0, p1, center= False, reduce_dimension= None):\n",
    "        \"\"\"Computes the attitude of a line going through two points\n",
    "        \n",
    "        Parameters:\n",
    "        - p0: coordinates of the first point\n",
    "        - p1: coordinates of the second point\n",
    "        - reduce_dimension: optional, default None, if set, the given coordinate is set to 0 to simulate projection on a cross section\n",
    "        \n",
    "        Returns:\n",
    "        - attitude: a dictionnary holding\n",
    "          \"dip_dir\" : the dip direction of the line. +or- 1 if dimension is  <3, value otherwise\n",
    "          \"dip\" : the dip of the line (ie., downward). None if dimension is <2, value otherwise\n",
    "        \"\"\"\n",
    "        v = np.array(p1) - np.array(p0)\n",
    "        center = np.mean([p1,p0],axis=0) if center else np.zeros(v.shape[-1])\n",
    "        if reduce_dimension is not None:\n",
    "            v[reduce_dimension] = 0\n",
    "            center[reduce_dimension] = np.mean([p0[reduce_dimension],p1[reduce_dimension]])\n",
    "        if len(v) == 1:\n",
    "            dip_dir = np.sign(v)[0]\n",
    "            dip = None\n",
    "        else:\n",
    "            # make v downward\n",
    "            if v[-1] > 0:\n",
    "                v *= -1 \n",
    "            if len(v) == 2:\n",
    "                dip_dir = np.sign(v[0])\n",
    "                v_abs = np.abs(v)\n",
    "                dip = np.rad2deg(np.arctan2(v_abs[1], v_abs[0]))\n",
    "            else:\n",
    "                dip_dir = np.rad2deg(np.arctan2(v[0],v[1]))\n",
    "                dip_dir = 360 - np.abs(dip_dir) if dip_dir < 0 else dip_dir\n",
    "                dip = -np.rad2deg(np.arctan2(v[2], np.linalg.norm(v[:2]) ))\n",
    "        size = np.linalg.norm(v)\n",
    "                \n",
    "        return {\"dip_dir\":dip_dir, \"dip\":dip, \"center\":center, \"size\":size}\n",
    "            \n",
    "    def compute_principal_directions(self, p, center= False, ):\n",
    "        \"\"\"Computes the principal directions in a set of vectors\n",
    "        \n",
    "        This can be used to compute the principal vectors in a set of vectors\n",
    "        or medium plan in a group of points (needs to set center to True).\n",
    "        \n",
    "        Parameters:\n",
    "        - p: points/vectors in the shape (# points, # dimensions)\n",
    "        - center: tells if points should be centered first, important for computing medium plane, default is False\n",
    "        \n",
    "        Returns: a dictionnary with:\n",
    "        - \"values\": the singular values of the principal axes\n",
    "        - \"vectors\": the vectors of the principal axes\n",
    "        - \"center\": the center of points if center is set to True, zeros otherwise\n",
    "        \"\"\"\n",
    "        p = np.array(p)\n",
    "        center = np.mean(p,axis=0) if center else np.zeros(p.shape[-1])\n",
    "        p = p - center\n",
    "        mat = np.dot(p.T, p)\n",
    "        _,s,Vh = linalg.svd(mat)\n",
    "        result = {\n",
    "            \"values\": s,\n",
    "            \"vectors\": Vh,\n",
    "            \"center\": center\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "    def compute_average_vector(self, vectors):\n",
    "        \"\"\"Compute the average vector\"\"\"\n",
    "        principal_directions = self.compute_principal_directions(vectors)\n",
    "        principal_vectors = principal_directions[\"vectors\"]\n",
    "        return principal_vectors[0]\n",
    "    \n",
    "    def compute_attitude_from_points(self, p):\n",
    "        \"\"\"Computes the attitude dip and dip_dir for a medium plane going through points\n",
    "        \n",
    "        Parameters:\n",
    "        - p: the points coordinates given in shape ()\n",
    "        \"\"\"\n",
    "        if len(p) < self.dimension:\n",
    "            raise MalissiaBaseError(\"Underdetermined attitude computation.\")\n",
    "            \n",
    "        if self.dimension == 2:\n",
    "            return self.compute_line_attitude_from_two_points(p, center= True)\n",
    "        elif self.dimension == 3:\n",
    "            return self.compute_plane_from_points(p)\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"unsupported dimension for attitude computation.\")\n",
    "        \n",
    "    def compute_plane_from_points(self, p):\n",
    "        \"\"\"Computes the attitude dip and dip_dir for a medium plane going through points\n",
    "        \n",
    "        Parameters:\n",
    "        - p: the points coordinates given in shape ()\n",
    "        \n",
    "        Returns:\n",
    "        - \"dip_dir\": azimuth from North towards the East, None if dip is zero\n",
    "        \"\"\"\n",
    "        principal_direction = self.compute_principal_directions(p, center= True)\n",
    "        \n",
    "        result = {}\n",
    "        result[\"center\"] = principal_direction[\"center\"]\n",
    "        result[\"major_axis\"] = principal_direction[\"vectors\"][0]\n",
    "        result[\"minor_axis\"] = principal_direction[\"vectors\"][1]\n",
    "        result[\"size\"] = np.sqrt(principal_direction[\"values\"][0])\n",
    "        \n",
    "        result[\"normal\"] = principal_direction[\"vectors\"][-1]\n",
    "        n = -result[\"normal\"] if result[\"normal\"][2]<0 else result[\"normal\"]\n",
    "        h = np.linalg.norm(n[:2])\n",
    "        result[\"dip\"] = np.rad2deg(np.arctan2(h, n[2]))\n",
    "        if h == 0:\n",
    "            result[\"dip_dir\"] = None\n",
    "        else:\n",
    "            dip_dir = np.rad2deg(np.arctan2(n[0], n[1])) % 360\n",
    "            result[\"dip_dir\"] = 360 - np.abs(dip_dir) if dip_dir < 0 else dip_dir\n",
    "        result[\"azimuth\"] = result[\"dip_dir\"] - 90 if result[\"dip_dir\"] > 90 else result[\"dip_dir\"] + 270\n",
    "        return result\n",
    "    \n",
    "    def compute_projection_on_plane(self, plane, object):\n",
    "        center_part = self.get_object_coordinates(plane, kind= \"center\")\n",
    "        center_explained = self.get_object_coordinates(object, kind= \"center\") \n",
    "        vec = center_part - center_explained \n",
    "        \n",
    "        normal = self.get_object_coordinates(plane, kind= \"normal\")\n",
    "        return center_explained + np.dot(vec,normal) * normal\n",
    "    \n",
    "    def evaluate_coverage(self, objects= None):\n",
    "        \"\"\"Compute the proportion of the space that is covered by the given objects.\"\"\"\n",
    "        # Note this is done along the first coordinate only for now as we are working on cross sections\n",
    "        \n",
    "        # this is looking for the segments of space that are not covered yet\n",
    "        # initialise by considering the whole space as uncovered\n",
    "        ref_segment = np.array(self.actual_extension[0])\n",
    "        uncovered_segments = np.array(self._get_uncovered_space(reference_segment= ref_segment, objects=objects))\n",
    "        segment_length = uncovered_segments[:,1] - uncovered_segments[:,0]\n",
    "        return 1.0 - np.sum(segment_length) / (ref_segment[1] - ref_segment[0])\n",
    "        \n",
    "    def _get_uncovered_space(self, reference_segment, objects= None):\n",
    "        uncovered_segment = [np.copy(reference_segment)]\n",
    "        for obj_i in self._interpreted_objects if objects is None else objects:\n",
    "            self._remove_object_part(uncovered_segment, obj_i)\n",
    "        return uncovered_segment\n",
    "    \n",
    "    def _remove_object_part(self, segments, object):\n",
    "        coord_x = self.get_object_coordinates(object, kind=\"nodes\")[:,0]\n",
    "        obj_segment = np.array([min(coord_x), max(coord_x)])\n",
    "        return self._remove_segment_part(segments, obj_segment)\n",
    "    \n",
    "    def _remove_segment_part(self, segments, remove_part):\n",
    "        p0, p1 = remove_part\n",
    "        for seg in segments:\n",
    "            s0, s1 = seg\n",
    "            p0_inside= False\n",
    "            if (p0 > s0) and (p0 < s1):# first node is inside -> move seg to the remaining left part\n",
    "                seg[1] = p0 \n",
    "                p0_inside= True\n",
    "            if (p1 < s1) and (p1 > s0):# second node is inside -> \n",
    "                # create a second segment for the remaining right part if first was inside else move seg to the right\n",
    "                if p0_inside:\n",
    "                    segments += [[p1,s1]]\n",
    "                else:\n",
    "                    seg[0] = p1 \n",
    "        return segments\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Description of the physical space parameters and data\n",
    "        \"\"\"\n",
    "        desc= [\"Representation space of type: {:s}\".format(type(self).__name__)]\n",
    "        desc+= [\"- Number of dimension(s): {}\".format(self.dimension)]\n",
    "        desc+= [\"- Coordinate label(s): {}\".format(self.coordinate_labels)]\n",
    "        desc+= [\"- Space extension:\"]\n",
    "        for dim_i, lim_i in zip(self.coordinate_labels,self.extension):\n",
    "            desc+= [\" |- Coord {}: {}\".format(dim_i, lim_i)]  \n",
    "        return \"\\n\".join(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class GeologicalDataset(object):\n",
    "    \"\"\"A GeologicalDataset gathers information about geological data to be interpreted.\n",
    "    \n",
    "    This class is a hybrid ontology&python class. It is providing pythonic algorithm and high level interface,\n",
    "    while the data is actually stored in an ontology.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, physical_space= None, time_space= None, representation_spaces= None, knowledge_framework= None):\n",
    "        \"\"\"Initialises a `GeologicalDataset`\n",
    "        \n",
    "        Parameters:\n",
    "        - physical_space: a `PhysicalRepresentationSpace`,  which defines the spatial coordinates of this dataset\n",
    "        - time_space: a `TemporalRepresentationSpace`,  which defines the time coordinates of this dataset\n",
    "        - representation_spaces: list of `Representationspace`s to which the dataset must be attached\n",
    "        Note: datasets can be created without representation space and attached later on by using `RepresentationSpace.attach_dataset`\n",
    "        or `GeologicalDataset.setup_representation_space`.\n",
    "        Alternativelly, a single `PhysicalRepresentationspace` and or `TemporalRepresentationspace` can be given here if `physical_space` and `time_space` are None.\n",
    "        - default_representation_space: the main RepresentationSpace to which this dataset is attached.\n",
    "        If None and representation_spaces are provided, then the first one will be taken.\n",
    "        If the default one is not initially in the full list, then it is added to it.\n",
    "        - ontology: the name of the ontology to be used for storing the data.\n",
    "        If None, the default will be taken from `the GeologicalKnowledgeManager`.\n",
    "        \n",
    "        Internals: this method initialises several internal attributes:\n",
    "        - extension: represents the extension of the dataset in the attached representation space\n",
    "        (i.e., the default on if this dataset is represented in several representation spaces\n",
    "        - representation_spaces: the dataset can be attached to and represented into several representation spaces,\n",
    "        `physical_space` and `time_space` are included into this list.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.knowledge_framework= knowledge_framework if knowledge_framework is not None else GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "        \n",
    "        self.extension = None\n",
    "        \n",
    "        # setup representation spaces\n",
    "        self.representation_spaces= set()\n",
    "        if physical_space is None:\n",
    "            # try to initialise the physical space with first existing data\n",
    "            observations = self.get_observations()\n",
    "            if len(observations) > 0:\n",
    "                coord_labels = [getattr(observations[0],\"coord{}_label\".format(i)) for i in range(1,4)]\n",
    "                coord_labels = [label[0] for label in coord_labels if len(label)>0]\n",
    "                physical_space = PhysicalRepresentationSpace(coordinate_labels=coord_labels)\n",
    "        self.setup_representation_space(physical_space, time_space, representation_spaces)\n",
    "        \n",
    "        # register the datast in the listed representation spaces\n",
    "        for space_i in self.representation_spaces:\n",
    "            space_i.attach_dataset(self)\n",
    "        \n",
    "        # initialize extension of the dataset\n",
    "        self.update_extension()\n",
    "            \n",
    "    def __dell__(self):\n",
    "        self.remove_all_observations()\n",
    "\n",
    "    def __setup_space(self, space, space_type):\n",
    "        \"\"\"Check if space of given type is in list or parameter and return the appropriate value.\n",
    "        \n",
    "        Take the given physical/time space, or if None use the first one in the list, and if none just leave None.\n",
    "        Adds the space to the `self.representation_spaces` set.\"\"\"\n",
    "        if space is not None: \n",
    "            self.representation_spaces.add(space)\n",
    "            return space\n",
    "        if len(self.representation_spaces) == 0: return None\n",
    "        space_list= [space_i for space_i in self.representation_spaces if isinstance(space_i, space_type)]\n",
    "        return space_list[0] if len(space_list) > 0 else None\n",
    "    \n",
    "    def update_extension(self, update_representation_spaces= True):\n",
    "        \"\"\"Sets the extension of the dataset in the physical space from existing observations\n",
    "        \n",
    "        If there is no observation, `self.extension` is set to None\"\"\"\n",
    "        observations = self.get_observations()\n",
    "        if (len(observations) == 0) or (self.physical_space is None) or (self.physical_space.dimension == 0):\n",
    "            self.extension = None \n",
    "            return\n",
    "            \n",
    "        di = observations[0]\n",
    "        coord = self.physical_space.get_object_coordinates(di)\n",
    "        self.extension = np.repeat([coord],2,axis=0).T\n",
    "        for di in observations[1:]:\n",
    "            coord = self.physical_space.get_object_coordinates(di)\n",
    "            for j, val in enumerate(coord):\n",
    "                self.extension[j,0] = min(self.extension[j,0], val)\n",
    "                self.extension[j,1] = max(self.extension[j,1], val)\n",
    "                \n",
    "        if update_representation_spaces:\n",
    "            if self.physical_space is not None:\n",
    "                self.physical_space.set_extension_from_data()\n",
    "        \n",
    "    def setup_representation_space(self, physical_space= None, time_space= None, representation_spaces= None):\n",
    "        \"\"\"Setup the representation space list and default\n",
    "        \n",
    "        Parameters:\n",
    "        - physical_space: a `PhysicalRepresentationSpace`,  which defines the spatial coordinates of this dataset\n",
    "        - time_space: a `TemporalRepresentationSpace`,  which defines the time coordinates of this dataset\n",
    "        - representation_spaces: list of `Representationspace`s to which the dataset must be attached\n",
    "        Note: datasets can be created without representation space and attached later on by using `RepresentationSpace.attach_dataset`\n",
    "        or `GeologicalDataset.setup_representation_space`.\n",
    "        Alternativelly, a single `PhysicalRepresentationspace` and or `TemporalRepresentationspace` can be given here if `physical_space` and `time_space` are None.\n",
    "        \"\"\"\n",
    "        if representation_spaces is not None: self.representation_spaces = self.representation_spaces.union(representation_spaces)\n",
    "        self.physical_space = self.__setup_space(physical_space, PhysicalRepresentationSpace)\n",
    "        self.time_space = self.__setup_space(time_space, TemporalRepresentationSpace)\n",
    "        \n",
    "        for space in self.representation_spaces:\n",
    "            space.attach_dataset(self)\n",
    "        \n",
    "    def get_observations(self, observation_type= None, qualities= None, name= None):\n",
    "        \"\"\"Accessor to the observations stored in the internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - observation_type: the type of the searched observations as defined by the internal ontology\n",
    "        - qualities: qualities to filter the observations (c.f. `KnowledgeFramework.search`)\n",
    "        - name: name of the serached observation (c.f. `KnowledgeFramework.search`)\"\"\"\n",
    "        observation_type = observation_type if observation_type is not None else self.knowledge_framework().PointBased_Observation\n",
    "        return self.knowledge_framework.search(type= observation_type, qualities= qualities, name= name)\n",
    "    \n",
    "    def get_unexplained_observations(self, observation_type= None, qualities= None, name= None):\n",
    "        \"\"\"Accessor filtering out explained observation\n",
    "        \n",
    "        Parameters:\n",
    "        - observation_type: the type of the searched observations as defined by the internal ontology\n",
    "        - qualities: qualities to filter the observations (c.f. `KnowledgeFramework.search`)\n",
    "        - name: name of the serached observation (c.f. `KnowledgeFramework.search`)\"\"\"\n",
    "        observations = self.get_observations(observation_type= observation_type, qualities= qualities, name= name)\n",
    "        observations = [obs_i for obs_i in observations if len(obs_i.is_Explained_By) == 0]\n",
    "        return observations\n",
    "    \n",
    "    def get_occurrence_observations(self):\n",
    "        \"\"\"helper method to access occurrence data, i.e., those having a occurrence quality\n",
    "        \n",
    "        :todo: for now the occurrence quality doesn't exist so all the observations are occurrence by default\"\"\"\n",
    "        return self.get_observations(qualities= \"occurrence\")\n",
    "    \n",
    "    def get_orientation_observations(self):\n",
    "        return self.get_observations(qualities= \"dip\")\n",
    "    \n",
    "    def remove_observation(self, observations, update_extension= True):\n",
    "        \"\"\"Removes the given observations stored in this dataset and internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - observations: an iterable containing objects of the internal ontology.\n",
    "        Note that you can use the `search`method to get such a list\n",
    "        - update_extension: if True (default) the extension will be updated\"\"\"\n",
    "        self.knowledge_framework.remove_all_instances(observations)\n",
    "        if update_extension: self.update_extension()\n",
    "            \n",
    "    def remove_observation_by_name(self, name:str, update_extension= True):\n",
    "        \"\"\"Removes the given observations stored in this dataset and internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation to be removed\n",
    "        - update_extension: if True (default) the extension will be updated\"\"\"\n",
    "        self.remove_observation(self.get_observations(name= name), update_extension)\n",
    "        \n",
    "    def remove_all_observations(self, update_extension= True):\n",
    "        \"\"\"Removes all the observations stored in this dataset and internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        Note: the update is performed only once at the end.\"\"\"\n",
    "        self.remove_observation(self.get_observations(), update_extension= False)\n",
    "        if update_extension: self.update_extension()\n",
    "        \n",
    "    def add_observation(self, name: str= None, update_extension= True, **kargs):\n",
    "        \"\"\"creates a new observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\n",
    "          |   Also Note: the arguments with None value are removed from the dict\"\"\"\n",
    "        if self.physical_space is  None:\n",
    "            raise MalissiaBaseError(\"Trying to add observation while physical space is not set. Please setup_representation_space first\")\n",
    "        \n",
    "        constructor = self.knowledge_framework.select_object_constructor(\"Observation\", dataset=self, **kargs)\n",
    "        new_observation = constructor(knowledge_framework= self.knowledge_framework, dataset= self, name= name, **kargs)\n",
    "        if update_extension: self.update_extension()\n",
    "        \n",
    "    def add_occurrence_observation(self, name: str, observed_object:str, occurrence= True, update_extension= True, **kargs):\n",
    "        \"\"\"creates a new occurrence observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - observed_object: the name of the observed object\n",
    "        - occurrence: True (default) if the object was observed here, False if it was observed that it is not there.\n",
    "        Note that this is different from not having observed that it is here, in which case there should not be an observation.\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\"\"\"\n",
    "        if observed_object is not None: kargs[\"geology\"]= observed_object\n",
    "        if occurrence is not None: kargs[\"occurrence\"]= occurrence\n",
    "        self.add_observation(name, update_extension= update_extension, **kargs)\n",
    "    \n",
    "    def add_orientation_observation(self, name: str, observed_object:str, dip, dip_dir, occurrence= True, update_extension= True, **kargs):\n",
    "        \"\"\"creates a new orientation observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - observed_object: the name of the observed object\n",
    "        - dip: the value of the measured dip (in degrees, 0-90)\n",
    "        - dip_dir: the value of the dip direction (in degrees, 0-360, from North towards the East)\n",
    "        - occurrence: True (default) if the object was observed here.\n",
    "        This is the default behaviour because if the measurement was made here, we assume that the object actually existed\n",
    "        so this is in itself a proof ox occurrence. However, one might want to record the orientation without specifically attaching any\n",
    "        observation of occurrence, in which case None should be given for occurrence and the quality won't be set.\n",
    "        False, would not make much sense as it would imply that the orientation was measured but we observed that the object wasn't there.\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\"\"\"\n",
    "        if occurrence is not None: kargs[\"occurrence\"]= occurrence\n",
    "        if occurrence == False: logging.warning(\"occurrence parameter was set to False while adding an orientation observation.\"\\\n",
    "            \"This is weird because it would imply the measure was taken but the rock couldn't be observed.\"\\\n",
    "            \"Did you intend to avoid recording the occurrence, in which case you should prefer None isntead of False.\")\n",
    "        \n",
    "        if observed_object is not None: kargs[\"geology\"]= observed_object\n",
    "        if (dip is not None) and (dip_dir is not None):\n",
    "            kargs[\"dip\"]= dip\n",
    "            kargs[\"dip_dir\"]= dip_dir\n",
    "        self.add_observation(name, update_extension= update_extension, **kargs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.get_observations())\n",
    "         \n",
    "    def __str__(self):\n",
    "        \"\"\"Description of the dataset\n",
    "        \"\"\"\n",
    "        desc= [\"A dataset of type: {:s}\".format(type(self).__name__)]\n",
    "        n = len(self)\n",
    "        if n == 0:\n",
    "            desc+= [\"- The dataset is empty\"]\n",
    "        else:\n",
    "            desc+= [\"- Size of dataset: {:d}\".format(n)]\n",
    "            desc+= [\"- Types of data:\"]\n",
    "            desc+= [\" |- occurrence:\\t{} entries\".format(len(self.get_occurrence_observations()))]\n",
    "            desc+= [\" |- orientation:\\t{} entries\".format(len(self.get_orientation_observations()))]\n",
    "        if self.extension is None:\n",
    "            desc+= [\"- Extension: \"+str(self.extension)]\n",
    "        else:\n",
    "            desc+= [\"- Extension:\"]\n",
    "            labels = self.physical_space.coordinate_labels if self.physical_space is not None else [\"Coord_{}\".format(i) for i in range(3)][:len(self.extension)]\n",
    "            for dim_i, lim_i in zip(labels,self.extension):\n",
    "                desc+= [\" |- Coord {}: [{}, {}]\".format(dim_i, *lim_i)]  \n",
    "        desc+= [\"- Number of spaces this dataset is attached to: {:d}\".format(len(self.representation_spaces))]\n",
    "        if self.physical_space is None:\n",
    "            desc+= [\" |- Physical space: None\"]\n",
    "        else:\n",
    "            desc+= [\" |- Physical space:\"+\"\\n | |\".join(self.physical_space.__str__().split(\"\\n\"))]\n",
    "        if self.time_space is None:\n",
    "            desc+= [\" |- Temporal space: None\"]\n",
    "        else:\n",
    "            desc+= [\" |- Temporal space:\"+\"\\n | |\".join(self.time_space.__str__().split(\"\\n\"))]\n",
    "        return \"\\n\".join(desc) \n",
    "    \n",
    "    def head(self, n:int= 5):\n",
    "        \"\"\"Returns the `n`first data in the dataset\"\"\"\n",
    "        return self.to_dataframe(max_rows=n)\n",
    "    \n",
    "    def info(self):\n",
    "        \"\"\"Returns a description of the dataset\"\"\"\n",
    "        return self.__str__()\n",
    "    \n",
    "    def to_dataframe(self, max_rows:int= None):\n",
    "        \"\"\"Creates a `pandas.DataFrame` showing the data in this dataset\n",
    "        \n",
    "        Parameters:\n",
    "        - max_rows: limits the number of rows in the output, unless None is given (default).\n",
    "        Note: interanlly, all the observations are still recovered from the internal ontology, \n",
    "        but only the `max_rows`first ones are show for consision.\n",
    "        \"\"\"\n",
    "        columns = [\"name\"]\n",
    "        if self.physical_space is not None:\n",
    "            columns += self.physical_space.coordinate_labels\n",
    "        if self.time_space is not None:\n",
    "            columns += self.time_space.coordinate_labels\n",
    "        columns += [\"dip_dir\",\"dip\",'geology', 'occurrence']\n",
    "        output_frame = pd.DataFrame(columns= columns)\n",
    "        output_frame.set_index(\"name\",inplace=True)\n",
    "        \n",
    "        observations = self.get_observations() \n",
    "        observations = observations if max_rows is None else observations[:max_rows]\n",
    "        for di in observations:\n",
    "            for prop in di.get_properties():\n",
    "                if prop.name == '': continue\n",
    "                if \"coord\" in prop.name: continue\n",
    "                output_frame.loc[di.name,prop.name] = prop[di][0]\n",
    "                \n",
    "            if self.physical_space is not None:\n",
    "                for label, val in zip(self.physical_space.coordinate_labels, self.physical_space.get_object_coordinates(di)):\n",
    "                    output_frame.loc[di.name,label] = val\n",
    "            if self.time_space is not None:\n",
    "                for label, val in zip(self.time_space.coordinate_labels, self.time_space.get_object_times(di)):\n",
    "                    output_frame.loc[di.name,label] = val\n",
    "        coordinate_types = {label:float for label in self.physical_space.coordinate_labels} if self.physical_space is not None else {}\n",
    "        time_types = {label:float for label in self.physical_space.coordinate_labels} if self.time_space is not None else {}\n",
    "        other_types = {'dip_dir':float, 'dip':float, 'geology':str, \"occurrence\": bool}\n",
    "        output_frame = output_frame.astype({**coordinate_types, **other_types})\n",
    "        return output_frame\n",
    "    \n",
    "def load_dataset_from_csv(source:str, dataset:GeologicalDataset = None, coordinate_labels = [\"X\",\"Y\",\"Z\"], **kargs) ->GeologicalDataset:\n",
    "    \"\"\"Loads a dataset from a csv file\n",
    "    \n",
    "    Parameters:\n",
    "    - source(str): the source file from which the data should be loaded\n",
    "    - dataset: the `GeologicalDataset` in which the data will be loaded. If None, the dataset will be created.\n",
    "    - **arkgs: passed to pandas.read_csv\n",
    "    \n",
    "    Return:\n",
    "    - the `GeologicalDataset` with the newly loaded data (a new `GeologicalDataset` is created if needed).\"\"\"\n",
    "    try:\n",
    "        dataframe = pd.read_csv(source, **filter_kargs(pd.read_csv,**kargs))\n",
    "    except Exception as e:\n",
    "        raise( Exception(\"This error occurred while loading a dataset from: {}\\nAdditional arguments were given: {}\".format(source,\n",
    "                            \",\".join([\"{}:{}\".format(key,val) for key, val in kargs.items()]))))\n",
    "        \n",
    "    coordinate_labels = [label for label in coordinate_labels if label in dataframe.columns]\n",
    "    if len(coordinate_labels) == 0:\n",
    "        logging.warning(\"There isn't any coordinate column in the loaded dataset.\\nCheck the output and consider changing the separator with sep keyword or cahnge 'coordinate_label' parameter.\")\n",
    "    return load_dataset_from_dataframe(dataframe, dataset, coordinate_labels= coordinate_labels, **filter_kargs(load_dataset_from_dataframe,**kargs) )\n",
    "\n",
    "def load_dataset_from_dataframe(dataframe, \n",
    "                                dataset:GeologicalDataset = None, \n",
    "                                coordinate_labels = [\"X\",\"Y\",\"Z\"], labels= None, index= None, \n",
    "                                dtypes= None, clear_existing_data= True):\n",
    "    \"\"\"Loads a dataset from a `pandas.DataFrame`\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe(`pandas.DataFrame`): the source dataframe from which the data should be loaded\n",
    "    - dataset: the `GeologicalDataset` in which the data will be loaded. If None, the dataset will be created.\n",
    "    - coordinate_labels: the labels to be used as coordinates of the physical space\n",
    "    - labels: a dict to relabel the dataframe columns prior to loading in the dataset.\n",
    "    This is usefull for example when the coordinates in the source aren't labelled the same as in the internal ontology.\n",
    "    The format is {\"old_label\":\"new_label\", ...}.\n",
    "    - index: the label of the column (in original DataFrame, i.e., before renaming), which is to be used as index\n",
    "    - dtypes: a dict containing a mapping between column name and type\n",
    "    - clear_existing_data: if set (default), any oservation stored in the internal ontology is removed prior to loading the new dataset \n",
    "    \n",
    "    Return:\n",
    "    - the `GeologicalDataset` with the newly loaded data (a new `GeologicalDataset` is created if needed).\"\"\"\n",
    "    if dataset is None:\n",
    "        physical_space = PhysicalRepresentationSpace(coordinate_labels= coordinate_labels)\n",
    "        dataset = GeologicalDataset(physical_space = physical_space)\n",
    "    else:\n",
    "        if dataset.physical_space is None or clear_existing_data:\n",
    "            physical_space = PhysicalRepresentationSpace(coordinate_labels= coordinate_labels)\n",
    "            dataset.setup_representation_space(physical_space = physical_space)\n",
    "        else:\n",
    "            if dataset.physical_space.coordinate_labels != coordinate_labels:\n",
    "                raise MalissiaBaseError(\"Trying to add data into an existing dataset with different coordiante labels\")\n",
    "                \n",
    "    if clear_existing_data:\n",
    "        dataset.remove_all_observations()\n",
    "\n",
    "    # declare default dtypes here, in case they should be relabelled\n",
    "    default_coord_types = {key:float for key in dataset.physical_space.coordinate_labels}\n",
    "    other_types = {'dip_dir':float, 'dip':float, 'geology':str, \n",
    "                   'observed_object':str, \"occurrence\":bool}\n",
    "    dtypes= dtypes if dtypes is not None else {**default_coord_types, **other_types}\n",
    "    assert isinstance(dtypes, dict), \"dtypes for type management should be given as a dict\"\n",
    "    \n",
    "    # relabelling\n",
    "    if labels is not None:\n",
    "        dataframe = dataframe.rename(columns= labels)\n",
    "        index= labels[index] if index in labels else index\n",
    "        dtypes= {labels[key] if key in labels else key: value for key, value in dtypes.items()}\n",
    "    \n",
    "    # setting the index\n",
    "    if index is not None:\n",
    "        # if already set, reset it\n",
    "        if type(dataframe.index) != pd.core.indexes.base.Index:\n",
    "            dataframe = dataframe.reset_index()\n",
    "        # then set the index\n",
    "        try:\n",
    "            dataframe = dataframe.set_index(index)\n",
    "        except Exception as e:\n",
    "            raise( Exception(\"Error while setting the index of the loaded dataset:\\nThis is how the dataframe looks like:\\n\"+dataframe.head().to_string()))\n",
    "    \n",
    "    for extra_type_i in dtypes.keys() - set(dataframe.columns):\n",
    "        del dtypes[extra_type_i]\n",
    "    dataframe = dataframe.astype(dtypes)\n",
    "        \n",
    "    for name_i, values_i in dataframe.iterrows():\n",
    "        # drop nan values and filter None objects\n",
    "        values_i = {key:val for key, val in values_i.dropna().items() if val is not None}\n",
    "        dataset.add_observation(name_i, **values_i, update_extension= False)\n",
    "    dataset.update_extension()\n",
    "    \n",
    "    # resetting the default size when all data are loaded if not specified\n",
    "    if \"size\" not in dataframe:\n",
    "        size = max(physical_space.get_size())\n",
    "        for di in dataset.get_observations():\n",
    "            di.size = [float(size/20)]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def remove_kargs(keys,**kargs):\n",
    "    if isinstance(keys,list):\n",
    "        for key in keys:\n",
    "            if key in kargs:\n",
    "                del kargs[key]\n",
    "    else:\n",
    "        if keys in kargs:\n",
    "            del kargs[keys]\n",
    "    return kargs \n",
    "\n",
    "def filter_kargs(target_function,**kargs):\n",
    "    \"\"\"Helper function to filter keyword arguments and only pass the needed ones in a function signature\"\"\"\n",
    "    sig = inspect.signature(target_function)\n",
    "    # check if there is a **kargs in the signature of the function, if yes it is ok as it will take care of the passed extra kargs\n",
    "    if not any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values()):\n",
    "        extra_args = kargs.keys() - sig.parameters.keys()\n",
    "        for args in extra_args:\n",
    "            del kargs[args]\n",
    "    return kargs\n",
    "\n",
    "def debuf_karg_filter(target_function, **kargs):\n",
    "    print(\"Kargs filtering:\")\n",
    "    print(\"before: {\",*[\"{}:{}\".format(key,val) for key, val in kargs.items()],\"}\")\n",
    "    print(\"after: {\",\",\".join([\"{}:{}\".format(key,val) for key, val in filter_kargs(target_function,**kargs).items()]),\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filter_kargs(pd.read_csv, **{\"sep\":\";\", \"truc\":\"test\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset\n",
    "\n",
    "Data are actually described within the ontology, here thanks to the *Data* class.<br>\n",
    "Adding new data points calls for creating new *Data* individuals (i.e., instances in the ontology). (see demos below, section datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(center, dip, dir, length= 1, ax= None, color = \"black\", **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "\n",
    "    center = np.array(center)\n",
    "    dip_rad = np.deg2rad(dip)\n",
    "    vec_x =  np.cos(dip_rad)\n",
    "    if dir == \"left\": vec_x *= -1\n",
    "    vec_z = -np.sin(dip_rad)\n",
    "    vect = 0.5 * length * np.array([vec_x,vec_z])\n",
    "    start = center - vect\n",
    "    end = center + vect\n",
    "    ax_plt.plot([start[0],end[0]],[start[1],end[1]], color = color, **kargs)\n",
    "    \n",
    "    return vect\n",
    "    \n",
    "def draw_dip_symbol(center, dip, dir, length= 1, polarity= None, ax= None, color = \"black\", polarity_ratio= 0.4, **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "    \n",
    "    vect = draw_line(center= center, dip= dip, dir= dir, length= length, ax= ax_plt, color = color, **kargs)\n",
    "    \n",
    "    if polarity is not None:\n",
    "        vect_pol = polarity_ratio * np.array([-vect[1],vect[0]])\n",
    "        if (dir == \"left\" and polarity == \"up\") or (dir == \"right\" and polarity == \"down\") : vect_pol *= -1\n",
    "        ax_plt.arrow(*center,*vect_pol, width=length/100, color = color, **kargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "class RenderingObject(object):\n",
    "    \"\"\"Class dedicated to rendering a `RepresentationSpace`\"\"\"\n",
    "    registered_drawing_methods = {}\n",
    "    \n",
    "    def __init__(self, space:RepresentationSpace):\n",
    "        \"\"\"Creating a rendering for the given `RepresentationSpace`\"\"\"\n",
    "        self.space = space\n",
    "\n",
    "    @classmethod\n",
    "    def register_drawing_method(cls, object_or_class, drawing_method):\n",
    "        \"\"\"Registers a drawing function for the given  `object`\n",
    "        \"\"\"\n",
    "        if (object_or_class is None):\n",
    "            raise MalissiaBaseError(\"Registering drawing method for an undefined object class.\")\n",
    "        if drawing_method is None: raise MalissiaBaseError(\"Registering  an undefined drawing method for an object class.\")\n",
    "        \n",
    "        object_class = object_or_class if isinstance(object_or_class,type) else type(object_or_class) \n",
    "        cls.registered_drawing_methods[object_class] = drawing_method\n",
    "        \n",
    "    @classmethod\n",
    "    def get_drawing_method(cls, self, object_or_class):\n",
    "        \"\"\"Accessor to the drawing method for a given object\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class_name (str): is the name of the class of the object to be drawned\n",
    "        \"\"\"\n",
    "        if (object_or_class is None):\n",
    "            raise MalissiaBaseError(\"Drawing methods are registered by object type. None was given, please give in a valid class.\")\n",
    "        \n",
    "        \n",
    "        if isinstance(object_or_class,type) :\n",
    "            return cls.registered_drawing_methods[object_or_class]\n",
    "        \n",
    "        elif isinstance(object_or_class, owl.Thing):\n",
    "            object_class = self.get_drawable_class_indiv(object_or_class)\n",
    "            if object_class :\n",
    "                return cls.registered_drawing_methods[object_class]\n",
    "            else:\n",
    "                raise MalissiaBaseError(\"No drawing method registered for this object: \"+ str(object_or_class))\n",
    "        \n",
    "        elif isinstance(object_or_class, list):\n",
    "            object_class = self.get_drawable_class_list(object_or_class)\n",
    "            if object_class :\n",
    "                return cls.registered_drawing_methods[object_class]\n",
    "            else:\n",
    "                raise MalissiaBaseError(\"No drawing method registered for this list_of_classes: \"+ str(object_or_class))\n",
    "        \n",
    "        object_class = object_or_class if isinstance(object_or_class,type) else type(object_or_class) \n",
    "        if object_class not in cls.registered_drawing_methods:\n",
    "            raise MalissiaBaseError(\"No drawing method registered for this class: \"+ str(object_class))\n",
    "        return cls.registered_drawing_methods[object_class]\n",
    "    \n",
    "\n",
    "    def get_drawable_class_indiv(self, individual):\n",
    "        \"\"\" this method returns the first drawble class of a given individual, assuming that the individual will have the same representation\n",
    "        using any drawing method associated with its classes. It retunrs None if no drawable method is found\"\"\"\n",
    "\n",
    "        for class_i in individual.is_a:\n",
    "            if class_i in self.registered_drawing_methods:\n",
    "                return class_i\n",
    "            else :\n",
    "                return None\n",
    "            \n",
    "    def get_drawable_class_list(self, individual_is_a_list):\n",
    "        \"\"\" this method returns the first drawble class of a list of classes of an individual (individual_is_a_list = indiidual.is_a)\n",
    "        assuming that the individual will have the same representation using any drawing method associated with its classes. \n",
    "        It retunrs None if no drawable method is found\"\"\"\n",
    "\n",
    "        for class_i in individual_is_a_list:\n",
    "            if class_i in self.registered_drawing_methods:\n",
    "                return class_i\n",
    "            else :\n",
    "                return None\n",
    "                \n",
    "\n",
    "\n",
    "        \n",
    "    def draw(self, object, **kargs):\n",
    "        self.get_drawing_method(object_or_class = object, self= self)(object, self, **kargs)\n",
    "        \n",
    "    def setup_ax(self, ax= None):\n",
    "            if ax is not None:\n",
    "                self.plt_ax = ax\n",
    "            elif self.plt_ax is None:\n",
    "                self.plt_ax = plt\n",
    "                \n",
    "    # def draw_interpreted_objects(self, ax= None, setup_drawing= True, **kargs):\n",
    "    #     if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "    #     for object_i in self.space.get_interpreted_objects():\n",
    "    #         object_class_name = type(object_i).name\n",
    "    #         drawing_method = self.get_drawing_method(object_class_name)\n",
    "    #         drawing_method(object_i, representation_space= self, ax= ax, setup_drawing= False, **kargs)\n",
    "                \n",
    "class AxisAlignedCrossSection(RenderingObject):\n",
    "    \"\"\"A specialised `Rendering` that procudes a cross-section.\n",
    "    \n",
    "    The cross-section is defined by two coordinates of the rendered `PhysicalRepresentationSpace`\"\"\"\n",
    "    registered_drawing_methods = {}\n",
    "    \n",
    "    def __init__(self, space:PhysicalRepresentationSpace, u= None, v= None, ax= None):\n",
    "        \"\"\"Creates a cross section through the given physical space\n",
    "        \n",
    "        Parameters:\n",
    "        - space: the space through which the cross section is going.\n",
    "        Note: the given space must be a `PhysicalRepresentationSpace` and have two or more coordinates\n",
    "        - u: the label of the abscissa axis among the physical space coordinates.\n",
    "        By default, if None is given, the first axis of the space is used.\n",
    "        - v: the label of the ordinate axis among the physical space coordinates.\n",
    "        By default, if None is given, the last axis of the space is used,\n",
    "        effectively using both coordinates if the space is 2D, but a vertical cross-section if it is 3D.\n",
    "        - ax: the matplotlib axis in which the space is to be rendered.\n",
    "        If None (default), then a new axis will be created.\"\"\"\n",
    "        assert isinstance(space,PhysicalRepresentationSpace), \"The given representation space must be a PhysicalRepresentationSpace, here: \"+str(type(space))\n",
    "        assert space.dimension > 1, \"Cross sections are only possible through spaces of dimensions >=2, here: \"+str(space.dimension)\n",
    "        self.space = space\n",
    "        \n",
    "        if u is None:\n",
    "            self.u = self.space.coordinate_labels[0]\n",
    "            self.u_index = 0\n",
    "        else:\n",
    "            assert u in self.space.coordinate_labels, \"The first given coordinate (u={}) is not in the represented space ({})\".format(u, \",\".join(space.coordinate_labels))\n",
    "            self.u = u\n",
    "            self.u_index = np.argwhere(self.space.coordinate_labels == self.u)[0,0]\n",
    "        \n",
    "        if v is None:\n",
    "            self.v = space.coordinate_labels[-1]\n",
    "            self.v_index = len(space.coordinate_labels) - 1\n",
    "        else:\n",
    "            assert v in space.coordinate_labels, \"The second given coordinate (v={}) is not in the represented space ({})\".format(v, \",\".join(space.coordinate_labels))\n",
    "            self.v = v\n",
    "            self.v_index = np.argwhere(self.space.coordinate_labels == self.v)[0,0]\n",
    "            \n",
    "        self.plt_ax = None\n",
    "                \n",
    "    def setup_drawing(self, ax= None):\n",
    "            self.setup_ax(ax)\n",
    "            \n",
    "            ax = self.plt_ax.gca() if self.plt_ax == plt else self.plt_ax\n",
    "            ax.set_aspect(\"equal\")\n",
    "            ax.set_xlim( *self.space.extension[self.u_index])\n",
    "            ax.set_xlabel(self.u)\n",
    "            ax.set_ylim( *self.space.extension[self.v_index])\n",
    "            ax.set_ylabel(self.v)\n",
    "    \n",
    "    def draw_segment(self, from_coord, to_coord, **kargs):\n",
    "        self.plt_ax.plot(\n",
    "            [from_coord[0],to_coord[0]],\n",
    "            [from_coord[1],to_coord[1]],\n",
    "            **filter_kargs(self.plt_ax.plot,**kargs)\n",
    "            )\n",
    "        \n",
    "    def draw_line(self, center, dip, dir, length= 1, color = \"black\", setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        center = np.array(center)\n",
    "        dip_rad = np.deg2rad(dip)\n",
    "        vec_x =  np.cos(dip_rad)\n",
    "        if dir == \"left\": vec_x *= -1\n",
    "        vec_z = -np.sin(dip_rad)\n",
    "        vect = 0.5 * length * np.array([vec_x,vec_z])\n",
    "        start = center - vect\n",
    "        end = center + vect\n",
    "        self.draw_segment(from_coord= start, to_coord= end, color = color, **kargs)\n",
    "        \n",
    "        return vect\n",
    "    \n",
    "    def draw_dip_symbol(self, data= None, dip= None, dip_dir= None, center= None,\n",
    "                        length= None, polarity= \"up\", color = \"black\", polarity_ratio= 0.4, setup_drawing= True, ax= None, zorder= 20, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "\n",
    "        dip = data.dip[0] if data is not None else dip\n",
    "        dip_dir = data.dip_dir[0] if data is not None else dip_dir\n",
    "        dir = \"right\" if dip_dir < 180 else \"left\"\n",
    "        center = self.get_center_coordinates(data) if data is not None else center\n",
    "        \n",
    "        length = data.size[0] if (data is not None) and (data.size is not None) else length\n",
    "        if length is None or length is np.nan:\n",
    "            length = np.abs(np.max(self.space.extension[:,1] - self.space.extension[:,0])) / 20\n",
    "        \n",
    "        vect = self.draw_line(center= center, dip= dip, dir= dir, length= length, color = color, setup_drawing=False, ax=ax, zorder= zorder, **kargs)\n",
    "        \n",
    "        if (data is not None) and (data.polarity is not None):\n",
    "            polarity = \"up\" if data.polarity[0] else \"down\"\n",
    "        if polarity is not None:\n",
    "            vect_pol = polarity_ratio * np.array([-vect[1],vect[0]])\n",
    "            if (dir == \"left\" and polarity == \"up\") or (dir == \"right\" and polarity == \"down\") : vect_pol *= -1\n",
    "            self.plt_ax.arrow(*center,*vect_pol, width=length/100, color = color,  zorder= zorder, **filter_kargs(self.plt_ax.arrow,**kargs))\n",
    "        \n",
    "    def filter_section_coordinates(self, coord):\n",
    "        return coord[[self.u_index,self.v_index]]\n",
    "        \n",
    "    def get_center_coordinates(self,di):\n",
    "        coord = self.space.get_object_coordinates(di, kind= \"center\")\n",
    "        return self.filter_section_coordinates(coord)\n",
    "    \n",
    "    def get_normal_coordinates(self,di):\n",
    "        coord = self.space.get_object_coordinates(di, kind= \"normal\")\n",
    "        return self.filter_section_coordinates(coord)\n",
    "    \n",
    "    def get_corner_coordinates(self, surf):\n",
    "        coord = self.space.get_object_coordinates(surf, kind= \"nodes\")\n",
    "        return self.filter_section_coordinates(coord.T)\n",
    "    \n",
    "    def draw_dip_data(self, ax= None, polarity= \"up\", setup_drawing= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        for dataset_i in self.space.get_datasets():\n",
    "            for di in dataset_i.get_orientation_observations():\n",
    "                self.draw_dip_symbol(data = di, setup_drawing= False, ax=ax, **kargs) \n",
    "    \n",
    "    def draw_point(self, center= None, data = None, color = \"black\", marker=\"*\", edge_color= \"black\", zorder= 10, \n",
    "                                                        setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        center = self.get_center_coordinates(data) if data is not None else center\n",
    "        self.plt_ax.scatter(*center, color = color, marker= marker, edgecolors= edge_color, zorder=zorder, **filter_kargs(self.plt_ax.scatter,**kargs))\n",
    "        \n",
    "    def draw_plane(self, plane, color = \"limegreen\", marker=\".\", edge_color= \"black\", \n",
    "                   draw_normal= False, zorder= 2, setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        corner_coords = self.get_corner_coordinates(plane)\n",
    "        self.plt_ax.scatter(*corner_coords, color = color, marker= marker, edgecolors= edge_color, zorder=zorder, **filter_kargs(self.plt_ax.scatter,**kargs))\n",
    "        \n",
    "        plane_coords = np.append(corner_coords,corner_coords[:,0].reshape((2,1)),axis=1)\n",
    "        self.plt_ax.plot(*plane_coords, color = color, marker= None, zorder=zorder-1, **filter_kargs(self.plt_ax.plot,**kargs))\n",
    "\n",
    "        if draw_normal:\n",
    "            center = np.array(self.get_center_coordinates(plane))\n",
    "            normal = np.array(self.get_normal_coordinates(plane))\n",
    "            length = spatial.distance.pdist(corner_coords.T, metric=\"euclidean\").max()\n",
    "            end = center + 0.05 * length * normal\n",
    "            self.draw_segment(center, end, color = color, marker= None, zorder=zorder-1, **kargs)\n",
    "\n",
    "\n",
    "    def draw_multiple_planes(self, list_of_planes, color = \"olivedrab\", marker=\".\", edge_color= \"black\", \n",
    "                   draw_normal= False, zorder= 2, setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        for plane in list_of_planes:\n",
    "            corner_coords = self.get_corner_coordinates(plane)\n",
    "            self.plt_ax.scatter(*corner_coords, color = color, marker= marker, edgecolors= edge_color, zorder=zorder, **filter_kargs(self.plt_ax.scatter,**kargs))\n",
    "            \n",
    "            plane_coords = np.append(corner_coords,corner_coords[:,0].reshape((2,1)),axis=1)\n",
    "            self.plt_ax.plot(*plane_coords, color = color, marker= None, zorder=zorder-1, **filter_kargs(self.plt_ax.plot,**kargs))\n",
    "\n",
    "            if draw_normal:\n",
    "                center = np.array(self.get_center_coordinates(plane))\n",
    "                normal = np.array(self.get_normal_coordinates(plane))\n",
    "                length = spatial.distance.pdist(corner_coords.T, metric=\"euclidean\").max()\n",
    "                end = center + 0.05 * length * normal\n",
    "                self.draw_segment(center, end, color = color, marker= None, zorder=zorder-1, **kargs)\n",
    "\n",
    "    \n",
    "    def draw_explanation(self, explained_object, projection, color= \"k\",\n",
    "                         highlight_explained_object= False, highlight_linewidth= 3,\n",
    "                         zorder= 3, linestyle= \"dashed\", **kargs):\n",
    "        center_object = self.get_center_coordinates(explained_object)\n",
    "        projection = self.filter_section_coordinates(projection)\n",
    "        self.draw_segment(center_object, projection, color= color, linestyle= linestyle)\n",
    "        \n",
    "        if highlight_explained_object:\n",
    "            self.draw(explained_object,zorder= zorder+1,\n",
    "                        path_effects=[patheffects.withStroke(foreground=color, linewidth= highlight_linewidth)]\n",
    "                      )\n",
    "    \n",
    "    def draw_occurrence_symbol(self, center= None, data = None, color = \"lightblue\", marker=\"o\", edge_color= \"black\", zorder= 20, setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        self.draw_point( center= center, data = data,  color= color, marker= marker, edge_color= edge_color, zorder=zorder, setup_drawing= False, ax=ax, **kargs) \n",
    "        \n",
    "    def draw_occurrence_data(self, ax= None, setup_drawing= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        for dataset_i in self.space.get_datasets():\n",
    "            for di in dataset_i.get_occurrence_observations():\n",
    "                self.draw_occurrence_symbol(data= di, setup_drawing= False, ax=ax, **kargs) \n",
    "            \n",
    "    def draw_interpreted_objects(self, ax= None, setup_drawing= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        for obj_i in self.space.get_interpreted_objects():\n",
    "            self.draw(obj_i, ax= ax, setup_drawing= False, **kargs) \n",
    "            \n",
    "    def draw_relation(self, obj1, obj2, ax= None, setup_drawing= True, color= \"black\", **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        obj1_nodes = self.get_corner_coordinates(obj1).T\n",
    "        obj2_nodes = self.get_corner_coordinates(obj2).T\n",
    "        dist = spatial.distance.cdist(obj1_nodes, obj2_nodes, \"euclidean\")\n",
    "        index_from, index_to = np.unravel_index(np.argmin(dist), shape= dist.shape)\n",
    "        self.draw_segment(obj1_nodes[index_from], obj2_nodes[index_to], color= color, **kargs)\n",
    "        \n",
    "    def show(self, ax= None, setup_drawing= True, show_objects= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        self.draw_occurrence_data(ax=ax, setup_drawing= False, **kargs)\n",
    "        self.draw_dip_data(ax=ax, setup_drawing= False, **kargs)\n",
    "        if show_objects:\n",
    "            self.draw_interpreted_objects(ax=ax, setup_drawing= False, **kargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_observation_in_AxisAlignedCrossSection(\n",
    "        observation,\n",
    "        drawing_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        drawing_space.setup_drawing(ax)\n",
    "    else:\n",
    "        drawing_space.setup_ax(ax)\n",
    "        \n",
    "    if (observation.occurrence is not None) and (len(observation.occurrence)>0) :\n",
    "        drawing_space.draw_occurrence_symbol(data= observation, ax= ax, setup_drawing= False, **kargs)\n",
    "    if (observation.dip is not None) and (len(observation.dip)>0):\n",
    "        drawing_space.draw_dip_symbol(data= observation, ax= ax, setup_drawing= False, **kargs)\n",
    "        \n",
    "if mogi().PointBased_Observation in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().PointBased_Observation]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().PointBased_Observation, draw_observation_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dataset_in_AxisAlignedCrossSection(\n",
    "        dataset: GeologicalDataset,\n",
    "        drawing_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        drawing_space.setup_drawing(ax)\n",
    "    else:\n",
    "        drawing_space.setup_ax(ax)\n",
    "            \n",
    "    # draw created objects\n",
    "    for d_i in dataset.get_observations():\n",
    "        drawing_space.draw(d_i, setup_drawing= False, **kargs)\n",
    "        \n",
    "if GeologicalDataset in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[GeologicalDataset]\n",
    "AxisAlignedCrossSection.register_drawing_method(GeologicalDataset, draw_dataset_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def draw_fold_in_AxisAlignedCrossSection(\n",
    "        fold,\n",
    "        drawing_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        color = \"darkgreen\",\n",
    "        color_axi_surf = \"blue\",\n",
    "        draw_limbs_normal= False,\n",
    "        draw_axial_surface = True, \n",
    "        draw_axial_surface_normal = False,\n",
    "        draw_explanation= False,\n",
    "        draw_explained_objects_normal = None,\n",
    "        **kargs\n",
    "    ):\n",
    "\n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "\n",
    "    if setup_drawing:\n",
    "        drawing_space.setup_drawing(ax)\n",
    "    else:\n",
    "        drawing_space.setup_ax(ax)\n",
    "    limb1 = fold.has_Limb1[0]\n",
    "    limb2 = fold.has_Limb2[0]    \n",
    "    plane1 = limb1.has_Representation[0]\n",
    "    drawing_space.draw_plane(plane1, color= color, draw_normal= draw_limbs_normal, **kargs)\n",
    "    plane2 = limb2.has_Representation[0]\n",
    "    drawing_space.draw_plane(plane2, color= color, draw_normal= draw_limbs_normal, **kargs)\n",
    "    if draw_axial_surface == True:\n",
    "        axial_surface = fold.has_Axial_Surface[0]\n",
    "        plane3 = axial_surface.has_Representation[0]\n",
    "        drawing_space.draw_plane(plane3, color= color_axi_surf, \n",
    "                                        draw_normal= draw_axial_surface_normal, **kargs)\n",
    "    \n",
    "    \n",
    "    if draw_explanation:\n",
    "        if (fold.explain is not None) and (len(fold.explain) > 0):\n",
    "            face1 = limb1.has_Representation[0]\n",
    "            face2 = limb2.has_Representation[0]\n",
    "            extreme_faces = [face1, face2]\n",
    "            space = drawing_space.space\n",
    "            n1 = space.get_object_coordinates(face1, \"normal\")\n",
    "            n2 = space.get_object_coordinates(face2, \"normal\")\n",
    "            vectors = [n1, n2]\n",
    "            plane = drawing_space(complex_surface = None, physical_space = space, \n",
    "                                   knowledge_framework = knowledge_framework, \n",
    "                                   vectors = vectors, extreme_faces= extreme_faces, force_computing_for_fold = True, fold = fold)\n",
    "\n",
    "            for explained_object in fold.explain:\n",
    "                \n",
    "                \n",
    "                # compute projection point\n",
    "                projection = drawing_space.space.compute_projection_on_plane(plane=plane, \n",
    "                                                                object= explained_object)\n",
    "                \n",
    "                # draw explanation link\n",
    "                drawing_space.draw_explanation(explained_object= explained_object, projection= projection,\n",
    "                                                      color= color, draw_normal= draw_explained_objects_normal,\n",
    "                                                      highlight_explained_object= True, **kargs )    \n",
    "    \n",
    "    \n",
    "if mogi().Fold in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().Fold]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().Fold, draw_fold_in_AxisAlignedCrossSection)\n",
    "\n",
    "if mogi().Chevron_Fold in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().Chevron_Fold]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().Chevron_Fold, draw_fold_in_AxisAlignedCrossSection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def draw_folding_train_in_AxisAlignedCrossSection(\n",
    "        folding_train,\n",
    "        drawing_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        color = \"darkgreen\",\n",
    "        color_axi_surf = \"blue\",\n",
    "        draw_limbs_normal= False,\n",
    "        draw_axial_surface = True, \n",
    "        draw_axial_surface_normal = False,\n",
    "        draw_explanation= False,\n",
    "        draw_explained_objects_normal = None,\n",
    "        **kargs\n",
    "    ):\n",
    "\n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    \n",
    "\n",
    "    if setup_drawing:\n",
    "        drawing_space.setup_drawing(ax)\n",
    "    else:\n",
    "        drawing_space.setup_ax(ax)\n",
    "\n",
    "    for fold in folding_train.has_Part:\n",
    "        limb1 = fold.has_Limb1[0]\n",
    "        limb2 = fold.has_Limb2[0]    \n",
    "        plane1 = limb1.has_Representation[0]\n",
    "        drawing_space.draw_plane(plane1, color= color, draw_normal= draw_limbs_normal, **kargs)\n",
    "        plane2 = limb2.has_Representation[0]\n",
    "        drawing_space.draw_plane(plane2, color= color, draw_normal= draw_limbs_normal, **kargs)\n",
    "        if draw_axial_surface == True:\n",
    "            axial_surface = fold.has_Axial_Surface[0]\n",
    "            plane3 = axial_surface.has_Representation[0]\n",
    "            drawing_space.draw_plane(plane3, color= color_axi_surf, \n",
    "                                            draw_normal= draw_axial_surface_normal, **kargs)\n",
    "        \n",
    "        \n",
    "        if draw_explanation:\n",
    "            if (fold.explain is not None) and (len(fold.explain) > 0):\n",
    "                face1 = limb1.has_Representation[0]\n",
    "                face2 = limb2.has_Representation[0]\n",
    "                extreme_faces = [face1, face2]\n",
    "                space = drawing_space.space\n",
    "                n1 = space.get_object_coordinates(face1, \"normal\")\n",
    "                n2 = space.get_object_coordinates(face2, \"normal\")\n",
    "                vectors = [n1, n2]\n",
    "                plane = drawing_space(complex_surface = None, physical_space = space, \n",
    "                                    knowledge_framework = knowledge_framework, \n",
    "                                    vectors = vectors, extreme_faces= extreme_faces, force_computing_for_fold = True, fold = fold)\n",
    "\n",
    "                for explained_object in fold.explain:\n",
    "                    \n",
    "                    \n",
    "                    # compute projection point\n",
    "                    projection = drawing_space.space.compute_projection_on_plane(plane=plane, \n",
    "                                                                    object= explained_object)\n",
    "                    \n",
    "                    # draw explanation link\n",
    "                    drawing_space.draw_explanation(explained_object= explained_object, projection= projection,\n",
    "                                                        color= color, draw_normal= draw_explained_objects_normal,\n",
    "                                                        highlight_explained_object= True, **kargs ) \n",
    "    if draw_explanation:\n",
    "        if (fold.explain is not None) and (len(fold.explain) > 0):\n",
    "            face1 = limb1.has_Representation[0]\n",
    "            face2 = limb2.has_Representation[0]\n",
    "            extreme_faces = [face1, face2]\n",
    "            space = drawing_space.space\n",
    "            n1 = space.get_object_coordinates(face1, \"normal\")\n",
    "            n2 = space.get_object_coordinates(face2, \"normal\")\n",
    "            vectors = [n1, n2]\n",
    "            plane = drawing_space(complex_surface = None, physical_space = space, \n",
    "                                knowledge_framework = knowledge_framework, \n",
    "                                vectors = vectors, extreme_faces= extreme_faces, force_computing_for_fold = True, fold = fold)\n",
    "\n",
    "            for explained_object in fold.explain:\n",
    "                \n",
    "                \n",
    "                # compute projection point\n",
    "                projection = drawing_space.space.compute_projection_on_plane(plane=plane, \n",
    "                                                                object= explained_object)\n",
    "                \n",
    "                # draw explanation link\n",
    "                drawing_space.draw_explanation(explained_object= explained_object, projection= projection,\n",
    "                                                    color= color, draw_normal= draw_explained_objects_normal,\n",
    "                                                    highlight_explained_object= True, **kargs )       \n",
    "    \n",
    "    \n",
    "if mogi().Fold_Train in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().Fold_Train]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().Fold_Train, draw_folding_train_in_AxisAlignedCrossSection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_plane_for_drawing(complex_surface, physical_space, \n",
    "                                   knowledge_framework, vectors = None, extreme_faces= None,\n",
    "                                     force_computing_for_fold = False, fold = None):\n",
    "    if force_computing_for_fold == True:\n",
    "        vectors = vectors\n",
    "        extreme_faces = extreme_faces\n",
    "    else:\n",
    "        if len(complex_surface.has_Part) < 2:\n",
    "            raise MalissiaBaseError('the complex_surface {} has less then two parts'.format(complex_surface.name))\n",
    "        vectors = [physical_space.get_object_coordinates(part,\n",
    "                     \"normal\") for part in complex_surface.has_Part ]\n",
    "        \n",
    "    vectors = np.array(vectors)\n",
    "    # Compute the mean along each axis\n",
    "    mean_vector = np.mean(vectors, axis=0)\n",
    "    dip, dip_dir = compute_dip_dir_from_normal(mean_vector) \n",
    "    center = compute_center_complex_surface(knowledge_framework, physical_space, \n",
    "                                            complex_surface, extreme_faces= extreme_faces, \n",
    "                                            force_computing_for_fold = force_computing_for_fold)\n",
    "    size = compute_size_complex_surface_for_representation(knowledge_framework, physical_space, \n",
    "                                                           complex_surface, extreme_faces= extreme_faces)\n",
    "    if force_computing_for_fold == True:\n",
    "        name = fold.name\n",
    "    else:\n",
    "        name = complex_surface.name\n",
    "    return constructor_planar_surface_from_center_attitude(knowledge_framework, physical_space.coordinate_labels, center, dip, dip_dir, size= size,\n",
    "                                                     polarity= True, name= 'plane of {} for projection'.format(name))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_stratigraphic_part_in_AxisAlignedCrossSection(\n",
    "        strati_part,\n",
    "        drawing_space, \n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        color = \"limegreen\",\n",
    "        draw_normal = False,\n",
    "        draw_explanation= False,\n",
    "        **kargs\n",
    "    ):\n",
    "\n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    if setup_drawing:\n",
    "        drawing_space.setup_drawing(ax)\n",
    "    else:\n",
    "        drawing_space.setup_ax(ax)\n",
    "    representation = strati_part.has_Representation[0]\n",
    "    \n",
    "    if knowledge_framework().Planar_Surface in representation.is_a :\n",
    "        drawing_space.draw_plane(representation, color= color, draw_normal= draw_normal, **kargs)\n",
    "        plane = representation   \n",
    "    \n",
    "    elif knowledge_framework().Complex_Surface in representation.is_a:\n",
    "        color = \"olivedrab\" if color == \"limegreen\" or color == None else color\n",
    "        drawing_space.draw_multiple_planes(representation.has_Part, color = color,  \n",
    "                   draw_normal= draw_normal, **kargs)\n",
    "            \n",
    "    space = drawing_space.space              \n",
    "    if draw_explanation:\n",
    "        if knowledge_framework().Complex_Surface in representation.is_a:\n",
    "            plane = compute_mean_plane_for_drawing(representation, \n",
    "                                               space, # must add getter of physical space from cplx_surf\n",
    "                                               knowledge_framework)\n",
    "\n",
    "        if (strati_part.explain is not None) and (len(strati_part.explain) > 0):\n",
    "            for explained_object in strati_part.explain:\n",
    "                \n",
    "                # compute projection point\n",
    "                projection = space.compute_projection_on_plane(plane=plane, object= explained_object)\n",
    "                \n",
    "                # draw explanation link\n",
    "                drawing_space.draw_explanation(explained_object= explained_object, projection= projection,\n",
    "                                                      color= color, draw_normal= draw_normal, \n",
    "                                                      highlight_explained_object= True, **kargs )\n",
    "        \n",
    "        \n",
    "if mogi().Stratigraphic_Part in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().Stratigraphic_Part]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().Stratigraphic_Part, draw_stratigraphic_part_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_limb_in_AxisAlignedCrossSection(\n",
    "        strati_part,\n",
    "        drawing_space, \n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        color = \"limegreen\",\n",
    "        draw_normal = False,\n",
    "        draw_explanation= False,\n",
    "        **kargs\n",
    "    ):\n",
    "\n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    if setup_drawing:\n",
    "        drawing_space.setup_drawing(ax)\n",
    "    else:\n",
    "        drawing_space.setup_ax(ax)\n",
    "    representation = strati_part.has_Representation[0]\n",
    "    \n",
    "    if knowledge_framework().Planar_Surface in representation.is_a :\n",
    "        drawing_space.draw_plane(representation, color= color, draw_normal= draw_normal, **kargs)\n",
    "        plane = representation   \n",
    "    \n",
    "    elif knowledge_framework().Complex_Surface in representation.is_a:\n",
    "        color = \"olivedrab\" if color == \"limegreen\" or color == None else color\n",
    "        drawing_space.draw_multiple_planes(representation.has_Part, color = color,  \n",
    "                   draw_normal= draw_normal, **kargs)\n",
    "            \n",
    "    space = drawing_space.space            \n",
    "        \n",
    "    if draw_explanation:\n",
    "        if knowledge_framework().Complex_Surface in representation.is_a:\n",
    "            plane = compute_mean_plane_for_drawing(representation, \n",
    "                                               space, # must add getter of physical space from cplx_surf\n",
    "                                               knowledge_framework)\n",
    "\n",
    "        if (strati_part.explain is not None) and (len(strati_part.explain) > 0):\n",
    "            for explained_object in strati_part.explain:\n",
    "                \n",
    "                # compute projection point\n",
    "                projection = space.compute_projection_on_plane(plane=plane, object= explained_object)\n",
    "                \n",
    "                # draw explanation link\n",
    "                drawing_space.draw_explanation(explained_object= explained_object, projection= projection,\n",
    "                                                      color= color, draw_normal= draw_normal, \n",
    "                                                      highlight_explained_object= True, **kargs )\n",
    "        \n",
    "        \n",
    "if mogi().Fold_Limb in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().Fold_Limb]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().Fold_Limb, draw_limb_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_situation_in_AxisAlignedCrossSection(\n",
    "        situation,\n",
    "        drawing_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        zorder= 100,\n",
    "        feature_color= 'deepskyblue',\n",
    "        feature_linewidth= 3,\n",
    "        explaining_color= 'orange',\n",
    "        explaining_linewidth= 4,\n",
    "        anomaly_color= 'red',\n",
    "        anomaly_linewidth= 5,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        drawing_space.setup_drawing(ax)\n",
    "    else:\n",
    "        drawing_space.setup_ax(ax)\n",
    "    \n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    for feature_i in situation.features:\n",
    "        drawing_space.draw(feature_i,\n",
    "                              path_effects=[patheffects.withStroke(foreground= feature_color, linewidth= feature_linewidth)],\n",
    "                              ax = ax, setup_drawing= False, zorder= zorder, **kargs)\n",
    "        \n",
    "    if situation.explaining_object is not None:\n",
    "        drawing_space.draw(situation.explaining_object,\n",
    "                              path_effects=[patheffects.withStroke(foreground= explaining_color, linewidth= explaining_linewidth)],\n",
    "                              ax = ax, setup_drawing= False, zorder= zorder+1,  **kargs)\n",
    "        \n",
    "    if (situation.anomalies is not None) and (len(situation.anomalies) > 0):\n",
    "        for anomaly in situation.anomalies:\n",
    "            drawing_space.draw(anomaly,\n",
    "                            color = anomaly_color, linewidth= anomaly_linewidth,\n",
    "                              ax = ax, setup_drawing= False, zorder= zorder+2,  **kargs)\n",
    "        \n",
    "if InterpretationSituation in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[InterpretationSituation]\n",
    "AxisAlignedCrossSection.register_drawing_method(InterpretationSituation, draw_situation_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_representation_anomaly_in_AxisAlignedCrossSection(\n",
    "        anomaly,\n",
    "        drawing_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        color = \"r\",\n",
    "        linewidth= 3,\n",
    "        zorder= 200,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        drawing_space.setup_drawing(ax)\n",
    "    else:\n",
    "        drawing_space.setup_ax(ax)\n",
    "    \n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    if (anomaly.is_Related_To_Representation is not None) and (len(anomaly.is_Related_To_Representation) >0):\n",
    "        if anomaly.is_Related_To_Representation[0] in drawing_space.registered_drawing_methods:\n",
    "            drawing_space.draw(anomaly.is_Related_To_Representation[0],\n",
    "                              path_effects=[patheffects.withStroke(foreground= color, linewidth= linewidth)],\n",
    "                              ax = ax, setup_drawing= False, zorder= zorder,  **kargs)\n",
    "        else:\n",
    "            drawing_space.draw(anomaly.is_Related_To[0],\n",
    "                              path_effects=[patheffects.withStroke(foreground= color, linewidth= linewidth)],\n",
    "                              ax = ax, setup_drawing= False, zorder= zorder, **kargs)\n",
    "        \n",
    "if mogi().RepresentationAnomaly in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().RepresentationAnomaly]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().RepresentationAnomaly, draw_representation_anomaly_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dipping_stratigraphy_anomaly_in_AxisAlignedCrossSection(\n",
    "        anomaly,\n",
    "        drawing_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        draw_normal= False,\n",
    "        color = \"r\",\n",
    "        linewidth= 5,\n",
    "        zorder = 200,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        drawing_space.setup_drawing(ax)\n",
    "    else:\n",
    "        drawing_space.setup_ax(ax)\n",
    "    \n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    drawing_space.draw(anomaly.is_Related_To[0],\n",
    "                              path_effects=[patheffects.withStroke(foreground= color, linewidth= linewidth)],\n",
    "                              ax = ax, setup_drawing= False, zorder= zorder,\n",
    "                              draw_normal= draw_normal, **kargs)\n",
    "        \n",
    "if mogi().DippingStratigraphyAnomaly in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().DippingStratigraphyAnomaly]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().DippingStratigraphyAnomaly, draw_dipping_stratigraphy_anomaly_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def get_shades_between_colors(color1, color2, num_shades):\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom', [color1, color2], N=num_shades)\n",
    "    shades = [mcolors.rgb2hex(cmap(i)[:3]) for i in range(cmap.N)]\n",
    "    return shades\n",
    "\n",
    "def get_shade_names(shades):\n",
    "    shade_names = [mcolors.to_rgba(shade) for shade in shades]\n",
    "    return [mcolors.to_hex(color) for color in shade_names]\n",
    "\n",
    "def draw_shared_sides_size_cplx_sur_anomaly_in_AxisAlignedCrossSection(\n",
    "        anomaly,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        draw_normal= False,\n",
    "        bounding_color1 = \"red\", \n",
    "        bounding_color2 = \"blue\",\n",
    "        zorder = 200,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "    if len(anomaly.is_Related_To) < 1 :\n",
    "        print('Warning: anomaly {} is not related to any complex_surface'.format(anomaly.name))\n",
    "        return None\n",
    "    \n",
    "    complex_surface = anomaly.is_Related_To[0]\n",
    "    if len(complex_surface.has_Part) < 2:\n",
    "        print('Warning: complex_surface {} related to the anomaly {} deos not have enough parts '.format(complex_surface.name, anomaly.name))\n",
    "        return None\n",
    "    space = representation_space.space \n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "\n",
    "    share_sides = get_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, space)\n",
    "    unmatched_sides = get_unmatched_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, space)\n",
    "    sides = unmatched_sides+share_sides\n",
    "    start_node_all_sides = {}\n",
    "    sizes = []\n",
    "    for side_i in sides:\n",
    "        start_i = side_i.has_End_Points[0]\n",
    "        start_size = side_i.size[0]\n",
    "        start_node_all_sides[side_i.has_End_Points[0].name] = [start_i, start_size ]\n",
    "        if start_size not in sizes:\n",
    "            sizes.append(start_size)\n",
    "\n",
    "    num_shades = len(sizes)\n",
    "\n",
    "    shades_between_colors = get_shades_between_colors(bounding_color1, bounding_color2, num_shades)\n",
    "    \n",
    "    # Normalize values between 0 and 1\n",
    "    norm = mcolors.Normalize(vmin=min(sizes), vmax=max(sizes))\n",
    "\n",
    "    for key, values in start_node_all_sides.items():\n",
    "        # Map the value to an index in the color shade list\n",
    "        center, size = values\n",
    "        index = int(norm(size) * (num_shades - 1))\n",
    "        color_i = shades_between_colors[index] \n",
    "\n",
    "        representation_space.draw_point(data= center, marker=\"o\", \n",
    "                                        color = None, c = color_i, ax = ax  )\n",
    "        \n",
    "if mogi().ComplexSurfaceSizeSharedSidesAnomaly in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().ComplexSurfaceSizeSharedSidesAnomaly]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().ComplexSurfaceSizeSharedSidesAnomaly, draw_shared_sides_size_cplx_sur_anomaly_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dip_variation_anomaly_in_AxisAlignedCrossSection(\n",
    "        anomaly,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        draw_normal= False,\n",
    "        color = \"r\",\n",
    "        linewidth= 5,\n",
    "        zorder = 200,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "    \n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    representation_space.draw(anomaly.is_Related_To[0], ax = ax, setup_drawing= False, zorder= zorder+1,\n",
    "                              path_effects=[patheffects.withStroke(foreground=color, linewidth= linewidth)],\n",
    "                              draw_normal= draw_normal, **kargs)\n",
    "    representation_space.draw(anomaly.is_Related_To[1], ax = ax, setup_drawing= False, zorder= zorder+1,\n",
    "                              path_effects=[patheffects.withStroke(foreground=color, linewidth= linewidth)],\n",
    "                              draw_normal= draw_normal, **kargs)\n",
    "    representation_space.draw_relation(*anomaly.is_Related_To, color= color, alpha= 0.8, linestyle= \"dashed\", ax = ax, setup_drawing= False,\n",
    "                                       zorder= zorder, **kargs)\n",
    "        \n",
    "if mogi().DipVariationAnomaly in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().DipVariationAnomaly]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().DipVariationAnomaly, draw_dip_variation_anomaly_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_internal_dip_variation_anomaly_two_parts_in_AxisAlignedCrossSection(\n",
    "        anomaly,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        draw_normal= False,\n",
    "        color = \"r\",\n",
    "        linewidth= 5,\n",
    "        zorder = 200,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "    \n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    representation_space.draw(anomaly.is_Related_To[0], ax = ax, setup_drawing= False, zorder= zorder+1,\n",
    "                              path_effects=[patheffects.withStroke(foreground=color, linewidth= linewidth)],\n",
    "                              draw_normal= draw_normal, **kargs)\n",
    "\n",
    "        \n",
    "if mogi().InternalDipVariationAnomalyTwoParts in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().InternalDipVariationAnomalyTwoParts]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().InternalDipVariationAnomalyTwoParts,\n",
    "                                                 draw_internal_dip_variation_anomaly_two_parts_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_internal_dip_variation_anomaly_multi_parts_in_AxisAlignedCrossSection(\n",
    "        anomaly,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        draw_normal= False,\n",
    "        color = \"r\",\n",
    "        linewidth= 5,\n",
    "        zorder = 200,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "    \n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    representation_space.draw(anomaly.is_Related_To[0], ax = ax, setup_drawing= False, zorder= zorder+1,\n",
    "                              path_effects=[patheffects.withStroke(foreground=color, linewidth= linewidth)],\n",
    "                              draw_normal= draw_normal, **kargs)\n",
    "        \n",
    "if mogi().InternalDipVariationAnomalyMultiParts in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().InternalDipVariationAnomalyMultiParts]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().InternalDipVariationAnomalyMultiParts,\n",
    "                                                 draw_internal_dip_variation_anomaly_multi_parts_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_discontinuous_stratigraphy_anomaly_in_AxisAlignedCrossSection(\n",
    "        anomaly,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        draw_normal = False,\n",
    "        color = \"r\",\n",
    "        linewidth= 5,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "    \n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    representation_space.draw(anomaly.is_Related_To[0], ax = ax, setup_drawing= False, zorder= 200,\n",
    "                              path_effects=[patheffects.withStroke(foreground=color, linewidth= linewidth)], \n",
    "                              draw_normal= draw_normal, **remove_kargs(\"zorder\",**kargs))\n",
    "    representation_space.draw(anomaly.is_Related_To[1], ax = ax, setup_drawing= False, zorder= 200,\n",
    "                              path_effects=[patheffects.withStroke(foreground=color, linewidth= linewidth)], \n",
    "                              draw_normal= draw_normal,  **remove_kargs(\"zorder\",**kargs))\n",
    "    representation_space.draw_relation(*anomaly.is_Related_To, color= color, alpha=0.8, linestyle= \"dashed\", ax = ax, setup_drawing= False,\n",
    "                                       zorder= 201,  **remove_kargs(\"zorder\",**kargs))\n",
    "        \n",
    "if mogi().DiscontinuousStratigraphyAnomaly in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().DiscontinuousStratigraphyAnomaly]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().DiscontinuousStratigraphyAnomaly, draw_discontinuous_stratigraphy_anomaly_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_polarity_anomaly_in_AxisAlignedCrossSection(\n",
    "        anomaly,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        draw_normal= False, \n",
    "        color = \"r\",\n",
    "        linewidth= 5,\n",
    "        zorder= 200,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "    \n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    representation_space.draw(anomaly.is_Related_To[0], ax = ax, setup_drawing= False, zorder= zorder+1,\n",
    "                              path_effects=[patheffects.withStroke(foreground=color, linewidth= linewidth)],  \n",
    "                              draw_normal= draw_normal, **kargs)\n",
    "    representation_space.draw(anomaly.is_Related_To[1], ax = ax, setup_drawing= False, zorder= zorder+1,\n",
    "                              path_effects=[patheffects.withStroke(foreground=color, linewidth= linewidth)],  \n",
    "                              draw_normal= draw_normal, **kargs)\n",
    "    representation_space.draw_relation(*anomaly.is_Related_To, color= color, alpha=0.8, linestyle= \"dashed\", ax = ax, setup_drawing= False,\n",
    "                                       zorder= zorder,  **kargs)\n",
    "        \n",
    "if mogi().PolarityAnomaly in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().PolarityAnomaly]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().PolarityAnomaly, draw_polarity_anomaly_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_explanation_anomaly_in_AxisAlignedCrossSection(\n",
    "        anomaly,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        color = \"r\",\n",
    "        linewidth= 5,\n",
    "        zorder= 200,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "    \n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    representation_space.draw(anomaly.is_Related_To_Explaining_Object[0], ax = ax, setup_drawing= False, zorder= zorder+1,\n",
    "                              path_effects=[patheffects.withStroke(foreground=color, linewidth= linewidth)],  \n",
    "                              **kargs)\n",
    "    \n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    explaining_rep = anomaly.is_Related_To_Explaining_Object[0].has_Representation\n",
    "    if (explaining_rep is None) or (len(explaining_rep) == 0):\n",
    "        raise MalissiaBaseError(\"Trying to draw an explanation error with an object that has no representation.\")\n",
    "    \n",
    "    explaining_rep = explaining_rep[0]\n",
    "    explained_object = anomaly.is_Related_To_Explained_Object[0]\n",
    "    if knowledge_framework.isinstance(explaining_rep, knowledge_framework().Planar_Surface):\n",
    "        # compute projection point\n",
    "        projection = representation_space.space.compute_projection_on_plane(plane= explaining_rep, object= explained_object)\n",
    "    else:\n",
    "        raise MalissiaNotImplementedYet(\"Projection on something that is not implemented\")\n",
    "                \n",
    "    # draw explanation link\n",
    "    representation_space.draw_explanation(explained_object= explained_object, projection= projection,\n",
    "                                        color= color, highlight_explained_object= True, highlight_linewidth= linewidth)\n",
    "        \n",
    "if mogi().ExplanationAnomaly in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().ExplanationAnomaly]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().ExplanationAnomaly, draw_explanation_anomaly_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_complex_surface_planar_anomaly(\n",
    "        anomaly,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        color = \"r\",\n",
    "        linewidth= 5,\n",
    "        zorder= 200,\n",
    "        draw_normal  = False, \n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "\n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    for sur in anomaly.is_Related_To :\n",
    "        if isinstance(sur, knowledge_framework().Planar_Surface):\n",
    "            print('warrning {} is not of type planar_surface'.format(sur.name))\n",
    "        representation_space.draw(sur, ax = ax, setup_drawing= False, zorder= zorder+1,\n",
    "                                            path_effects=[patheffects.withStroke(foreground=color, linewidth= linewidth)],  \n",
    "                                            draw_normal= draw_normal, **kargs)\n",
    "        \n",
    "    representation_space.draw_relation(*anomaly.is_Related_To, color= color, alpha=0.8, linestyle= \"dashed\", ax = ax, setup_drawing= False,\n",
    "                                       zorder= zorder,  **kargs)\n",
    "        \n",
    "if mogi().ComplexSurfacePlanarAnomaly in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().ComplexSurfacePlanarAnomaly]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().ComplexSurfacePlanarAnomaly, draw_complex_surface_planar_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_complex_surface_nb_neighbors_anomaly(\n",
    "        anomaly,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        color = \"r\",\n",
    "        linewidth= 5,\n",
    "        zorder= 200,\n",
    "        draw_normal  = False, \n",
    "        **kargs\n",
    "    ):\n",
    "    \n",
    "    \n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "\n",
    "    # remove any existing path effect for it to be overriden\n",
    "    if \"path_effects\" in kargs:\n",
    "        del kargs[\"path_effects\"]\n",
    "    \n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "\n",
    "    if knowledge_framework().Complex_Surface in anomaly.is_Related_To[0].is_a:\n",
    "        \n",
    "        representation_space.draw_multiple_planes(anomaly.is_Related_To[0].has_Part, \n",
    "                                                  path_effects=[patheffects.withStroke(foreground= color, linewidth= linewidth)],\n",
    "                              ax = ax, setup_drawing= False, zorder= zorder,\n",
    "                              draw_normal= draw_normal, **kargs)\n",
    "            \n",
    "    \n",
    "        \n",
    "if mogi().ComplexSurfaceNeighborNumberAnomaly in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().ComplexSurfaceNeighborNumberAnomaly]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().ComplexSurfaceNeighborNumberAnomaly, draw_complex_surface_nb_neighbors_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_shared_sides_not_parallel_anomaly_in_AxisAlignedCrossSection(\n",
    "        anomaly,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        color = \"red\", \n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "    if len(anomaly.is_Related_To) < 1 :\n",
    "        print('Warning: anomaly {} is not related to any complex_surface'.format(anomaly.name))\n",
    "        return None\n",
    "    \n",
    "    complex_surface = anomaly.is_Related_To[0]\n",
    "    if len(complex_surface.has_Part) < 2:\n",
    "        print('Warning: complex_surface {} related to the anomaly {} deos not have enough parts '.format(complex_surface.name, anomaly.name))\n",
    "        return None\n",
    "    space = representation_space.space \n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "\n",
    "    share_sides = get_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, space)\n",
    "    unmatched_sides = get_unmatched_shared_sides_in_one_complex_surface(complex_surface, knowledge_framework, space)\n",
    "    sides = unmatched_sides+share_sides\n",
    "    start_node_all_sides = {}\n",
    "    sizes = []\n",
    "    for side_i in sides:\n",
    "        start_i = side_i.has_End_Points[0]\n",
    "        representation_space.draw_point(data= start_i, marker=\"o\", \n",
    "                                        color = color, ax = ax  )\n",
    "        \n",
    "if mogi().SharedSideNotParallelAnomaly in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().SharedSideNotParallelAnomaly]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().SharedSideNotParallelAnomaly, draw_shared_sides_not_parallel_anomaly_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation process in itself is run in a **GeologicalInterpretationProcess** and follow a very simple and generic algorithm.<br>\n",
    "This algorithm implements a Deming wheel process of continual improvement:\n",
    "1. Plan:\n",
    "    1. Select a situation\n",
    "    2. Select an action\n",
    "2. Do: Implement the action (e.g., CreateInterpretationElement)\n",
    "    1. List features\n",
    "    2. Identify possible explanations\n",
    "    3. Rank/chose explanations\n",
    "    4. Instanciate individuals\n",
    "    5. Infer and set parameters\n",
    "3. Check: Evaluate consistency\n",
    "    1. Evaluate internal consistency\n",
    "    2. Evaluate relational likelihood\n",
    "    3. Evaluate feature explanation\n",
    "4. Act: Generate anomalies and report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "from enum import Enum, Flag, auto\n",
    "class TerminationFlag(Flag):\n",
    "    \"\"\"Defines flags for specifying why the `GeologicalInterpretationProcess` stoped\n",
    "    \n",
    "    NOTSTARTED: it has not been started yet\n",
    "    PROCESSING: it is currently processing\n",
    "    USER: was terminated by user\n",
    "    MAX_ITER: maximum iteration number reached\n",
    "    N_ITER: maximum number of iterations for this run reached\n",
    "    \"\"\"\n",
    "    NOTSTARTED = 0\n",
    "    PROCESSING = auto()\n",
    "    USER = auto()\n",
    "    MAX_ITER = auto()\n",
    "    N_ITER = auto()\n",
    "   \n",
    "class AbortIterationFlag(Flag):\n",
    "   \"\"\"Flags to declare reasons for aborting interpretation iteration\n",
    "   \n",
    "   UNDEFINED_MOVE: the key for the move is not defined or the action is not implemented\n",
    "   NO_POSSIBLE_EXPLANATION: the situation has no possible explanation\n",
    "   \"\"\"\n",
    "   UNDEFINED_MOVE = auto()\n",
    "   NO_POSSIBLE_EXPLANATION = auto()\n",
    "   UNDEFINED_CONSTRUCTOR= auto()\n",
    "   \n",
    "class InterpretationIteration(object):\n",
    "   \"\"\"Class to handle informations about each iteration\n",
    "   \n",
    "   Iterations are stored in the history of the interpretation process.\n",
    "   \n",
    "   Attributes:\n",
    "   - epoch: the id of the corresponding epoch (iteration)\n",
    "   - situation: the situation selected at this iteration\n",
    "   - move: the move to be implemented during this iteration\n",
    "   - result: the result of the given iteration\"\"\"\n",
    "   \n",
    "   def __init__(self, epoch):\n",
    "      self.epoch= epoch\n",
    "      self.situation= None\n",
    "      self.interpretation_move = None\n",
    "      \n",
    "   def __str__(self):\n",
    "        return \"\\n\".join([str(k)+\": \"+str(val) for k, val in self.__dict__.items()])\n",
    "\n",
    "def select_user_unexplained_observation(process):\n",
    "   \"\"\"user selects a single feature to be explained\"\"\"\n",
    "   observations = process.dataset.get_unexplained_observations()\n",
    "   message = \"; \".join([\"{}: {}\".format(i,obs_i.name) for i, obs_i in enumerate(observations)])\n",
    "   selected_feature_index = int(input(message))\n",
    "   selected_feature = observations[selected_feature_index]\n",
    "   process.iteration.situation = InterpretationSituation( features= [selected_feature], process= process)\n",
    "   \n",
    "def select_user_unexplained_observations(process):\n",
    "   \"\"\"user selects several observations to be explained\"\"\"\n",
    "   observations = process.dataset.get_unexplained_observations()\n",
    "   message = \"; \".join([\"{}: {}\".format(i,obs_i.name) for i, obs_i in enumerate(observations)])\n",
    "   number_of_observations = int(input('enter the desired number of observations to be selected'))\n",
    "   if number_of_observations > len(observations):\n",
    "      raise MalissiaBaseError('the chosen number of observations is greater than the number of existing observations')\n",
    "   features = [] \n",
    "   for nb in range(number_of_observations):\n",
    "      selected_feature_index = int(input(message))\n",
    "      selected_feature = observations[selected_feature_index]\n",
    "      features.append(selected_feature)\n",
    "   process.iteration.situation = InterpretationSituation( features= features, process= process)\n",
    "\n",
    "def select_single_random_unexplained_observation(process):\n",
    "   \"\"\"randomly selects a single feature to be explained\"\"\"\n",
    "   observations = process.dataset.get_unexplained_observations()\n",
    "   selected_feature = random.choice(observations) if len(observations) > 0 else []\n",
    "   process.iteration.situation = InterpretationSituation( features= [selected_feature], process= process)\n",
    "   \n",
    "def select_random_unexplained_observations(process, n_min= 2, n_max= 6):\n",
    "   \"\"\"randomly selects a given number of features to be explained\"\"\"\n",
    "   observations = process.dataset.get_unexplained_observations()\n",
    "   if len(observations) == 0:\n",
    "      return []\n",
    "   \n",
    "   n_max = min(n_max, len(observations))\n",
    "   n_min = min(n_min, len(observations))\n",
    "   n = random.randint(n_min,n_max)\n",
    "   selected_feature = random.sample(observations, k= n)\n",
    "   process.iteration.situation = InterpretationSituation( features= selected_feature, process= process)\n",
    "\n",
    "def select_user_unexplained_feature(process):\n",
    "   \"\"\"user selects a single feature to be explained\"\"\"\n",
    "   features = process.get_unexplained_features()\n",
    "   message = \"; \".join([\"{}: {}\".format(i,obs_i.name) for i, obs_i in enumerate(features)])\n",
    "   selected_feature_index = int(input(message))\n",
    "   selected_feature = features[selected_feature_index]\n",
    "   process.iteration.situation = InterpretationSituation( features= [selected_feature], process= process)\n",
    "\n",
    "def select_user_unexplained_features(process):\n",
    "   \"\"\"user selects a single feature to be explained\"\"\"\n",
    "   features = process.get_unexplained_features()\n",
    "   message = \"; \".join([\"{}: {}\".format(i,obs_i.name) for i, obs_i in enumerate(features)])\n",
    "   number_of_features = int(input('enter the desired number of features to be selected'))\n",
    "   if number_of_features > len(features):\n",
    "      raise MalissiaBaseError('the chosen number of features is greater than the number of existing unexplained_features')\n",
    "   features_to_be_returned = [] \n",
    "   for nb in range(number_of_features):\n",
    "      selected_feature_index = int(input(message))\n",
    "      selected_feature = features[selected_feature_index]\n",
    "      features_to_be_returned.append(selected_feature)\n",
    "   process.iteration.situation = InterpretationSituation( features= features_to_be_returned, process= process)\n",
    "\n",
    "def select_single_random_unexplained_feature(process):\n",
    "   \"\"\"randomly selects a single feature to be explained\"\"\"\n",
    "   process.iteration.situation = select_random_unexplained_features(process= process, n_max= 1)\n",
    "   \n",
    "def select_random_unexplained_features(process, n_min= 2, n_max= 6):\n",
    "   \"\"\"randomly selects a given number of features to be explained\"\"\"\n",
    "   features = process.get_unexplained_features()\n",
    "   if len(features) == 0:\n",
    "      return []\n",
    "   \n",
    "   n_max = min(n_max, len(features))\n",
    "   n_min = min(n_min, len(features))\n",
    "   n = random.randint(n_min,n_max)\n",
    "   selected_feature = random.sample(features, k= n)\n",
    "   process.iteration.situation = InterpretationSituation( features= selected_feature, process= process)\n",
    "\n",
    "class GeologicalInterpretationProcess(object):\n",
    "   \"\"\"GeologicalInterpretationProcess implements the core process of a geological intepretation.\n",
    "    \n",
    "    It connects all the required elements and resulting artefacts relatively to a given interpretation sequence:\n",
    "     - knowledge_framework: a GeologicalKnowledgeFramework\n",
    "     - dataset: a GeologicalDataSet that interfaces all the available data\n",
    "     - representation_space: a `RepresentationSpace` defining the study zone\n",
    "     - strategies: a dict that associates algorithm stages with preferred options\"\"\"\n",
    "   \n",
    "   default_strategies = {\n",
    "      \"SituationSelection\": select_random_unexplained_features\n",
    "   }\n",
    "     \n",
    "   def __init__(self,\n",
    "                dataset: GeologicalDataset,\n",
    "                representation_space: RepresentationSpace = None,\n",
    "                knowledge_framework= None,\n",
    "                strategies = {}):\n",
    "      \"\"\"Creates a GeologicalInterpretationProcess\n",
    "        \n",
    "      ---------------------------\n",
    "      Parameters:\n",
    "       - dataset (GeologicalDataset): a dataset to be explained by this interpretor\n",
    "       - representation_space (RepresentationSpace): the representation space that must be explainined by the interpretation.\n",
    "       If None (default), the physical space of the dataset is used instead.\n",
    "       - knowledge_framework: a GeologicalKnowledgeFramework that defines the concepts used for this interpretation.\n",
    "         If None is given, the the default knowledge framework is used (`GeologicalKnowledgeManager().get_knowledge_framework()`)\n",
    "       - strategies: a dict that associates algorithm stages with preferred options\n",
    "      \"\"\"\n",
    "      self.dataset = dataset\n",
    "      self.interpreted_objects = list()\n",
    "      self.anomalies = list()\n",
    "      self.representation_space = representation_space if representation_space is not None else self.dataset.physical_space\n",
    "      self.knowledge_framework= GeologicalKnowledgeManager().get_knowledge_framework() if knowledge_framework is None else knowledge_framework\n",
    "      \n",
    "      self.strategies = {**GeologicalInterpretationProcess.default_strategies, **strategies}\n",
    "      \n",
    "      self.update_status(init= True)\n",
    "         \n",
    "      self.interpretation_moves ={\n",
    "         \"NewInterpretation\": self.apply_new_interpretation,\n",
    "         \"UpdateInterpretation\": self.apply_update_interpretation,\n",
    "         \"RemoveInterpretation\": self.apply_remove_interpretation\n",
    "      }\n",
    "      \n",
    "   def __init_status(self, init):\n",
    "      \"\"\"creates the status on first run or reset\n",
    "      \n",
    "      This is setting the:\n",
    "      - status: to store information on the interpretation current progress\n",
    "      - report: to pass on information to the next iteration\n",
    "        - abort_iteration: signal to abort the iteration\n",
    "        - abort_reason: a keyword to explain the reason for aborting\n",
    "      - history: to store the past iterations \n",
    "      \"\"\"\n",
    "      if not hasattr(self, \"status\"):\n",
    "         self.status = {}\n",
    "         init = True\n",
    "      if init:\n",
    "         self.status[\"epoch\"] = None # index of the current iteration\n",
    "         self.iteration = None\n",
    "         self.report = {}\n",
    "         self.history = []\n",
    "         self.interpreted_objects = []\n",
    "         self.anomalies = []\n",
    "      \n",
    "   def update_status(self, init= False):\n",
    "      \"\"\"Performs status update operations\n",
    "      \n",
    "      If the status is not set yet, this method will also create it and initialise it.\n",
    "      \n",
    "      Parameters:\n",
    "      - init (Bool): if True, the status will be reset to initial values. Default is False.\"\"\"\n",
    "      self.__init_status(init)\n",
    "            \n",
    "      # updating indicators\n",
    "      self.status[\"coverage\"] = self.evaluate_coverage() # ratio of representation space covered by an explanation\n",
    "      self.status[\"data_explanation_ratio\"] = self.evaluate_data() # ratio of explained observations\n",
    "      self.status[\"anomaly_explanation_ratio\"] =  self.evaluate_anomalies() # ratio of explained observations\n",
    "      \n",
    "      # setting termination status\n",
    "      if init:\n",
    "         self.status[\"termination_criterion\"] = TerminationFlag.NOTSTARTED # gives explanation about why it terminated\n",
    "      else:\n",
    "         self.status[\"termination_criterion\"] = TerminationFlag.PROCESSING # gives explanation about why it terminated\n",
    "      \n",
    "   def register_interpreted_object(self, object, **kargs):\n",
    "      \"\"\"Registers an interpreted object\"\"\"\n",
    "      self.interpreted_objects.append(object)\n",
    "      self.representation_space.attach_interpreted_object(object, **kargs)\n",
    "      \n",
    "   \n",
    "   def get_unexplained_features(self):\n",
    "        \"\"\"Accessor filtering out explained observation and interpreted_objects\n",
    "        \"\"\"\n",
    "        unexplained_observations = [obs_i for obs_i in self.dataset.get_observations()\n",
    "                                          if (obs_i.is_Explained_By is not None) and len(obs_i.is_Explained_By) == 0]\n",
    "        unexplained_interpreted_objects = [obj_i for obj_i in self.interpreted_objects\n",
    "                                          if (obj_i.is_Explained_By is not None) and (len(obj_i.is_Explained_By) == 0)]\n",
    "        unexplained_anomalies = [obj_i for obj_i in self.anomalies\n",
    "                                          if (obj_i.is_Explained_By is not None) and (len(obj_i.is_Explained_By) == 0)]\n",
    "        return unexplained_observations + unexplained_interpreted_objects + unexplained_anomalies\n",
    "     \n",
    "   def evaluate_coverage(self):\n",
    "      \"\"\"Evaluates the amount of physical space that is explained\n",
    "      \n",
    "      Returns:\n",
    "      - a float between 0 and 1, representing the ratio of covered space\"\"\"\n",
    "      return self.representation_space.evaluate_coverage(self.interpreted_objects)\n",
    "      \n",
    "   def evaluate_data(self):\n",
    "      \"\"\"Evaluates the amount of data that is explained\n",
    "      \n",
    "      Returns:\n",
    "      - a float between 0 and 1, representing the ratio of explained data\"\"\"\n",
    "      return 0\n",
    "   \n",
    "   def evaluate_anomalies(self):\n",
    "      \"\"\"Evaluates the amount of raised anomalies that are now explained\n",
    "      \n",
    "      Returns:\n",
    "      - a float between 0 and 1, representing the ratio of explained anomalies\"\"\"\n",
    "      return 0\n",
    "   \n",
    "   def __str__(self):\n",
    "      \"\"\"Return a report describing the interpretation process\"\"\"\n",
    "      desc = [\"Geological Interpretation Process:\"]\n",
    "      if self.dataset is None:\n",
    "         desc+= [\"|- Dataset: None\"]\n",
    "      else:\n",
    "         desc+= [\"|- Dataset: \"+\"\\n| |\".join(self.dataset.__str__().split(\"\\n\"))]\n",
    "         \n",
    "      if self.knowledge_framework is None:\n",
    "         desc+= [\"|- Geological Knowledge Framework: None\"]\n",
    "      else:\n",
    "         desc+= [\"|- \"+\"\\n| |\".join(self.knowledge_framework.__str__().split(\"\\n\"))]\n",
    "         \n",
    "      if self.representation_space is None:\n",
    "         desc+= [\"|- No representation space defined, this is an abstract interpretation only.\"]\n",
    "      else:\n",
    "         desc+= [\"|- \"+\"\\n| |\".join(self.representation_space.__str__().split(\"\\n\"))]\n",
    "         \n",
    "      return \"\\n\".join(desc)\n",
    "   \n",
    "   def run(self, max_iter= None, n_iter= None):\n",
    "      \"\"\"Runs the iterative interpretation process\n",
    "      \n",
    "      Parameters:\n",
    "      - max_iter (int): if set, it specifies the maximum number of iterations to run before terminating.\n",
    "      - n_iter (int): if set, specifies to run for another \"n_iter\" number of iteration.\n",
    "      Compared to max_iter, this is ignoring the number of already passed iterations.\n",
    "      Iteration (epoch) are numbered starting at 0, this way the counter also represents the number of passed iterations.\n",
    "      \"\"\"\n",
    "      \n",
    "      # run the iterative process as long as a termination criterion is not reached\n",
    "      \n",
    "      #first update the status to make sure it is up to date, and reset the termination criteria\n",
    "      self.update_status()\n",
    "      \n",
    "      # new epoch increments the iteration count and check termination\n",
    "      try:\n",
    "         self.status[\"current_run_starting_epoch\"] = self.status[\"epoch\"]\n",
    "         while self.new_epoch(max_iter = max_iter, n_iter = n_iter):\n",
    "            \n",
    "            # 1. Plan: Select a situation & an action\n",
    "            self.plan()\n",
    "            \n",
    "            # 2. Do: execute the specified action\n",
    "            if not self.report[\"abort_iteration\"]:\n",
    "               self.do()\n",
    "            \n",
    "            # 3. Check\n",
    "            # -----------------------------\n",
    "            #  3.1 Evaluate internal consistency\n",
    "            #  3.2 Evaluate relational likelihood\n",
    "            #  3.3 Evaluate feature explanation\n",
    "            if not self.report[\"abort_iteration\"]:\n",
    "               self.check()\n",
    "            \n",
    "            # 4. Act\n",
    "            # -----------------------------\n",
    "            # 4.1 Generate anomalies and report\n",
    "            # 4.2 update status\n",
    "            if not self.report[\"abort_iteration\"]:\n",
    "               self.act()\n",
    "            \n",
    "      except KeyboardInterrupt:\n",
    "         self.status[\"termination_criterion\"] = TerminationFlag.USER\n",
    "         \n",
    "      return self.status[\"termination_criterion\"]\n",
    "         \n",
    "   def new_epoch(self, max_iter= None, n_iter= None):\n",
    "      \"\"\"Starts a new epoch (iteration) unless termination criteria were reached\n",
    "      \n",
    "      Returns:\n",
    "      - True if the new epoch should start, False if iterations should stop\n",
    "      - max_iter (int): if set, it specifies the maximum number of iterations to run before terminating\"\"\"\n",
    "      \n",
    "      # Increment epoch number\n",
    "      self.increment_epoch()\n",
    "      \n",
    "      if (max_iter is not None) and (self.status[\"epoch\"] >= max_iter):\n",
    "         self.status[\"termination_criterion\"] = TerminationFlag.MAX_ITER\n",
    "         return False\n",
    "      \n",
    "      if (n_iter is not None) and (self.status[\"epoch\"] - self.status[\"current_run_starting_epoch\"] >= n_iter):\n",
    "         self.status[\"termination_criterion\"] = TerminationFlag.N_ITER\n",
    "         return False\n",
    "      \n",
    "      # Create a new iteration\n",
    "      self.iteration = InterpretationIteration(epoch= self.status[\"epoch\"])\n",
    "            \n",
    "      self.report[\"abort_iteration\"] = False\n",
    "      self.report[\"abort_reason\"] = None\n",
    "      \n",
    "      return True\n",
    "      \n",
    "   def increment_epoch(self):\n",
    "      \"\"\"Initialize or increment the epoch count\"\"\"\n",
    "      if self.status[\"epoch\"] is None:\n",
    "         self.status[\"epoch\"] = 0\n",
    "         self.status[\"current_run_starting_epoch\"] = self.status[\"epoch\"] \n",
    "      else:\n",
    "         self.status[\"epoch\"] += 1\n",
    "      \n",
    "   def select_strategy(self, key):\n",
    "      \"\"\"Select a the implementation to be used based on strategy\"\"\"\n",
    "      if key not in self.strategies:\n",
    "         raise MalissiaBaseError(\"The task has no defined strategy: \" + key)\n",
    "      strat = self.strategies[key]\n",
    "      if isinstance(strat,list):\n",
    "         strat = random.choice(strat)\n",
    "      return strat\n",
    "   \n",
    "   def plan(self, move_key = None, random_move = False ):\n",
    "      \"\"\"Perform the Plan part of the algorithm\n",
    "      \n",
    "      1. Select a situation to be explained\n",
    "      2. Select a Type of move to be performed in the interpretation process\"\"\"\n",
    "      \n",
    "      # Select a Situation\n",
    "      strategy_for_selecting_situation = self.select_strategy(\"SituationSelection\")\n",
    "      strategy_for_selecting_situation(self)\n",
    "      \n",
    "      # Select interpretation move (see self.interpretation_moves)\n",
    "      self.select_interpretation_move(key= move_key, random= random_move)\n",
    "      \n",
    "   def select_interpretation_move(self, key = None, random = False):\n",
    "      \"\"\"Select the type of interpretation action to be taken.\n",
    "      \n",
    "      Parameters:\n",
    "      - key: the key corresponding to the chosen action, see `interpretation_moves`.\n",
    "      This is to be used only for forcing the choice. Default, None.\n",
    "      - random: if True, will be chosen among the available options (default: False).\n",
    "      \"\"\"\n",
    "      \n",
    "      # random of user choice\n",
    "      if key is None and random:\n",
    "         key = random.choice( self.interpretation_moves.keys() )\n",
    "         \n",
    "      if key is not None:\n",
    "         self.iteration.interpretation_move = key\n",
    "      else:\n",
    "         # default move is creating an new interpretation\n",
    "         # Todo: this could be defined by a strategy\n",
    "         self.iteration.interpretation_move = \"NewInterpretation\"\n",
    "         # when applicable do others\n",
    "         # when existing features show anomalies, try and update\n",
    "         #if exisitng explanation anomalies:\n",
    "         #   if keep_trying:\n",
    "         #     self.iteration.interpretation_move = \"UpdateInterpretation\"\n",
    "         #   else:\n",
    "         #      self.iteration.interpretation_move = \"RemoveInterpretation\"\n",
    "      return self.interpretation_moves[self.iteration.interpretation_move]\n",
    "   \n",
    "   def do(self, **kargs):\n",
    "      \"\"\"Applies the selected move in the current context\"\"\"\n",
    "      if self.iteration.interpretation_move not in self.interpretation_moves:\n",
    "         print(\"The move {} is not implemented.\".format(self.iteration.interpretation_move))\n",
    "         self.report[\"abort_iteration\"] = True\n",
    "         self.report[\"abort_reason\"] = AbortIterationFlag.UNDEFINED_MOVE\n",
    "         return\n",
    "      \n",
    "      move_method = self.interpretation_moves[self.iteration.interpretation_move]\n",
    "      move_method(**kargs)\n",
    "      \n",
    "      if (\"sync_reasoner\" in kargs) and (kargs[\"sync_reasoner\"]):\n",
    "         self.knowledge_framework.sync_reasoner()\n",
    "   \n",
    "   def apply_new_interpretation(self, random_choice= False, force_explanation= None, force_constructor= None, debug= False):\n",
    "      \"\"\"Implements the creation of a new interpretation object\"\"\"\n",
    "      \n",
    "      situation = self.iteration.situation\n",
    "      if situation is None:\n",
    "         raise MalissiaBaseError(\"Something needs to be selected for interpretation. Here the situtation is None.\")\n",
    "      \n",
    "      if len(situation.features) == 0:\n",
    "         raise MalissiaBaseError(\"Not implemented yet: trying to interprete without selected features.\")\n",
    "      \n",
    "      # if force explanation, use it, else try and find possible explanations\n",
    "      if force_explanation is not None:\n",
    "         situation.candidate_explanation_class = force_explanation\n",
    "      else:\n",
    "         # get a list of possibly explaining concepts for the features to be explained\n",
    "         possible_explanations = [concept\n",
    "                              for feature_i in situation.features\n",
    "                              for concept in self.knowledge_framework.get_possible_interpretations_of(feature_i)]\n",
    "         \n",
    "         # check if there are candidate explanations\n",
    "         if len(possible_explanations) == 0:\n",
    "            self.report[\"abort_iteration\"] = True\n",
    "            self.report[\"abort_reason\"] = AbortIterationFlag.NO_POSSIBLE_EXPLANATION\n",
    "            return\n",
    "         \n",
    "         # select a possible explanation\n",
    "         if random_choice:\n",
    "            # pick an explaining concept\n",
    "            situation.candidate_explanation_class = random.choice(possible_explanations)\n",
    "         else:\n",
    "            ## todo : sort existing concepts by preference\n",
    "            #possible_explanations = possible_explanations\n",
    "            situation.candidate_explanation_class = possible_explanations[0]\n",
    "            \n",
    "      # check for existing instances of this concept in the selected context   \n",
    "      if len(situation.context) > 0:\n",
    "         existing_explaining_instances = [context_object\n",
    "                              for context_object in situation.context\n",
    "                              if self.knowledge_framework.isinstance(context_object, situation.candidate_explanation_class)]\n",
    "      else:\n",
    "         existing_explaining_instances = []\n",
    "         \n",
    "      # if applicable use existing explaining instance else create a new one\n",
    "      if len(existing_explaining_instances) > 0:\n",
    "         situation.existing_explaining_object = True\n",
    "         situation.explaining_object = random.choice(existing_explaining_instances)\n",
    "      else:\n",
    "         constructor = force_constructor if force_constructor is not None else self.knowledge_framework.select_object_constructor(\n",
    "                              situation.candidate_explanation_class,\n",
    "                              interpretation_situation = situation,\n",
    "                              interpretation_status = self.status,\n",
    "                              random_choice = random_choice,\n",
    "                              debug= debug\n",
    "                              )\n",
    "         \n",
    "         if constructor is None:\n",
    "            self.report[\"abort_iteration\"] = True\n",
    "            self.report[\"abort_reason\"] = AbortIterationFlag.UNDEFINED_CONSTRUCTOR\n",
    "            return\n",
    "         else:\n",
    "            situation.explaining_object = constructor(\n",
    "                                                   interpretation_situation = situation,\n",
    "                                                   interpretation_status = self.status\n",
    "                                                   )\n",
    "            self.register_interpreted_object(situation.explaining_object)\n",
    "   \n",
    "   def apply_update_interpretation(self):\n",
    "      \"\"\"Implements the update of an existing interpretation object\"\"\"\n",
    "      pass\n",
    "   \n",
    "   def apply_remove_interpretation(self):\n",
    "      \"\"\"Implements the removal of an existing interpretation object\"\"\"\n",
    "      self.knowledge_framework.remove_all_instances(self.iteration.situation.features)\n",
    "       \n",
    "   \n",
    "   def check(self, **karg):\n",
    "      \"\"\"Check the consistency of the interpretation step\"\"\"\n",
    "      \n",
    "      candidate_object = self.iteration.situation.explaining_object\n",
    "      if candidate_object is None:\n",
    "         raise MalissiaBaseError(\"resulting object is None.\\nIteration state: {}\".format(self.iteration))\n",
    "      \n",
    "      # check internal consistency of the interpreted object\n",
    "      internal_anomaly = self.knowledge_framework.evaluate_internal_consistency(candidate_object, physical_space= self.representation_space)\n",
    "      self.iteration.situation.anomalies += internal_anomaly\n",
    "      self.anomalies += internal_anomaly\n",
    "      \n",
    "      # check the consistency between the interpreted object and what it is explaining\n",
    "      explanation_anomaly = self.knowledge_framework.evaluate_explanation_consistency(candidate_object, physical_space= self.representation_space)\n",
    "      self.iteration.situation.anomalies += explanation_anomaly\n",
    "      self.anomalies += explanation_anomaly\n",
    "      \n",
    "      # check the consistency between the different interpreted objects\n",
    "      \n",
    "   def act(self, ** kargs):\n",
    "      \"\"\"Finalise an iteration\"\"\"   \n",
    "      self.update_status()\n",
    "      \n",
    "      # store finalised iteration in history\n",
    "      self.history += [self.iteration]\n",
    "      \n",
    "   def show(self, drawing_space, **kargs):\n",
    "      drawing_space.draw(self, **kargs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_interpretation_process_in_AxisAlignedCrossSection(\n",
    "        process: GeologicalInterpretationProcess,\n",
    "        drawing_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        draw_dataset= True,\n",
    "        draw_situation= True,\n",
    "        draw_interpretation= True,\n",
    "        draw_explanation= False,\n",
    "        draw_anomalies= False,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        drawing_space.setup_drawing(ax)\n",
    "    else:\n",
    "        drawing_space.setup_ax(ax)\n",
    "        \n",
    "    # draw dataset\n",
    "    if draw_dataset:\n",
    "        drawing_space.draw(process.dataset, setup_drawing= False, **kargs)\n",
    "    \n",
    "    # draw created objects\n",
    "    if draw_interpretation:\n",
    "        for feature_i in process.interpreted_objects:\n",
    "            drawing_space.draw(feature_i, color= \"gold\", setup_drawing= False, zorder= 40, draw_explanation= draw_explanation, **kargs)\n",
    "            \n",
    "    # draw anomalies\n",
    "    if draw_anomalies:\n",
    "        for anomaly in process.anomalies:\n",
    "            drawing_space.draw(anomaly, color= \"pink\", setup_drawing= False, zorder= 50, **kargs)\n",
    "        \n",
    "    # draw situation\n",
    "    if draw_situation:\n",
    "        if process.iteration is not None and process.iteration.situation is not None:\n",
    "            drawing_space.draw(process.iteration.situation, setup_drawing= False, **kargs)\n",
    "        \n",
    "\n",
    "if GeologicalInterpretationProcess in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[GeologicalInterpretationProcess]\n",
    "AxisAlignedCrossSection.register_drawing_method(GeologicalInterpretationProcess, draw_interpretation_process_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.remove_all_instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_from_csv(\"../inputs/sparse_data.csv\", sep=\";\", index= \"id\", coordinate_labels=[\"x\",\"y\",\"z\"], labels={\"ID\":\"id\", \"dip\": \"dip\", \"strike\":\"dip_dir\",\"name\":\"observed_object\"})\n",
    "#sp = PhysicalRepresentationSpace(coordinate_labels=[\"x\",\"y\",\"z\"])\n",
    "#sp.extension = [[0,230], [-40,-40], [-70,0]]\n",
    "#section1 = AxisAlignedCrossSection(sp, )\n",
    "section = AxisAlignedCrossSection(dataset.physical_space, )\n",
    "section.draw(dataset)\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Interpretation Process\n",
    "gip = GeologicalInterpretationProcess(dataset)\n",
    "sp = gip.representation_space\n",
    "gip.update_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan the iteration\n",
    "gip.new_epoch()\n",
    "print(\"Iteration:\",gip.iteration.epoch)\n",
    "gip.iteration.situation = InterpretationSituation(\n",
    "    features= [mogi().D4,mogi().D5], process= gip\n",
    ")\n",
    "gip.iteration.interpretation_move = \"NewInterpretation\"\n",
    "gip.show(section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do :  apply the move\n",
    "gip.do()\n",
    "gip.check()\n",
    "gip.act()\n",
    "gip.show(section)\n",
    "print(gip.iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan the iteration\n",
    "gip.new_epoch()\n",
    "print(\"Iteration:\",gip.iteration.epoch)\n",
    "gip.iteration.situation = InterpretationSituation(\n",
    "    features= [mogi().D3], process= gip\n",
    ")\n",
    "gip.iteration.interpretation_move = \"NewInterpretation\"\n",
    "gip.do()\n",
    "gip.check()\n",
    "gip.act()\n",
    "gip.show(section)\n",
    "print(gip.iteration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan the iteration\n",
    "gip.new_epoch()\n",
    "print(\"Iteration:\",gip.iteration.epoch)\n",
    "gip.iteration.situation = InterpretationSituation(\n",
    "    features= [gip.history[0].situation.explaining_object, gip.history[1].situation.explaining_object], process= gip\n",
    ")\n",
    "gip.iteration.interpretation_move = \"NewInterpretation\"\n",
    "gip.show(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.do()\n",
    "gip.check()\n",
    "gip.act()\n",
    "gip.show(section)\n",
    "print(gip.iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Interpreted Objects:\\n- \"+\"\\n- \".join([obj_i.name for obj_i in gip.interpreted_objects]))\n",
    "print(\"\\nAnomalies:\\n- \"+\"\\n- \".join([obj_i.name for obj_i in gip.anomalies]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan the iteration\n",
    "gip.new_epoch()\n",
    "print(\"Iteration:\",gip.iteration.epoch)\n",
    "gip.iteration.situation = InterpretationSituation(\n",
    "    features= [gip.history[-1].situation.anomalies[0], gip.history[-1].situation.explaining_object], process= gip\n",
    ")\n",
    "gip.iteration.interpretation_move = \"NewInterpretation\"\n",
    "gip.show(section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.do()\n",
    "gip.check()\n",
    "gip.act()\n",
    "gip.show(section)\n",
    "print(gip.iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Interpreted Objects:\\n- \"+\"\\n- \".join([obj_i.name for obj_i in gip.interpreted_objects]))\n",
    "print(\"\\nAnomalies:\\n- \"+\"\\n- \".join([obj_i.name for obj_i in gip.anomalies]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan the iteration\n",
    "gip.new_epoch()\n",
    "print(\"Iteration:\",gip.iteration.epoch)\n",
    "gip.iteration.situation = InterpretationSituation(\n",
    "    features= [mogi().D1, mogi().D2], process= gip\n",
    ")\n",
    "gip.iteration.interpretation_move = \"NewInterpretation\"\n",
    "gip.do()\n",
    "gip.check()\n",
    "gip.act()\n",
    "gip.show(section)\n",
    "print(gip.iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Interpreted Objects:\\n- \"+\"\\n- \".join([obj_i.name for obj_i in gip.interpreted_objects]))\n",
    "print(\"\\nAnomalies:\\n- \"+\"\\n- \".join([obj_i.name for obj_i in gip.anomalies]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan the iteration\n",
    "gip.new_epoch()\n",
    "print(\"Iteration:\",gip.iteration.epoch)\n",
    "gip.iteration.situation = InterpretationSituation(\n",
    "    features= [gip.history[-3].situation.explaining_object,gip.history[-1].situation.explaining_object], process= gip\n",
    ")\n",
    "gip.iteration.interpretation_move = \"NewInterpretation\"\n",
    "gip.do()\n",
    "gip.check()\n",
    "gip.act()\n",
    "gip.show(section)\n",
    "print(gip.iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan the iteration\n",
    "gip.new_epoch()\n",
    "print(\"Iteration:\",gip.iteration.epoch)\n",
    "gip.iteration.situation = InterpretationSituation(\n",
    "    features= [gip.history[-1].situation.anomalies[0], mogi().D1, mogi().D2], process= gip\n",
    ")\n",
    "gip.iteration.interpretation_move = \"NewInterpretation\"\n",
    "gip.do()\n",
    "gip.check()\n",
    "gip.act()\n",
    "gip.show(section)\n",
    "print(gip.iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section.draw(mogi().stratigraphic_part4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in mogi().stratigraphic_part3.has_Representation[0].has_Part:\n",
    "    print(p, p.has_Node0, p.has_Node1, p.has_Node2, p.has_Node3, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(mogi().stratigraphic_part4.has_Representation[0].has_Node0, \n",
    "      mogi().stratigraphic_part4.has_Representation[0].has_Node1,\n",
    "       mogi().stratigraphic_part4.has_Representation[0].has_Node2, \n",
    "        mogi().stratigraphic_part4.has_Representation[0].has_Node3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan the iteration\n",
    "gip.new_epoch()\n",
    "print(\"Iteration:\",gip.iteration.epoch)\n",
    "gip.iteration.situation = InterpretationSituation(\n",
    "    features= [mogi().stratigraphic_part4,mogi().stratigraphic_part3], process= gip\n",
    ")\n",
    "gip.iteration.interpretation_move = \"NewInterpretation\"\n",
    "gip.do()\n",
    "gip.check()\n",
    "gip.act()\n",
    "gip.show(section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stratigraphic_part7 = sp.get_object_coordinates(mogi().stratigraphic_part7, 'nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_stratigraphic_part7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stratigraphic_part6 = sp.get_object_coordinates(mogi().stratigraphic_part6, 'nodes')\n",
    "print(n_stratigraphic_part6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stratigraphic_part8 = [n_stratigraphic_part7[0], \n",
    "                         n_stratigraphic_part7[3],\n",
    "                         n_stratigraphic_part6[2],\n",
    "                         n_stratigraphic_part6[1]]\n",
    "print(n_stratigraphic_part8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_stratigraphic_part8 = create_nodes_from_coords(mogi, n_stratigraphic_part8,\n",
    "                                                      sp.coordinate_labels)\n",
    "plan_8 = create_planar_surface(mogi,nodes_stratigraphic_part8 )\n",
    "set_center(mogi, plan_8)\n",
    "set_attitude(mogi, plan_8)\n",
    "stratigraphic_part8 = create_surface_part(mogi, plan_8 )\n",
    "mogi.show_all_instance_qualities([stratigraphic_part8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stratigraphic_part6 = sp.get_object_coordinates(mogi().stratigraphic_part6, 'nodes')\n",
    "print(n_stratigraphic_part6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().stratigraphic_part5.has_Representation[0].has_Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_plan9 = sp.get_object_coordinates(mogi().part_nb3, 'nodes')\n",
    "print(n_plan9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stratigraphic_part9 = [n_stratigraphic_part6[0], \n",
    "                         n_stratigraphic_part6[3],\n",
    "                         n_plan9[3],\n",
    "                         n_plan9[2]]\n",
    "print(n_stratigraphic_part9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_stratigraphic_part9 = create_nodes_from_coords(mogi, n_stratigraphic_part9,\n",
    "                                                      sp.coordinate_labels)\n",
    "plan_9 = create_planar_surface(mogi,nodes_stratigraphic_part9 )\n",
    "set_center(mogi, plan_9)\n",
    "set_attitude(mogi, plan_9)\n",
    "stratigraphic_part9 = create_surface_part(mogi, plan_9 )\n",
    "mogi.show_all_instance_qualities([stratigraphic_part9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = [part for part in mogi().stratigraphic_part5.has_Representation[0].has_Part]\n",
    "for part_i in [mogi().stratigraphic_part9, mogi().stratigraphic_part8, \n",
    "            mogi().stratigraphic_part6, mogi().stratigraphic_part7]:\n",
    "    parts.append(part_i.has_Representation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cplx = mogi().Complex_Surface(has_Part = parts)\n",
    "final_part = create_surface_part(mogi, final_cplx)\n",
    "final_cplx.non_planar = True\n",
    "final_part.deformed = True\n",
    "neighbor_constructor_from_multiple_planars(final_cplx, mogi, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forging manually the surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_part1_coo = [[32., 0, -54.], [ 32.,   45, -54 ], [-2., 45., -25], [ -2.,   0, -25. ]]\n",
    "my_part2_coo = [[32., 45, -54.], [ 32.,   0, -54 ], [58., 0., -26], [ 58.,   45, -26. ]]\n",
    "my_part3_coo = [[58., 45., -26.], [ 58.,   0., -26 ], [93., 0., -54], [ 93.,   45, -54. ]]\n",
    "my_part4_coo = [[115., 0., -25.], [ 115.,   45., -25 ], [93., 45., -54], [ 93.,   0, -54. ]]\n",
    "my_part5_coo = [[115., 0., -25.], [ 155.,   0., -54 ], [155., 45., -54], [ 115.,   45, -25. ]]\n",
    "my_part6_coo = [[191., 0., -25.], [ 191.,   45., -25 ], [155., 45., -54], [ 155.,   0, -54. ]]\n",
    "my_part7_coo = [[191., 0., -25.], [ 213.,   0., -54 ], [213., 45., -54], [ 191.,   45, -25. ]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = get_physical_space_in_constructor(gip.iteration.situation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_manual_coords = [my_part1_coo, my_part2_coo, my_part3_coo, my_part4_coo\n",
    "                      , my_part5_coo, my_part6_coo, my_part7_coo]\n",
    "list_nodes_manu = list_coords_to_list_nodes(list_manual_coords, mogi, sp.coordinate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planars = list_of_planars_from_nodes(list_nodes_manu, mogi)\n",
    "my_parts = list_of_parts_from_nodes(planars, mogi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cplx = mogi().Complex_Surface(has_Part = planars)\n",
    "attribute_parts_nodes_to_complex_surface(final_cplx)\n",
    "compute_center_complex_surface(mogi, sp, final_cplx )\n",
    "final_part = create_surface_part(mogi, final_cplx)\n",
    "final_cplx.non_planar = True\n",
    "final_part.deformed = True\n",
    "neighbor_constructor_from_multiple_planars(final_cplx, mogi, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = internal_dip_variation_anomaly_multi_parts_constructor(mogi, final_part)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan the iteration\n",
    "gip.new_epoch()\n",
    "print(\"Iteration:\",gip.iteration.epoch)\n",
    "gip.iteration.situation = InterpretationSituation(\n",
    "    features= [gip.history[-1].situation.anomalies[0]], process= gip\n",
    ")\n",
    "gip.iteration.interpretation_move = \"NewInterpretation\"\n",
    "gip.do()\n",
    "gip.check()\n",
    "gip.act()\n",
    "gip.show(section)\n",
    "print(gip.iteration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.new_epoch()\n",
    "gip.show(section, draw_explanation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip = GeologicalInterpretationProcess(dataset= dataset)\n",
    "gip.update_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.run(n_iter=100)\n",
    "gip.show(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in gip.history:\n",
    "    print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ontology manipulation\n",
    "\n",
    "The knowledge manipulated in this package is formalised in an ontology,<br>\n",
    "which is store in a *.owl* file.\n",
    "\n",
    "It is named **MOGI** for **M**inimal **O**ntology for **G**eological **I**nterpretation\n",
    "\n",
    "To manipulated this ontology, we use the package **owlready2** available from here: https://owlready2.readthedocs.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ontology provides access to its components, e.g.:\n",
    "* classes\n",
    "* properties\n",
    "* individuals\n",
    "* rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(mogi().classes()))\n",
    "print(list(mogi().properties()))\n",
    "print(list(mogi().individuals()))\n",
    "print(list(mogi().rules()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specific elements can be searched through simple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(iri = \"*Surface*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(type= mogi().PointBased_Observation, qualities= [\"dip\",\"occurrence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(type= mogi().PointBased_Observation, qualities= {\"dip\":45})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(qualities= {\"dip\":45})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(name=\"o?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(name=\"o1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi._ontology_backend.Thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoner\n",
    "\n",
    "Ontologies are even more powerful thansk to their capabilities to use reasoning for infering types, properties, and relationships that were not explicitly stated.\n",
    "This is usefull for obtaining results implied by the already stated information.\n",
    "\n",
    "This is achieved by running a *reasoner* on the ontology as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing GeologicalKnowledgeManager and ontology manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### when the ontoology is attached to the knowledge manager, it is now referred to as mogi() and the roginal backend (owl or others) is now referred to as _ontology_backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "print(GeologicalKnowledgeManager().get_knowledge_framework())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our approach, geological datasets will be progressively interpreted in terms of structural objects,<br>\n",
    "based on a formal definition of concepts own by a **GeologicalKnowledgeManager**.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.sync_reasoner()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().Stratigraphic_Part.is_Possible_Explanation_Of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().Stratigraphic_Part.has_Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.get_objects_potentially_explained_by(mogi().Stratigraphic_Surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(type= mogi().PointBased_Observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Representation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(2)\n",
    "\n",
    "space.set_extension_from_data(padding= None)\n",
    "\n",
    "print(space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(3)\n",
    "space.compute_line_attitude_from_two_points([0,0,0],[1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [[0.,0,0],[-1,0,-1],[0,-1,0],[-1,-1,-1.3]]\n",
    "space= PhysicalRepresentationSpace(3)\n",
    "plane = space.compute_attitude_from_points(p)\n",
    "plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dip = 90\n",
    "dip_dir = 270\n",
    "space= PhysicalRepresentationSpace(3)\n",
    "space.compute_normal_from_dip_dir(dip, dip_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(3)\n",
    "space.compute_dip_dir_from_normal([-1,1,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(pv.PolyData(p))\n",
    "plotter.add_mesh(pv.PolyData(plane[\"center\"]), color=\"red\")\n",
    "\n",
    "plotter.add_arrows(plane[\"center\"],plane[\"major_axis\"], color= \"red\")\n",
    "plotter.add_arrows(plane[\"center\"],plane[\"minor_axis\"], color= \"green\")\n",
    "plotter.add_arrows(plane[\"center\"],plane[\"normal\"], color= \"blue\")\n",
    "plotter.show_axes()\n",
    "plotter.show(cpos='xz', window_size = (600,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = pv.Plotter()\n",
    "center = np.array([0,0,0])\n",
    "plotter.add_arrows(np.repeat([center],4,axis=0), np.array(p))\n",
    "plane = space.compute_principal_directions(p)\n",
    "average = space.compute_average_vector(p)\n",
    "plotter.add_arrows(center, average, color=\"Black\")\n",
    "\n",
    "#plotter.add_arrows(plane[\"center\"],plane[\"vectors\"][0], color= \"red\")\n",
    "#plotter.add_arrows(plane[\"center\"],plane[\"vectors\"][1], color= \"green\")\n",
    "#plotter.add_arrows(plane[\"center\"],plane[\"vectors\"][2], color= \"blue\")\n",
    "plotter.show_axes()\n",
    "plotter.show(cpos='xz', window_size = (600,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = PhysicalRepresentationSpace(coordinate_labels= \"depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space.compute_line_attitude_from_two_points([0,0,0],[-10,0,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: PhysicalRepresentationSpace()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(coordinate_labels=[\"X\",\"Y\"])\n",
    "print(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(coordinate_labels=[\"X\",\"Z\"])\n",
    "print(space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing DataSet & Manual entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = mogi().search(type=mogi().PointBased_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(obs) > 0 : \n",
    "    mogi.show_instance_qualities(obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= GeologicalDataset()\n",
    "dataset.remove_all_observations()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_observations(observation_type= mogi().PointBased_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_all_instance_qualities(dataset.get_observations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_observation_by_name(\"D8\")\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_all_observations()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "physical_space = PhysicalRepresentationSpace(coordinate_labels= [\"x\",\"y\",\"z\"])\n",
    "dataset.setup_representation_space(physical_space= physical_space)\n",
    "dataset.add_occurrence_observation(name=\"DD\", observed_object= \"Keuper\", x= 1, z= 2, y= 0, occurrence= True)\n",
    "dataset.add_occurrence_observation(name=\"DN\", observed_object= \"Keuper\",  x= 3, z= 1, y= 0)\n",
    "dataset.update_extension()\n",
    "print(dataset)\n",
    "\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_orientation_observation(name=\"DO\", observed_object= \"Trias\", dip= 30, dip_dir= 270, x= 2, z= 2, y= 0)\n",
    "print(dataset)\n",
    "\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_from_csv(\"../inputs/data_for_paper.csv\", sep=\";\", index= \"Id\", coordinate_labels=[\"x\",\"y\",\"z\"], labels={\"ID\":\"Id\", \"strike\":\"dip_dir\",\"name\":\"observed_object\"})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrates errors because of wrong separator\n",
    "try:\n",
    "    dataset2= load_dataset_from_csv(\"../inputs/data_for_paper.csv\", labels={\"ID\":\"Id\", \"strike\":\"dip_dir\",\"name\":\"observed_object\"})\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    gkf = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    observations = gkf.search(type=gkf().PointBased_Observation)\n",
    "    gkf.remove_all_instances(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrates errors when leaving wrong labels\n",
    "try:\n",
    "    dataset3 = load_dataset_from_csv(\"../inputs/data_for_paper.csv\", sep=\";\", index= \"ID\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    gkf = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    observations = gkf.search(type=gkf().PointBased_Observation)\n",
    "    gkf.remove_all_instances(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_head = np.array(['name', 'x', 'y', 'z', 'dip_dir', 'dip', 'observed_object','occurrence'])\n",
    "data_array = np.array([['D1', 15, 20, 35, 270, 45, 'Trias_Base',True],\n",
    "                       ['D2', 30, 25, 50, 270, 45, 'Trias_Base',True],\n",
    "                       ['D3', 60, 30, 40, 90, 45, 'Trias_Base',True],\n",
    "                       ['D4', 75, 15, 25, 90, 45, 'Trias_Base',True],\n",
    "                       ['D5', 110, 20, 40, 270, 63, 'Trias_Base',True],\n",
    "                       ['D6', 120, 20, 60, 270, 64, 'Trias_Base',True],\n",
    "                       ['D7', 155, 20, 60, 89, 39, 'Trias_Base',True],\n",
    "                       ['D8', 190, 20, 30, 91, 40, 'Trias_Base',True],\n",
    "                       ['D11', 25, 22, 45, np.nan, np.nan, None ,True],\n",
    "                       ['D22', 50, 22, 50, np.nan, np.nan, None,True],\n",
    "                       ['D44', 100, 30, 20, np.nan, np.nan, None,True],\n",
    "                       ['D77', 168, 30, 47, np.nan, np.nan, None,True]]\n",
    ")\n",
    "data_test = pd.DataFrame(data = data_array, columns = data_head)\n",
    "data_test = data_test.astype({'name':str, 'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'observed_object':str, 'occurrence':bool})\n",
    "data_test.set_index(\"name\", inplace = True)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_from_dataframe(data_test, coordinate_labels= [\"x\",\"y\",\"z\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "chart = HTML(dataset.info().replace(\"\\n\",\"<br>\"))\n",
    "display(chart)\n",
    "\n",
    "chart = HTML(dataset.to_dataframe().to_html())\n",
    "display(chart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing visualizations & drawings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_line([0,0],30, \"left\")\n",
    "draw_dip_symbol([0,1],60, \"right\", polarity= \"up\", color= \"red\" )\n",
    "plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(3)\n",
    "space.extension = [[-10,10], [-10,10], [-10,10]]\n",
    "section = AxisAlignedCrossSection(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#section.draw_dip_symbol(center=[1,1],dip=30, dip_dir=90, polarity= \"down\", length=3 )\n",
    "#section.draw_occurrence_symbol(center=[0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing drawing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes1 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-5, 0., -1.], name=\"N1_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-3., 0, 1.], name=\"N1_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-3, 3, 1], name=\"N1_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-5, 3, -1], name=\"N1_3\")\n",
    "    ]\n",
    "\n",
    "surf1 = constructor_surface_part_from_nodes(mogi,nodes1, name=\"surf_from_nodes1\")\n",
    "\n",
    "nodes2 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-3, 0., 1.], name=\"N2_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-1., 0., -1.], name=\"N2_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-1.0, 3.0, -1.0], name=\"N2_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-3, 3, 1], name=\"N2_3\")\n",
    "    ]\n",
    "\n",
    "surf2 = constructor_surface_part_from_nodes(mogi,nodes2, name=\"surf_from_nodes2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nodes3 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-1, -1., -1.], name=\"N3_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[1., -1, 1.], name=\"N3_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[1, 4.0, 1.0], name=\"N3_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-1., 4., -1.], name=\"N3_3\")\n",
    "    ]\n",
    "surf3 = constructor_surface_part_from_nodes(mogi,nodes3, name=\"surf_from_nodes3\")\n",
    "\n",
    "\n",
    "nodes4 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[3, 3.0, 1.0], name=\"N4_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[3., 0, 1.], name=\"N4_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[5, 0.0, -1.0], name=\"N4_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[5, 3.0, -1.0], name=\"N4_3\")\n",
    "    ]\n",
    "surf4 = constructor_surface_part_from_nodes(mogi,nodes4, name=\"surf_from_nodes4\")\n",
    "\n",
    "nodes5 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-5, 0., 0.], name=\"N5_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-3., 0, 0.], name=\"N5_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-3, 3, 0], name=\"N5_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-5, 3, 0], name=\"N5_3\")\n",
    "    ]\n",
    "\n",
    "surf5 = constructor_surface_part_from_nodes(mogi,nodes5, name=\"surf_from_nodes5\")\n",
    "\n",
    "nodes6= [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-3, 0., 0.], name=\"N6_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[7., 0., 0.], name=\"N6_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[7.0, 3.0, 0.0], name=\"N6_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-3, 3, 0], name=\"N6_3\")\n",
    "    ]\n",
    "\n",
    "surf6 = constructor_surface_part_from_nodes(mogi,nodes6, name=\"surf_from_nodes6\")\n",
    "\n",
    "nodes7= [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-3, 0., 0.], name=\"N7_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2., 0., 0.], name=\"N7_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2.0, 3.0, 0.0], name=\"N7_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-3, 3, 0], name=\"N7_3\")\n",
    "    ]\n",
    "\n",
    "surf7 = constructor_surface_part_from_nodes(mogi,nodes7, name=\"surf_from_nodes7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex1 = compute_complex_surface_two_planars(surf1.has_Representation[0], surf2.has_Representation[0], \n",
    "                    physical_space= space, knowledge_framework= mogi)\n",
    "strati_def = construct_complex_stratigraphic_part(mogi, [surf1, surf2], space)\n",
    "strati_def.explain = [surf1, surf2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1 = fold_constructor_from_two_stratigraphic_parts(surf3, surf4, \n",
    "                    physical_space= space, knowledge_framework= mogi)\n",
    "fold1.explain = [surf3, surf4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_stratigraphic_part_in_AxisAlignedCrossSection(strati_def, representation_space= section, draw_explanation=True )\n",
    "#draw_stratigraphic_part_in_AxisAlignedCrossSection(surf5, representation_space= section)\n",
    "#draw_stratigraphic_part_in_AxisAlignedCrossSection(surf4, representation_space= section)\n",
    "section.draw(strati_def, draw_explanation = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_fold_in_AxisAlignedCrossSection(fold1, representation_space= section, draw_explanation= True)\n",
    "section.draw(fold1, draw_explanation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing objets constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation\n",
    "dip = 60\n",
    "dip_dir = 45\n",
    "Rotation.from_euler(\"XZY\",[dip,dip_dir,0], degrees= True).as_matrix()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rotation.from_euler(\"XZY\",[[60,0,0],[60,180,0]], degrees= True).mean().as_euler(\"XZY\",degrees= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rotation.from_euler(\"XZY\",[[60,0,0],[60,180,0]], degrees= True).mean().as_matrix()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(ExampleTask, strategy_type= StrategyType.RANDOM)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(ExampleTask)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    strat = Strategy()\n",
    "    strat.check_applicability()\n",
    "except MalissiaBaseError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = RandomExampleTask()\n",
    "if task.check_applicability():\n",
    "    task.execute()\n",
    "task.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ActionTask(None)\n",
    "task.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    task.execute()\n",
    "except MalissiaBaseError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExampleTask.available_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserExampleTask.available_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActionTask.available_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = UserExampleTask()\n",
    "if task.check_applicability():\n",
    "    task.execute()\n",
    "task.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(SelectionTask, choices = [1,2,27,42])\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = dataset.get_observations()\n",
    "task = __default_strategy_factory__.generate_task(SelectionTask, choices = choices, strategy_type= StrategyType.RANDOM)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)\n",
    "print(\"Result type:\",type(task.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(SelectObservation, dataset = dataset)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)\n",
    "print(\"Result type:\",type(task.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(SelectSituation, context = gip)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)\n",
    "print(\"Result type:\",type(task.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the creation of the different possible instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "# clean all individuals\n",
    "mogi.remove_all_instances()\n",
    "mogi.sync_reasoner()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_all_instance_qualities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = constructor_point(mogi, [\"X\",\"Y\",\"Z\"], [1,2,3], name= \"point_test\")\n",
    "mogi.show_instance_qualities(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.sync_reasoner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "physical_space.get_object_coordinates(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = constructor_vector(mogi, [\"X\",\"Y\",\"Z\"], [1,2,3], name= \"vector_test\")\n",
    "mogi.show_instance_qualities(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.sync_reasoner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "physical_space#.get_object_coordinates(vect, kind=\"vect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs_occur = constructor_observation(mogi, name= \"obs_occurrence\", coord_labels= [\"X\",\"Y\",\"Z\"], X= 1, Y= 2, Z= 3, occurrence= True)\n",
    "mogi.show_instance_qualities(test_obs_occur)\n",
    "mogi.show_instance_qualities(test_obs_occur.has_Center[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs_dip = constructor_observation(mogi, name= \"obs_dip\", coord_labels= [\"X\",\"Y\",\"Z\"], X= 1, Y= 2, Z= 3,\n",
    "                                       dip= 30, dip_dir = 270, polarity= True, size= 0.2)\n",
    "mogi.show_instance_qualities(test_obs_dip)\n",
    "mogi.show_instance_qualities(test_obs_dip.has_Center[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.sync_reasoner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "physical_space.get_object_coordinates(test_obs_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space.get_object_coordinates(test_obs_dip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planar_Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0,0,0], name=\"N0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[1,0,1], name=\"N1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[1,1,1], name=\"N2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0,1,0], name=\"N3\")\n",
    "    ]\n",
    "plan = constructor_planar_surface_from_nodes(mogi,nodes, name=\"plan_from_nodes\")\n",
    "mogi.show_instance_qualities(plan)\n",
    "mogi.show_instance_qualities(plan.has_Center[0])\n",
    "mogi.show_instance_qualities(plan.has_Normal[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_labels = [\"X\",\"Y\",\"Z\"]\n",
    "coords = [\n",
    "    [0,0,0],\n",
    "    [1,0,1],\n",
    "    [1,1,1],\n",
    "    [0,1,0]\n",
    "    ]\n",
    "plan = constructor_planar_surface_from_coords(mogi, coord_labels= coord_labels, coords= coords, name= \"plan_from_coord\")\n",
    "mogi.show_instance_qualities(plan)\n",
    "mogi.show_instance_qualities(plan.has_Center[0])\n",
    "mogi.show_instance_qualities(plan.has_Normal[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, dr, po = physical_space.compute_dip_dir_from_normal([-1,0,0])\n",
    "print(d, dr, po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_labels = [\"X\",\"Y\",\"Z\"]\n",
    "center = [0.,0.,0.]\n",
    "plan = constructor_planar_surface_from_center_attitude(mogi, coord_labels= coord_labels, center= center,\n",
    "                                                       size= 2, dip= d, dip_dir= dr, polarity = 1, \n",
    "                                                       name= \"plan_from_attitude\"\n",
    "                                                       )\n",
    "#mogi.show_instance_qualities(plan)\n",
    "#mogi.show_instance_qualities(plan.has_Center[0])\n",
    "#mogi.show_instance_qualities(plan.has_Normal[0])\n",
    "\n",
    "print(plan.has_Normal[0].coord1, plan.has_Normal[0].coord2, plan.has_Normal[0].coord3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.sync_reasoner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "physical_space.get_object_coordinates(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space.get_object_coordinates(plan, kind= \"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space.get_object_coordinates(plan, kind= \"normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratigraphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "points = [mogi().N0, mogi().N1, mogi().N2, mogi().N3]\n",
    "#mogi.show_all_instance_qualities(points)\n",
    "#print(np.array([physical_space.get_object_coordinates(feature_i) for feature_i in points]))\n",
    "location_constructor([mogi().N0, mogi().N1, mogi().N2, mogi().N3], physical_space= physical_space, method= \"random\", return_as_dict= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "points = [mogi().N0, mogi().N1, mogi().N2, mogi().N3]\n",
    "#mogi.show_all_instance_qualities(points)\n",
    "#print(np.array([physical_space.get_object_coordinates(feature_i) for feature_i in points]))\n",
    "attitude_constructor(points, physical_space= physical_space, knowledge_framework= mogi, method= \"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "points = [mogi().N0, mogi().N2]\n",
    "#mogi.show_all_instance_qualities(points)\n",
    "#print(np.array([physical_space.get_object_coordinates(feature_i) for feature_i in points]))\n",
    "attitude_constructor(points, physical_space= physical_space, knowledge_framework= mogi, method= \"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(coordinate_labels= [\"X\",\"Y\",\"Z\"])\n",
    "obs = []\n",
    "obs += [constructor_observation(mogi, name= \"O1\", coord_labels= [\"X\",\"Y\",\"Z\"], X= 1, Y= 2, Z= 3,\n",
    "                                       dip= 30, dip_dir = 270, polarity= True, size= 0.2)]\n",
    "#mogi.show_all_instance_qualities(points)\n",
    "#print(np.array([physical_space.get_object_coordinates(feature_i) for feature_i in points]))\n",
    "attitude_constructor(obs, physical_space= physical_space, knowledge_framework= mogi, method= \"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(coordinate_labels= [\"X\",\"Y\",\"Z\"])\n",
    "obs = []\n",
    "obs += [constructor_observation(mogi, name= \"O1\", coord_labels= [\"X\",\"Y\",\"Z\"], X= 1, Y= 2, Z= 3,\n",
    "                                       dip= 90, dip_dir = 90, polarity= True, size= 0.2)]\n",
    "obs += [constructor_observation(mogi, name= \"O2\", coord_labels= [\"X\",\"Y\",\"Z\"], X= 1, Y= 2, Z=2,\n",
    "                                       dip= 90, dip_dir = 90, polarity= True, size= 0.2)]\n",
    "#mogi.show_all_instance_qualities(points)\n",
    "#print(np.array([physical_space.get_object_coordinates(feature_i) for feature_i in points]))\n",
    "attitude_constructor(obs, physical_space= physical_space, knowledge_framework= mogi, method= \"average\", regression= \"orthogonal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0,0,0], name=\"N0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[1,0,1], name=\"N1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[1,1,1], name=\"N2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0,1,0], name=\"N3\")\n",
    "    ]\n",
    "surf = constructor_surface_part_from_nodes(mogi,nodes, name=\"surf_from_nodes\")\n",
    "mogi.show_instance_qualities(surf)\n",
    "mogi.show_instance_qualities(surf.has_Representation[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_labels = [\"X\",\"Y\",\"Z\"]\n",
    "coords = [\n",
    "    [0,0,0],\n",
    "    [1,0,1],\n",
    "    [1,1,1],\n",
    "    [0,1,0]\n",
    "    ]\n",
    "surf = constructor_surface_part_from_coords(mogi, coord_labels= coord_labels, coords= coords, name= \"surf_from_coord\")\n",
    "mogi.show_instance_qualities(surf)\n",
    "mogi.show_instance_qualities(surf.has_Representation[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_labels = [\"X\",\"Y\",\"Z\"]\n",
    "center = [0.,0.,0.]\n",
    "surf = constructor_surface_part_from_center_attitude(mogi, coord_labels= coord_labels, center= center,\n",
    "                                                       size= 2, dip= 30, dip_dir= 90, polarity= 1,\n",
    "                                                       name= \"surf_from_attitude\"\n",
    "                                                       )\n",
    "mogi.show_instance_qualities(surf)\n",
    "mogi.show_instance_qualities(surf.has_Representation[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.sync_reasoner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "physical_space.get_object_coordinates(surf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space.get_object_coordinates(surf, kind= \"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space.get_object_coordinates(surf, kind= \"normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = mogi.search(type=mogi().Stratigraphic_Part)[0]\n",
    "evaluate_stratigraphic_part_explanation_consistency(part, knowledge_framework= mogi, physical_space= physical_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1, part2 = mogi.search(type=mogi().Stratigraphic_Part)[-2:]\n",
    "part1.explain = [part2]\n",
    "mogi.show_all_instance_qualities(\n",
    "    [part1.has_Representation[0],  part2.has_Representation[0]] +\n",
    "    evaluate_stratigraphic_part_explanation_consistency(part1, knowledge_framework= mogi, physical_space= physical_space)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = mogi.search(type=mogi().Stratigraphic_Part)[0]\n",
    "mogi.show_all_instance_qualities(\n",
    "    [part.has_Representation[0]] +\n",
    "    evaluate_statigraphic_part_internal_consistency(part, knowledge_framework= mogi)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_instance_qualities(\n",
    "    representation_anomaly_constructor(knowledge_framework= mogi, object= part, representation= part.has_Representation[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_instance_qualities(\n",
    "    dipping_stratigraphy_anomaly_constructor(knowledge_framework= mogi, dipping_object= part)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_instance_qualities(\n",
    "    dip_variation_anomaly_constructor(knowledge_framework= mogi, parts= [part,surf])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_instance_qualities(\n",
    "    discontinuous_stratigraphy_anomaly_constructor(knowledge_framework= mogi, parts= [part,surf])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_instance_qualities(\n",
    "    stratigraphy_border_anomaly_constructor(knowledge_framework= mogi,\n",
    "                                            stratigraphic_surface= surf,\n",
    "                                            part= part,\n",
    "                                            border= part\n",
    "                                            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_instance_qualities(\n",
    "    polarity_anomaly_constructor(knowledge_framework= mogi,\n",
    "                            parts=[surf, part]\n",
    "                                )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_instance_qualities(\n",
    "    explanation_anomaly_constructor(knowledge_framework= mogi,\n",
    "                            explaining_object= part,\n",
    "                            explained_object= surf\n",
    "                                )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratigraphicpart1 = mogi.Stratigraphic_Part()\n",
    "stratigraphicpart2 = mogi.Stratigraphic_Part()\n",
    "anom3 = discontinuousStratigaphyAnomaly_constructor(knowledge_framework= mogi, stratigraphicpart2 = stratigraphicpart2, stratigraphicpart1 = stratigraphicpart1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom3.is_Related_To"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = mogi().search(type= mogi().PointBased_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = mogi().search(type= mogi().Stratigraphic_Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(obs) > 0:\n",
    "    interpretations = mogi.get_possible_interpretations_of(obs[0])\n",
    "interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretations[0].has_Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.get_objects_potentially_explained_by(mogi().Stratigraphic_Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(obs) > 1:\n",
    "    interpretations = mogi.get_possible_interpretations_of(obs[1])\n",
    "interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().Stratigraphic_Surface.is_Possible_Explanation_Of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip = GeologicalInterpretationProcess(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.run(max_iter=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip = GeologicalInterpretationProcess(dataset, strategies= {\"SituationSelection\":select_user_unexplained_feature} )\n",
    "gip.run(max_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing interpretation from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.remove_all_instances()\n",
    "mogi.sync_reasoner()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_head = np.array(['name', 'x', 'y', 'z', 'dip_dir', 'dip', 'observed_object','occurrence'])\n",
    "data_array = np.array([['D1', 15, 20, 35, 270, 45, 'Trias_Base',True],\n",
    "                       ['D2', 30, 25, 50, 270, 45, 'Trias_Base',True],\n",
    "                       ['D3', 60, 30, 40, 90, 45, 'Trias_Base',True],\n",
    "                       ['D4', 75, 15, 25, 90, 45, 'Trias_Base',True],\n",
    "                       ['D5', 110, 20, 40, 270, 63, 'Trias_Base',True],\n",
    "                       ['D6', 120, 20, 60, 270, 64, 'Trias_Base',True],\n",
    "                       ['D7', 155, 20, 60, 89, 39, 'Trias_Base',True],\n",
    "                       ['D8', 190, 20, 30, 91, 40, 'Trias_Base',True],\n",
    "                       ['D11', 25, 22, 45, np.nan, np.nan, None ,True],\n",
    "                       ['D22', 50, 22, 50, np.nan, np.nan, None,True],\n",
    "                       ['D44', 100, 30, 20, np.nan, np.nan, None,True],\n",
    "                       ['D77', 168, 30, 47, np.nan, np.nan, None,True]]\n",
    ")\n",
    "data_test = pd.DataFrame(data = data_array, columns = data_head)\n",
    "data_test = data_test.astype({'name':str, 'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'observed_object':str, 'occurrence':bool})\n",
    "data_test.set_index(\"name\", inplace = True)\n",
    "dataset = load_dataset_from_dataframe(data_test, coordinate_labels= [\"x\",\"y\",\"z\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_instance_qualities(mogi().D1_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = AxisAlignedCrossSection(dataset.physical_space)\n",
    "section.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation = InterpretationSituation(\n",
    "    [mogi().D11, mogi().D1, mogi().D2]#, mogi().D4,mogi().D5, mogi().D3]\n",
    ")\n",
    "section.show()\n",
    "section.draw(situation)\n",
    "print(situation.features)\n",
    "mogi.show_all_instance_qualities(situation.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating attitude constructor\n",
    "\n",
    "p = compute_extremities([mogi().D1, mogi().D2], dataset.physical_space).reshape((-1,3)).tolist()\n",
    "p += [dataset.physical_space.get_object_coordinates(point_i).tolist() for point_i in [mogi().D11]]\n",
    "pca = dataset.physical_space.compute_principal_directions(p, center=True)\n",
    "\n",
    "attitude = attitude_constructor(situation.features, dataset.physical_space)\n",
    "normal_vec, dip_vec, az_vec = compute_principal_vectors_from_dip_dir(attitude[\"dip\"],attitude[\"dip_dir\"],attitude[\"polarity\"], as_array=True)\n",
    "normal_vec *= attitude[\"size\"]\n",
    "dip_vec *= attitude[\"size\"]\n",
    "az_vec *= attitude[\"size\"]\n",
    "\n",
    "plotter = pv.Plotter()\n",
    "points = pv.PolyData(p)\n",
    "plotter.add_mesh(points, render_points_as_spheres=True, point_size= 20)\n",
    "center = pv.PolyData(pca[\"center\"])\n",
    "plotter.add_mesh(center,color=\"green\")\n",
    "\n",
    "normal = pv.Arrow(start=pca[\"center\"], direction= pca[\"vectors\"][-1], scale=attitude[\"size\"])\n",
    "plotter.add_mesh(normal, color=\"lime\")\n",
    "v0 = pv.Arrow(start=pca[\"center\"], direction= pca[\"vectors\"][0], scale=attitude[\"size\"])\n",
    "plotter.add_mesh(v0, color=\"pink\")\n",
    "v1 = pv.Arrow(start=pca[\"center\"], direction= pca[\"vectors\"][1], scale=attitude[\"size\"])\n",
    "plotter.add_mesh(v1, color=\"cyan\")\n",
    "\n",
    "normal = pv.Arrow(start=pca[\"center\"], direction= normal_vec, scale=attitude[\"size\"])\n",
    "plotter.add_mesh(normal, color=\"green\")\n",
    "v0 = pv.Arrow(start=pca[\"center\"], direction= dip_vec, scale=attitude[\"size\"])\n",
    "plotter.add_mesh(v0, color=\"red\")\n",
    "v1 = pv.Arrow(start=pca[\"center\"], direction= az_vec, scale=attitude[\"size\"])\n",
    "plotter.add_mesh(v1, color=\"blue\")\n",
    "\n",
    "plotter.show(cpos='xz', window_size = (600,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = constructor_surface_part_from_interpretation(knowledge_framework= mogi, interpretation_situation= situation,\n",
    "                                             physical_space= dataset.physical_space, name= \"interp_surf\",extension_factor=0.15)\n",
    "mogi.show_instance_qualities(interp)\n",
    "mogi.show_instance_qualities(interp.has_Representation[0])\n",
    "mogi.show_instance_qualities(interp.has_Representation[0].has_Center[0])\n",
    "mogi.show_all_instance_qualities(interp.explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = AxisAlignedCrossSection(dataset.physical_space)\n",
    "section.show()\n",
    "\n",
    "drawing_function = section.get_drawing_method(interp.is_instance_of[0])\n",
    "drawing_function(interp, section, setup_drawing= False)\n",
    "section.get_drawing_method(InterpretationSituation)(situation, section)\n",
    "\n",
    "#ext = compute_extremities(situation.features, dataset.physical_space).reshape((-1,3))\n",
    "#plt.scatter(ext[:,0],ext[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = pv.Plotter()\n",
    "ext_pv = pv.PolyData(p)\n",
    "plotter.add_mesh(ext_pv)\n",
    "\n",
    "surf = interp.has_Representation[0]\n",
    "interp_ext = pv.PolyData(compute_extremities([surf], dataset.physical_space).reshape(-1,3), faces=[4,0,1,2,3])\n",
    "plotter.add_mesh(interp_ext, color=\"green\")\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section.draw(situation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.remove_all_instances()\n",
    "mogi.sync_reasoner()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_head = np.array(['name', 'x', 'y', 'z', 'dip_dir', 'dip', 'observed_object','occurrence'])\n",
    "data_array = np.array([['D1', 15, 20, 35, 270, 45, 'Trias_Base',True],\n",
    "                       ['D2', 30, 25, 50, 270, 45, 'Trias_Base',True],\n",
    "                       ['D3', 60, 30, 40, 90, 45, 'Trias_Base',True],\n",
    "                       ['D11', 25, 22, 45, np.nan, np.nan, None ,True]]\n",
    ")\n",
    "data_test = pd.DataFrame(data = data_array, columns = data_head)\n",
    "data_test = data_test.astype({'name':str, 'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'observed_object':str, 'occurrence':bool})\n",
    "data_test.set_index(\"name\", inplace = True)\n",
    "dataset = load_dataset_from_dataframe(data_test, coordinate_labels= [\"x\",\"y\",\"z\"])\n",
    "section = AxisAlignedCrossSection(dataset.physical_space)\n",
    "section.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_anomaly_1 = representation_anomaly_constructor(mogi, mogi().D1, mogi().D1.has_Representation[0])\n",
    "section.show()\n",
    "section.draw(representation_anomaly_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_part = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [20,0,45], dip= 30, dip_dir= 270, size= 20)\n",
    "dipping_anomaly_1 = dipping_stratigraphy_anomaly_constructor(mogi, surf_part)\n",
    "section.show()\n",
    "section.draw(dipping_anomaly_1, draw_normal= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_part = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [20,0,45], dip= 30, dip_dir= 270, size= 20)\n",
    "dipping_anomaly_2 = evaluate_statigraphic_part_internal_consistency(surf_part, mogi)\n",
    "section.show()\n",
    "section.draw(dipping_anomaly_2[0], draw_normal= True, linewidth= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_part = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [23,0,45], dip= 30, dip_dir= 270, size= 20)\n",
    "surf_part.explain.append(mogi().D1)\n",
    "surf_part.explain.append(mogi().D2)\n",
    "surf_part.explain.append(mogi().D11)\n",
    "section.draw(surf_part, draw_explanation= True)\n",
    "section.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_part = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [23,0,45], dip= 30, dip_dir= 270, size= 20)\n",
    "surf_part.explain.append(mogi().D1)\n",
    "surf_part.explain.append(mogi().D2)\n",
    "surf_part.explain.append(mogi().D11)\n",
    "section.draw(surf_part, draw_explanation= True)\n",
    "section.show()\n",
    "\n",
    "explanation_anomalies = evaluate_stratigraphic_part_explanation_consistency(surf_part, mogi, physical_space= section.space)\n",
    "for anomaly in explanation_anomalies:\n",
    "    section.draw(anomaly, draw_normal= True, linewidth= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_part_1 = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [20,0,45], dip= 30, dip_dir= 270, size= 20)\n",
    "surf_part_2 = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [45,0,50], dip= 00, dip_dir= 270, size= 10)\n",
    "dip_variation_anomaly_1 = dip_variation_anomaly_constructor(mogi, [surf_part_1,surf_part_2])\n",
    "section.show()\n",
    "section.draw(dip_variation_anomaly_1, draw_normal= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_part_1 = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [20,0,45], dip= 30, dip_dir= 270, size= 20)\n",
    "surf_part_2 = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [45,0,50], dip= 30, dip_dir= 270, polarity= 1, size= 30)\n",
    "dip_variation_anomaly_1 = dip_variation_anomaly_constructor(mogi, [surf_part_1,surf_part_2])\n",
    "section.show()\n",
    "section.draw(dip_variation_anomaly_1, draw_normal= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_part_1 = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [20,0,45], dip= 30, dip_dir= 270, size= 20)\n",
    "surf_part_2 = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [45,0,50], dip= 0, dip_dir= 270, polarity= -1, size= 30)\n",
    "polarity_anomaly_1 = polarity_anomaly_constructor(mogi, [surf_part_1,surf_part_2])\n",
    "section.show()\n",
    "section.draw(polarity_anomaly_1, draw_normal= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.remove_all_instances()\n",
    "mogi.sync_reasoner()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_head = np.array(['name', 'x', 'y', 'z', 'dip_dir', 'dip', 'observed_object','occurrence'])\n",
    "data_array = np.array([['D1', 15, 20, 35, 270, 45, 'Trias_Base',True],\n",
    "                       ['D2', 30, 25, 50, 270, 45, 'Trias_Base',True],\n",
    "                       ['D3', 60, 30, 40, 90, 45, 'Trias_Base',True],\n",
    "                       ['D11', 25, 22, 45, np.nan, np.nan, None ,True]]\n",
    ")\n",
    "data_test = pd.DataFrame(data = data_array, columns = data_head)\n",
    "data_test = data_test.astype({'name':str, 'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'observed_object':str, 'occurrence':bool})\n",
    "data_test.set_index(\"name\", inplace = True)\n",
    "dataset = load_dataset_from_dataframe(data_test, coordinate_labels= [\"x\",\"y\",\"z\"])\n",
    "section = AxisAlignedCrossSection(dataset.physical_space)\n",
    "section.show()\n",
    "print(\"Coverage: {} %\".format(100*dataset.physical_space.evaluate_coverage()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_part_1 = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [20,0,45], dip= 0, dip_dir= 270, size= 10)\n",
    "surf_part_2 = constructor_surface_part_from_center_attitude(\n",
    "    knowledge_framework= mogi, coord_labels= dataset.physical_space.coordinate_labels,\n",
    "    center= [40,0,45], dip= 0, dip_dir= 270, size= 10)\n",
    "dataset.physical_space.attach_interpreted_object(surf_part_1)\n",
    "dataset.physical_space.attach_interpreted_object(surf_part_2)\n",
    "\n",
    "section.show()\n",
    "print(\"Coverage: {:.2f} %\".format(100*dataset.physical_space.evaluate_coverage()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing interpretation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.remove_all_instances()\n",
    "\n",
    "data_head = np.array(['name', 'x', 'y', 'z', 'dip_dir', 'dip', 'observed_object','occurrence'])\n",
    "data_array = np.array([['D1', 15, 20, 35, 270, 45, 'Trias_Base',True],\n",
    "                       ['D2', 30, 25, 50, 270, 45, 'Trias_Base',True],\n",
    "                       ['D3', 60, 30, 40, 90, 45, 'Trias_Base',True],\n",
    "                       ['D4', 75, 15, 25, 90, 45, 'Trias_Base',True],\n",
    "                       ['D5', 110, 20, 40, 270, 63, 'Trias_Base',True],\n",
    "                       ['D6', 120, 20, 60, 270, 64, 'Trias_Base',True],\n",
    "                       ['D7', 155, 20, 60, 89, 39, 'Trias_Base',True],\n",
    "                       ['D8', 190, 20, 30, 91, 40, 'Trias_Base',True],\n",
    "                       ['D11', 25, 22, 45, np.nan, np.nan, None ,True],\n",
    "                       ['D22', 50, 22, 50, np.nan, np.nan, None,True],\n",
    "                       ['D44', 100, 30, 20, np.nan, np.nan, None,True],\n",
    "                       ['D77', 168, 30, 47, np.nan, np.nan, None,True]]\n",
    ")\n",
    "data_test = pd.DataFrame(data = data_array, columns = data_head)\n",
    "data_test = data_test.astype({'name':str, 'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'observed_object':str, 'occurrence':bool})\n",
    "data_test.set_index(\"name\", inplace = True)\n",
    "dataset = load_dataset_from_dataframe(data_test, coordinate_labels= [\"x\",\"y\",\"z\"])\n",
    "\n",
    "section = AxisAlignedCrossSection(dataset.physical_space)\n",
    "section.draw(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_all_instance_qualities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip = GeologicalInterpretationProcess(dataset= dataset)\n",
    "#print(gip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section.draw(gip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.new_epoch()\n",
    "gip.plan(\n",
    "    #random_move=True !!!\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gip.iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.show(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.do()\n",
    "gip.show(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gip.iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gip.interpreted_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_all_instance_qualities(list(gip.interpreted_objects)[0].has_Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.check()\n",
    "print(gip.iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.show(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.run(n_iter=3)\n",
    "gip.show(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.interpreted_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.representation_space.evaluate_coverage(objects= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.update_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = dataset.physical_space.get_object_coordinates(mogi().stratigraphic_part160, kind=\"nodes\")\n",
    "np.array(\n",
    "    [np.min(coord, axis=0),\n",
    "     np.max(coord, axis=0)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.show(section, draw_explanation= True, draw_anomalies= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in gip.history:\n",
    "    print(\"---New Iteration---\")\n",
    "    print(iteration)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gip.iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing complex_surface and folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes1 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-9, 0., 0.], name=\"N1_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0., 0, 0.], name=\"N1_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0, 3, 0], name=\"N1_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-9, 3, 0], name=\"N1_3\")\n",
    "    ]\n",
    "\n",
    "surf1 = constructor_surface_part_from_nodes(mogi,nodes1, name=\"surf_from_nodes1\")\n",
    "\n",
    "nodes2 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0, 0., 0.], name=\"N2_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[3., 0., 0.], name=\"N2_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[3.0, 3.0, 0.0], name=\"N2_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0, 3, 0], name=\"N2_3\")\n",
    "    ]\n",
    "\n",
    "surf2 = constructor_surface_part_from_nodes(mogi,nodes2, name=\"surf_from_nodes2\")\n",
    "\n",
    "\n",
    "\n",
    "nodes3 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-9, 1., 0.], name=\"N3_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2., 1, 0.], name=\"N3_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-7., 3.0, 0.0], name=\"N3_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-9.0, 3.0, 0.0], name=\"N3_3\")\n",
    "    ]\n",
    "surf3 = constructor_surface_part_from_nodes(mogi,nodes3, name=\"surf_from_nodes3\")\n",
    "\n",
    "\n",
    "nodes4 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-9, 1., 0.], name=\"N4_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-7., 1, 0.], name=\"N4_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-7., 3.0, 0.0], name=\"N4_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-9.0, 3.0, 0.0], name=\"N4_3\")\n",
    "    ]\n",
    "surf4 = constructor_surface_part_from_nodes(mogi,nodes4, name=\"surf_from_nodes4\")\n",
    "mogi.show_instance_qualities(surf1)\n",
    "#mogi.show_instance_qualities(surf1.has_Representation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = PhysicalRepresentationSpace(3)\n",
    "space._name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex1 =  compute_complex_surface_two_planars(surf1.has_Representation[0], surf2.has_Representation[0], \n",
    "                    physical_space= space, knowledge_framework= mogi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_sides_are_parallel(complex_surface=complex1, knowledge_framework=mogi, physical_space= space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = np.array(space.get_object_coordinates( object= surf1, kind= \"normal\")) \n",
    "nn2 = np.array(space.get_object_coordinates( object= surf2, kind= \"normal\"))\n",
    "cn1 = np.array(space.get_object_coordinates( object= surf1, kind= \"center\")) \n",
    "cn2 = np.array(space.get_object_coordinates( object= surf2, kind= \"center\"))  \n",
    "v1 = space.get_object_coordinates( object= surf1, kind= \"nodes\")\n",
    "v2 = space.get_object_coordinates( object= surf2, kind= \"nodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn3 = np.array(space.get_object_coordinates( object= mogi().part1_in_Cmpx_Su_from_Cmpx_Su_Pla_1, kind= \"normal\")) \n",
    "nn4 = np.array(space.get_object_coordinates( object= mogi().part2_in_Cmpx_Su_from_Cmpx_Su_Pla_1, kind= \"normal\"))\n",
    "cn3 = np.array(space.get_object_coordinates( object= mogi().part1_in_Cmpx_Su_from_Cmpx_Su_Pla_1, kind= \"center\")) \n",
    "cn4 = np.array(space.get_object_coordinates( object= mogi().part2_in_Cmpx_Su_from_Cmpx_Su_Pla_1, kind= \"center\"))  \n",
    "v3 = space.get_object_coordinates( object= mogi().part1_in_Cmpx_Su_from_Cmpx_Su_Pla_1, kind= \"nodes\")\n",
    "v4 = space.get_object_coordinates( object= mogi().part2_in_Cmpx_Su_from_Cmpx_Su_Pla_1, kind= \"nodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyvista  as pv\n",
    "\n",
    "vertices_1 = np.array(v1)\n",
    "faces1 = np.hstack([ [ 4, 0, 1, 2, 3]])\n",
    "surfa1 = pv.PolyData(vertices_1, faces1)\n",
    "vertices_2 = np.array(v2)\n",
    "faces2 = np.hstack([[ 4, 0, 1, 2, 3]])\n",
    "surfa2 = pv.PolyData(vertices_2, faces2)\n",
    "\n",
    "\n",
    "arrow1 = pv.Arrow(cn1, nn1)\n",
    "arrow2 = pv.Arrow(cn2, nn2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p = pv.Plotter()\n",
    "\n",
    "p.add_mesh(arrow1, color='red', show_edges=True)\n",
    "p.add_mesh(arrow2, color='red', show_edges=True)\n",
    "p.add_mesh(surfa1, color='grey', show_edges=True)\n",
    "p.add_mesh(surfa2, color='yellow', show_edges=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Render all of them\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pv.Plotter()\n",
    "\n",
    "\n",
    "vertices_3 = np.array(v3)\n",
    "faces3 = np.hstack([ [ 4, 0, 1, 2, 3]])\n",
    "surfa3 = pv.PolyData(vertices_3, faces3)\n",
    "vertices_4 = np.array(v4)\n",
    "faces4 = np.hstack([[ 4, 0, 1, 2, 3]])\n",
    "surfa4 = pv.PolyData(vertices_4, faces4)\n",
    "arrow3 = pv.Arrow(cn3, nn3)\n",
    "arrow4 = pv.Arrow(cn4, nn4)\n",
    "p.add_mesh(arrow3, color='red', show_edges=True)\n",
    "p.add_mesh(arrow4, color='red', show_edges=True)\n",
    "p.add_mesh(surfa3, color='grey', show_edges=True)\n",
    "p.add_mesh(surfa4, color='yellow', show_edges=True)\n",
    "\n",
    "\n",
    "\n",
    "vertices_1 = np.array(v1)\n",
    "faces1 = np.hstack([ [ 4, 0, 1, 2, 3]])\n",
    "surfa1 = pv.PolyData(vertices_1, faces1)\n",
    "vertices_2 = np.array(v2)\n",
    "faces2 = np.hstack([[ 4, 0, 1, 2, 3]])\n",
    "surfa2 = pv.PolyData(vertices_2, faces2)\n",
    "\n",
    "\n",
    "arrow1 = pv.Arrow(cn1, nn1)\n",
    "arrow2 = pv.Arrow(cn2, nn2)\n",
    "\n",
    "\n",
    "p.add_mesh(arrow1, color='red', show_edges=True)\n",
    "p.add_mesh(arrow2, color='red', show_edges=True)\n",
    "p.add_mesh(surfa1, color='grey', show_edges=True)\n",
    "p.add_mesh(surfa2, color='yellow', show_edges=True)\n",
    "p.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.remove_all_instances()\n",
    "mogi.sync_reasoner()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "space= PhysicalRepresentationSpace(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing manual stratigraphic_parts construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes1 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-8, 0., -1.], name=\"N1_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6., 0, 1.], name=\"N1_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6, 3, 1], name=\"N1_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-8, 3, -1], name=\"N1_3\")\n",
    "    ]\n",
    "\n",
    "surf1 = constructor_surface_part_from_nodes(mogi,nodes1, name=\"surf_from_nodes1\")\n",
    "\n",
    "nodes2 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6, 0., 1.], name=\"N2_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-4., 0., -1.], name=\"N2_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-4.0, 3.0, -1], name=\"N2_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6, 3, 1], name=\"N2_3\")\n",
    "    ]\n",
    "\n",
    "surf2 = constructor_surface_part_from_nodes(mogi,nodes2, name=\"surf_from_nodes2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nodes3 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-4, 0., -1.], name=\"N3_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2., 0, 1.], name=\"N3_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2, 3.0, 1.0], name=\"N3_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-4., 3., -1.], name=\"N3_3\")\n",
    "    ]\n",
    "surf3 = constructor_surface_part_from_nodes(mogi,nodes3, name=\"surf_from_nodes3\")\n",
    "\n",
    "\n",
    "nodes4 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2, 0.0, 1.0], name=\"N4_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0., 0, -1.], name=\"N4_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0, 3.0, -1.0], name=\"N4_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2, 3.0, 1.0], name=\"N4_3\")\n",
    "    ]\n",
    "surf4 = constructor_surface_part_from_nodes(mogi,nodes4, name=\"surf_from_nodes4\")\n",
    "\n",
    "nodes5 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0, 0.,-1.], name=\"N5_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2., 0, 1.], name=\"N5_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2, 3, 1], name=\"N5_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0, 3, -1], name=\"N5_3\")\n",
    "    ]\n",
    "\n",
    "surf5 = constructor_surface_part_from_nodes(mogi,nodes5, name=\"surf_from_nodes5\")\n",
    "\n",
    "nodes6= [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2, 0., 1.], name=\"N6_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[4., 0., -1.], name=\"N6_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[4.0, 3.0, -1.0], name=\"N6_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2, 3, 1], name=\"N6_3\")\n",
    "    ]\n",
    "\n",
    "surf6 = constructor_surface_part_from_nodes(mogi,nodes6, name=\"surf_from_nodes6\")\n",
    "\n",
    "nodes7= [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2, 0., -2.], name=\"N7_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2., 3.,-2.], name=\"N7_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2.0, 3.0, 0.0], name=\"N7_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2, 0, 0], name=\"N7_3\")\n",
    "    ]\n",
    "\n",
    "surf7 = constructor_surface_part_from_nodes(mogi,nodes7, name=\"surf_from_nodes7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = [surf1,surf2]\n",
    "strati1 = construct_complex_stratigraphic_part(mogi, features1, space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = [surf3,surf4]\n",
    "strati2 = construct_complex_stratigraphic_part(mogi, features2, space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3 = [strati1,strati2]\n",
    "strati3 = construct_complex_stratigraphic_part(mogi, features3, space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "space= PhysicalRepresentationSpace(3)\n",
    "space.extension = [[-10,10], [-10,10], [-10,10]]\n",
    "section = AxisAlignedCrossSection(space)\n",
    "#section.draw(strati1,  draw_explanation = True)\n",
    "#section.draw(strati2,  draw_explanation = True)\n",
    "section.draw(strati1,  draw_explanation = True)\n",
    "section.draw(strati4,  draw_explanation = True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features4 = [surf5,surf6]\n",
    "strati4 = construct_complex_stratigraphic_part(mogi, features4, space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features5 = [strati1,strati4]\n",
    "strati5 = construct_complex_stratigraphic_part(mogi, features5, space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_all_instance_qualities([mogi().part_nb11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "space= PhysicalRepresentationSpace(3)\n",
    "space.extension = [[-10,10], [-10,10], [-10,10]]\n",
    "section = AxisAlignedCrossSection(space)\n",
    "\n",
    "\n",
    "section.draw(strati5,  draw_explanation = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing construction of stratigraphic_parts in manual interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes1 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-8, 0., -1.], name=\"N1_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6., 0, 1.], name=\"N1_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6, 3, 1], name=\"N1_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-8, 3, -1], name=\"N1_3\")\n",
    "    ]\n",
    "\n",
    "surf1 = constructor_surface_part_from_nodes(mogi,nodes1, name=\"surf_from_nodes1\")\n",
    "\n",
    "nodes2 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6, 0., 1.], name=\"N2_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-4., 0., -1.], name=\"N2_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-4.0, 3.0, -1], name=\"N2_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6, 3, 1], name=\"N2_3\")\n",
    "    ]\n",
    "\n",
    "surf2 = constructor_surface_part_from_nodes(mogi,nodes2, name=\"surf_from_nodes2\")\n",
    "\n",
    "\n",
    "features1 = [surf1,surf2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space.extension = [[-10,10], [-10,10], [-10,10]]\n",
    "section = AxisAlignedCrossSection(space)\n",
    "situation = InterpretationSituation(features1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section.draw(situation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = constructor_surface_part_from_interpretation_of_parts(knowledge_framework= mogi, interpretation_situation= situation,\n",
    "                                             physical_space= space, name= \"interp_parts\",extension_factor=0.15)\n",
    "mogi.show_instance_qualities(interp)\n",
    "mogi.show_instance_qualities(interp.has_Representation[0])\n",
    "mogi.show_all_instance_qualities(interp.explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing_function = section.get_drawing_method(interp.is_instance_of[0])\n",
    "drawing_function(interp, section, setup_drawing= False)\n",
    "section.get_drawing_method(InterpretationSituation)(situation, section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing anomalies and updating for stratigraphic_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes1 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-8, 0., -1.], name=\"N1_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6., 0, 1.], name=\"N1_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6, 3, 1], name=\"N1_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-8, 3, -1], name=\"N1_3\")\n",
    "    ]\n",
    "\n",
    "surf1 = constructor_surface_part_from_nodes(mogi,nodes1, name=\"surf_from_nodes1\")\n",
    "\n",
    "nodes2 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6, 0., 1.], name=\"N2_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-4., 0., -1.], name=\"N2_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-4.0, 3.0, -1], name=\"N2_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-6, 3, 1], name=\"N2_3\")\n",
    "    ]\n",
    "\n",
    "surf2 = constructor_surface_part_from_nodes(mogi,nodes2, name=\"surf_from_nodes2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nodes3 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-4, -2., -1.], name=\"N3_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2., -2, 1.], name=\"N3_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2, 4.0, 1.0], name=\"N3_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-4., 4., -1.], name=\"N3_3\")\n",
    "    ]\n",
    "surf3 = constructor_surface_part_from_nodes(mogi,nodes3, name=\"surf_from_nodes3\")\n",
    "\n",
    "\n",
    "nodes4 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2, 0.0, 1.0], name=\"N4_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0., 0, -1.], name=\"N4_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0, 3.0, -1.0], name=\"N4_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2, 3.0, 1.0], name=\"N4_3\")\n",
    "    ]\n",
    "surf4 = constructor_surface_part_from_nodes(mogi,nodes4, name=\"surf_from_nodes4\")\n",
    "\n",
    "nodes5 = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0, 0.,-1.], name=\"N5_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2., 0, 1.], name=\"N5_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2, 3, 1], name=\"N5_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0, 3, -1], name=\"N5_3\")\n",
    "    ]\n",
    "\n",
    "surf5 = constructor_surface_part_from_nodes(mogi,nodes5, name=\"surf_from_nodes5\")\n",
    "\n",
    "nodes6= [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2, 0., 1.], name=\"N6_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[4., 0., -1.], name=\"N6_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[4.0, 3.0, -1.0], name=\"N6_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2, 3, 1], name=\"N6_3\")\n",
    "    ]\n",
    "\n",
    "surf6 = constructor_surface_part_from_nodes(mogi,nodes6, name=\"surf_from_nodes6\")\n",
    "\n",
    "nodes7= [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2, 0., -2.], name=\"N7_0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[2., 3.,-2.], name=\"N7_1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2.0, 3.0, 0.0], name=\"N7_2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[-2, 0, 0], name=\"N7_3\")\n",
    "    ]\n",
    "\n",
    "surf7 = constructor_surface_part_from_nodes(mogi,nodes7, name=\"surf_from_nodes7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = [surf1,surf2]\n",
    "strati1 = construct_complex_stratigraphic_part(mogi, features1, space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1 = fold_constructor_from_two_stratigraphic_parts(candidate_limb_part1= surf1, candidate_limb_part2= surf2, physical_space= space, knowledge_framework= mogi )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_constructor_from_two_stratigraphic_parts(surf2, surf1, physical_space= space, knowledge_framework= mogi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().surf_from_nodes1_rep.has_Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = [strati1,surf3]\n",
    "strati2 = construct_complex_stratigraphic_part(mogi, features2, space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_2 = strati2.has_Representation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an = check_complex_surface_geometry(rep_2, space,mogi)\n",
    "typo_an = an.is_a[0]\n",
    "typo_an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_surf = updated_complex_surface_constructor_after_anomaly(mogi, space, an, typo_an )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "space.extension = [[-10,10], [-10,10], [-10,10]]\n",
    "section = AxisAlignedCrossSection(space)\n",
    "#section.draw(strati1,  draw_explanation = True)\n",
    "#section.draw(strati2,  draw_explanation = True)\n",
    "#section.draw(strati1,  draw_explanation = False)\n",
    "#section.draw(strati1)\n",
    "#section.draw(strati2)\n",
    "section.draw(an)\n",
    "#section.draw_multiple_planes(new_surf.has_Part)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
