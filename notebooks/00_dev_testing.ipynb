{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geological Interpretor Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for testing and developping some of the basic code in this package."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ontology manipulation\n",
    "\n",
    "The knowledge manipulated in this package is formalised in an ontology,<br>\n",
    "which is store in a *.owl* file.\n",
    "\n",
    "It is named **MOGI** for **M**inimal **O**ntology for **G**eological **I**nterpretation\n",
    "\n",
    "To manipulated this ontology, we use the package **owlready2** available from here: https://owlready2.readthedocs.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import owlready2 as owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owl.onto_path.append(\"../ontologies/\")\n",
    "mogi = owl.get_ontology(\"mogi.owl\").load()\n",
    "mogi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ontology provides access to its components, e.g.:\n",
    "* classes\n",
    "* properties\n",
    "* individuals\n",
    "* rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(mogi.classes()))\n",
    "print(list(mogi.properties()))\n",
    "print(list(mogi.individuals()))\n",
    "print(list(mogi.rules()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specific elements can be searched through simple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(iri = \"*Surface\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoner\n",
    "\n",
    "Ontologies are even more powerful thansk to their capabilities to use reasoning for infering types, properties, and relationships that were not explicitly stated.\n",
    "This is usefull for obtaining results implied by the already stated information.\n",
    "\n",
    "This is achieved by running a *reasoner* on the ontology as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owl.sync_reasoner(infer_property_values=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geological Knowledge Manager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GeologicalKnowledgeManager** may know different instances of **GeologicalKnowledgeFramework**,<br>\n",
    "for example to allow differenciating scenarios or for allowing customisation of knowledge and its formalisation.\n",
    "\n",
    "**GeologicalKnowledgeFramework** provides access to concept definitions for providing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "class GeologicalKnowledgeManager(object):\n",
    "    \"\"\"GeologicalKnowledgeManager is managing one or several GeologicalKnowledgeFramework.\n",
    "    \n",
    "    The GeologicalKnowledgeManager is typically a singleton, so there is always one and only one instance of it.\n",
    "    \n",
    "    The GeologicalKnowledgeManager may know different instances of GeologicalKnowledgeFramework,\n",
    "    for example to allow different interpretation scenarios or for allowing user-specific customisation\n",
    "    of knowledge and its formalisation.\n",
    "    \n",
    "    GeologicalKnowledgeFramework are typically ontologies and extensions defined in this package or elsewhere.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __new__(cls):\n",
    "        \"\"\"Method to access (and create if needed) the only allowed instance of this class.\n",
    "        \n",
    "        Returns:\n",
    "        - an instance of GeologicalKnowledgeManager\"\"\"\n",
    "        if not hasattr(cls, 'instance'):\n",
    "            cls.instance = super(GeologicalKnowledgeManager, cls).__new__(cls)\n",
    "            cls.initialised= False\n",
    "        return cls.instance\n",
    "        \n",
    "    def __init__(self, default= \"mogi\", default_source_directory= \"../ontologies/\", default_source_file= \"mogi.owl\", default_ontology_backend= \"owlready2\"):\n",
    "        \"\"\"Initializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        if not self.initialised:\n",
    "            self._initialise(default= default, default_source_directory= default_source_directory, default_source_file= default_source_file, default_ontology_backend= default_ontology_backend)\n",
    "            \n",
    "    def _initialise(self, default, default_source_directory, default_source_file, default_ontology_backend):\n",
    "        \"\"\"Initializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        self.default= default\n",
    "        self.default_source_directory= default_source_directory\n",
    "        self.default_source_file= default_source_file\n",
    "        self.default_ontology_backend= default_ontology_backend\n",
    "        \n",
    "        self.knowledge_framework_dict = {}\n",
    "        \n",
    "        self.initialised= True\n",
    "        \n",
    "    def reset(self, default= \"mogi\", default_source_directory= \"../ontologies/\", default_source_file= \"mogi.owl\", default_ontology_backend= \"owlready2\"):\n",
    "        \"\"\"Reinitializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        self._initialise(default= default, default_source_directory= default_source_directory, default_source_file= default_source_file, default_ontology_backend= default_ontology_backend)\n",
    "             \n",
    "    def load_knowledge_framework(self, name=None, source= None, source_directory= None, backend= None):\n",
    "        \"\"\"Gets and initilises the ontology from the specified source.\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name to be given to the knowledge framework. If None (default) the file name will be used.\n",
    "        - source: filename to the ontology source. If None(default) the default ontology is used.\n",
    "        - source_directory: where the system should look for ontology definition files. If None, the `GeologicalKnowledgeFramework` will decide.\n",
    "        - backend: the ontology backend to be used. If None, the `GeologicalKnowledgeFramework` will decide.\"\"\"\n",
    "        source = source if source is not None else self.default_source_file\n",
    "        name = name if name is not None else os.path.basename(source).split(os.path.extsep)[0]\n",
    "        self.knowledge_framework_dict[name] = GeologicalKnowledgeFramework(name= name, source= source, source_directory= source_directory, backend= backend)\n",
    "    \n",
    "    def get_knowledge_framework(self,name= \"default\"):\n",
    "        \"\"\"Accessor to knowledge frameworks.\"\"\"\n",
    "        name = self.default if name == \"default\" else name\n",
    "        assert len(self.knowledge_framework_dict) > 0, \"No ontology has been loaded yet. Please use GeologicalKnowledgeManager().load_knowledge_framework() first\"\n",
    "        assert name in self.knowledge_framework_dict.keys(), \"The specified ontology hasn't been loaded: \"+name+\\\n",
    "            \"\\navailable ontology names are: \"+\"\\n\".join(self.knowledge_framework_dict.keys())\n",
    "        return self.knowledge_framework_dict[name]\n",
    "    \n",
    "class GeologicalKnowledgeFramework(object):\n",
    "    \"\"\"A GeologicalKnowledgeFramework holds the definition of concepts and relationships describing knowledge.\n",
    "    \n",
    "    This is typically an overlay around a formal ontology definition, which also brings additional capabilities,\n",
    "    such as algorithms and factories to achieve specific tasks and create objects.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, source, source_directory= None, backend= None):\n",
    "        \"\"\"Initialise a KnowledgeFramework form a given ontology file (source).\n",
    "        \n",
    "        Parameters:\n",
    "        - name: should be the name under which this KnowledgeFramework is known in the manager\n",
    "        - source: the source file for the ontology definition\n",
    "        - source_directory: the directory where the source files for the ontology definition are looked for.\n",
    "        If None (default) the default path provided by the `KnowledgeManager` is used.\n",
    "        - backend: the ontology backend to be used for this knwoledge framework.\n",
    "        If None (default) the default ontology backend provided by the `KnowledgeManager` is used.\"\"\"\n",
    "        self.name= name\n",
    "        \n",
    "        self.__source_directory= None\n",
    "        self.init_source_directory(source_directory)\n",
    "        self.initialise_ontology_backend(backend)\n",
    "        \n",
    "        self.load_ontology(source)\n",
    "    \n",
    "    def init_source_directory(self, source_directory):\n",
    "        \"\"\"Initialises the folder where source files are searched.\n",
    "        \n",
    "        Parameters:\n",
    "        - source_directory: if None, the previous value is used if it wasn't None, else the `GeologicalKnowledgeManager`default is used.\"\"\"\n",
    "        if source_directory is not None:\n",
    "            self.__source_directory= source_directory\n",
    "        elif self.__source_directory is None:\n",
    "            self.__source_directory= GeologicalKnowledgeManager().default_source_directory\n",
    "    \n",
    "    def initialise_ontology_backend(self, backend_name:str= None):\n",
    "        \"\"\"Initializes the ontology package used as a backend to access ontologies.\n",
    "        \n",
    "        This will:\n",
    "        - try to import the backend as onto\n",
    "        - set the default path for ontologies\"\"\"\n",
    "                \n",
    "        self.__ontology_backend = None\n",
    "        backend_name= GeologicalKnowledgeManager().default_ontology_backend if backend_name is None else backend_name\n",
    "        if backend_name == \"owlready2\":\n",
    "            try:\n",
    "                import owlready2 as owl2 \n",
    "                self.__ontology_backend = owl2\n",
    "                if self.__source_directory not in self.__ontology_backend.onto_path:\n",
    "                    self.__ontology_backend.onto_path.append(self.__source_directory)\n",
    "            except ImportError:\n",
    "                raise ImportError(\"Your are trying to use Owlready2 as a backend for ontology management, but it doesn't appear to be installed.\"\\\n",
    "                \"This is either because OwlReady2 is given as default option or because you asked for it.\"\\\n",
    "                \"Please install the OwlReady2 package from https://owlready2.readthedocs.io\"\\\n",
    "                \"or give another backend through GeologicalKnowledgeManager().initialise_ontology_backend()\")\n",
    "                \n",
    "            # also test if java is correctly installed & accessible, as it is used by owlready2 for reasoning\n",
    "            try:\n",
    "                os.system(\"java -version\")\n",
    "            except:\n",
    "                raise ImportError(\"Java doesn't appear to be installed properly as the command `java -version` returned an error.\"\\\n",
    "                    \"This error occured while loading owlready2 package as an ontology backend, because java is used for the reasoning engine.\")\n",
    "        else:\n",
    "            raise Exception(\"The specified backed for ontology is not supported: \"+backend_name)\n",
    "          \n",
    "        \n",
    "    def load_ontology(self, source):\n",
    "        \"\"\"Loads the ontology specified by source.\n",
    "        \n",
    "        Parameters:\n",
    "        - source: the source file for the ontology definition\n",
    "        - source_directory: the directory where the source files for the ontology definition are looked for.\n",
    "        If None (default) the default path provided by the `KnowledgeManager` is used.\"\"\"\n",
    "        self.__source= source\n",
    "        try:\n",
    "            self.__onto = self.__ontology_backend.get_ontology(self.__source).load()\n",
    "        except Exception as err:\n",
    "            raise Exception(\"Unexpected exception received while loading ontology:\\n - source: {}\\n - onto_path: {}\".format(self.__source, self.__ontology_backend.onto_path))\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.__onto\n",
    "        \n",
    "    def get_ontology_backend(self):\n",
    "        \"\"\"Gets the ontology backend\"\"\"\n",
    "        assert self.__ontology_backend is not None, \"Trying to access the ontology backend without initialising it.\"\n",
    "        return self.__ontology_backend\n",
    "    \n",
    "    def search(self, name= None, type= None, qualities= None, prepend_star=True) -> list:\n",
    "        \"\"\"Search function to interface the serach capabilities of the internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the search object (you can use * to replace any set of characters and ? to replace any single character)\n",
    "        Note: if `prepend_star` a * is always prepended to allows the search to work because of the internal prefix names\n",
    "        - type: the type of the searched observations as defined by the internal ontology\n",
    "        - qualities: qualities to filter the observations.\n",
    "        If `qualities` is a :\n",
    "         * `str`: a single quality will be searched for with any value (\"*\"),\n",
    "         * `list`: a list of qualities will be searched for with any values (\"*\")\n",
    "         * `dict`: a list of qualities defined by the keys and with the associated values will be searched for\"\"\"\n",
    "        if name is None:\n",
    "            name = \"*\"\n",
    "        elif prepend_star:\n",
    "            name = \"*\"+name\n",
    "        \n",
    "        if isinstance(qualities,list):\n",
    "            kargs = {quality_i: \"*\" for quality_i in qualities} \n",
    "        elif isinstance(qualities,str):\n",
    "            kargs = {qualities: \"*\"}\n",
    "        elif isinstance(qualities,dict):\n",
    "            kargs = qualities\n",
    "        else:\n",
    "            kargs = {}\n",
    "            assert (qualities is None) or isinstance(qualities,dict), \"qualities should be given as either None, a str, a list, or a dict\"\n",
    "        if type is not None: kargs[\"type\"] = type\n",
    "        return self.__onto.search(iri= name, **kargs)\n",
    "    \n",
    "    def remove_entity(self, entity):\n",
    "        self.__ontology_backend.destroy_entity(entity)\n",
    "        \n",
    "    def remove_entities(self, entities):\n",
    "        for entity in entities:\n",
    "            self.remove_entity(entity)\n",
    "        \n",
    "    def show_entity_qualities(self,entity):\n",
    "        print(\"Entity:\",entity)\n",
    "        for prop in entity.get_properties():\n",
    "            print(\"{}:{}\".format(prop.name, prop[entity]))\n",
    "            \n",
    "    def has_quality(self, object, quality):\n",
    "        return getattr(self.__onto, quality) in object.get_properties()\n",
    "    \n",
    "    def has_qualities(self, object, qualities):\n",
    "        qualities = np.array(qualities).ravel()\n",
    "        return np.all([self.has_quality(object, quality_i) for quality_i in qualities])\n",
    "            \n",
    "    def sync_reasoner(self, **kargs):\n",
    "        \"\"\"Synchronise the reasoner.\n",
    "        \n",
    "        Parameters:\n",
    "        - **kargs:\n",
    "        |-infer_property_values\"\"\"\n",
    "        self.__ontology_backend.sync_reasoner(**kargs)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"Describe the knowledge framework\"\"\"\n",
    "        desc = [\"Geological Knowledge Framework:\"]\n",
    "        desc += [\" |- Name: {}\".format(self.name)]\n",
    "        desc += [\" |- Backend: {}\".format(self.__ontology_backend)]\n",
    "        desc += [\" |- Source: {}\".format(self.__source)]\n",
    "        desc += [\" |- Ontology: {}\".format(self.__onto)]\n",
    "        return \"\\n\".join(desc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "print(GeologicalKnowledgeManager().get_knowledge_framework())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our approach, geological datasets will be progressively interpreted in terms of structural objects,<br>\n",
    "based on a formal definition of concepts own by a **GeologicalKnowledgeManager**.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(type= mogi().Ponctual_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(type= mogi().Ponctual_Observation, qualities= [\"dip\",\"occurrence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(type= mogi().Ponctual_Observation, qualities= {\"dip\":45})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(qualities= {\"dip\":45})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(name=\"*D1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(name=\"D1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Spaces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distinguish two kind of operations here:\n",
    "* representation\n",
    "* visualisation\n",
    "\n",
    "A representation is a formal description of how something appears in a given representation space, but it doesn't have to be visualised.<br>\n",
    "A visualisation takes care of the rendering of a representation with a given support (image, screen)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation should also be made a bit more abstract.<br>\n",
    "1. There is a variety of object that can be rendered in a representation space (typically, different kinds of a dataset components)\n",
    "2. Several kinds of representation spaces could be envisionned (e.g., spatial 1D,2D,3D, or temporal, or just an abstract text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class RepresentationSpace(object):\n",
    "    \"\"\"A general framework for Representating geological objects\"\"\"\n",
    "    \n",
    "class TemporalRepresentationSpace(RepresentationSpace):\n",
    "    \"\"\"A `RepresentationSpace` representing temporal apsects of represented objects.\"\"\"\n",
    "    \n",
    "class PhysicalRepresentationSpace(RepresentationSpace):\n",
    "    \"\"\"A type of `RepresentationSpace` representing physical aspects of the represented objects.\"\"\"\n",
    "    \n",
    "    __default_coordinate_labels = [\"X\",\"Y\",\"Z\"]\n",
    "    \n",
    "    def __init__(self, dimension: int=None, coordinate_label: str|list= None, dataset= None, **kargs):\n",
    "        \"\"\"Initialisation of the representation space.\n",
    "        \n",
    "        Parameters:\n",
    "        - dimension (int): specify the number of dimensions of the representation space, typically 1D, 2D, or 3D (i.e., 1, 2, or 3),\n",
    "        NB: larger dimension spaces are not supported. At least either the `dimension` parameter or `coordinate_label` parameter should be given.\n",
    "        - coordinate_label(str|list(str)): gives the label(s) of the coordinates. If given, the number of dimensions is deduced from the size of the list\n",
    "        and `dimensions`is ignored, otherwise, the labels are taken from the `__default_coordinate_labels` based on the number of `dimension`s. \n",
    "        At least either the `dimension` parameter or `coordinate_label` parameter should be given.\n",
    "        - dataset: a Dataset object containing the data to be attached to this representation space.\n",
    "        Note that the RepresentationSpace can be created first and then updated automatically when creating the dataset attached to this space.\n",
    "        - **kargs:\n",
    "            - use_extension: if True, uses the extension of the dataset, else keeps the current ones\n",
    "            - padding: if use_extension is True, the given paddign will be used to keep a space around the dataset\n",
    "        \"\"\"\n",
    "        assert not (coordinate_label is None and dimension is None), \"At least one of the parameters should be specified\"\n",
    "        if coordinate_label is None:\n",
    "            assert isinstance(dimension, int),\"dimension parameter must be an integer\"\n",
    "            assert dimension in [1,2,3], \"The specified number of dimensions ({:d}) is not supported, should be 1, 2 or 3.\".format(dimension)\n",
    "            self.dimension= dimension\n",
    "            self.coordinate_labels= PhysicalRepresentationSpace.__default_coordinate_labels[:self.dimension]\n",
    "        elif isinstance(coordinate_label,str):\n",
    "            self.dimension= 1\n",
    "            self.coordinate_labels=  [coordinate_label]\n",
    "        elif isinstance(coordinate_label, list):\n",
    "            self.dimension= len(coordinate_label)\n",
    "            self.coordinate_labels= coordinate_label\n",
    "        else:\n",
    "            raise(\"Unsupported initialisation of representation space: dimension({}) and coordinate_label ({}).\\n At least one of the parameters shoudl be specified.\".format(dimension, coordinate_label))\n",
    "\n",
    "        self.__default_padding= 0.1\n",
    "        self.__datasets= []\n",
    "        self.set_extension()\n",
    "        self.attach_dataset(dataset,**kargs)\n",
    "        \n",
    "    def attach_dataset(self, dataset, use_extension= True, padding= None, **kargs):\n",
    "        \"\"\"Attach a dataset to the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - dataset: a Dataset object to be attached\n",
    "        - use_extension: if True, uses the extension of the dataset, else keeps the current ones\n",
    "        - padding: if use_extension is True, the given paddign will be used to keep a space around the dataset\"\"\"\n",
    "        if dataset == None: return\n",
    "        if dataset in self.__datasets: return\n",
    "        self.__datasets += [dataset]\n",
    "        \n",
    "        if use_extension:\n",
    "            self.set_extension_from_data(padding= padding)\n",
    "        \n",
    "        dataset.setup_representation_space(physical_space= self)\n",
    "    \n",
    "    def get_datasets(self):\n",
    "        \"\"\"dataset getter\"\"\"\n",
    "        return self.__datasets\n",
    "        \n",
    "    def set_extension(self, extension:list= None):\n",
    "        \"\"\"Setter for the extension (min,max) of the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - extension: a list containing a pair of min and max value for each dimension of the space.\n",
    "        If None, the default will be set, i.e., [[0,1]] * dimension\"\"\"\n",
    "        if extension is None: \n",
    "            self.extension = [[0,1]] * self.dimension\n",
    "            return\n",
    "        extension= np.array(extension)\n",
    "        assert extension.shape[0] == self.dimension, \"The specified extension ({}) do not match the space dimensions ({})\".format(extension, self.dimension)\n",
    "        assert extension.shape[1] == 2, \"The specified extension should provide both lower andupper bounds for each dimension, given: {}\".format(extension)\n",
    "        self.extension= extension\n",
    "        \n",
    "    def set_extension_from_data(self, padding= None):\n",
    "        \"\"\"Sets the extension of the space from the attached dataset\n",
    "        \n",
    "        If no dataset is attached yet, then default extension are used instead (min:0,max:1).\n",
    "        \n",
    "        Parameters:\n",
    "        - padding: a space that is left around the dataset, either a value compatible with the coordinates, or a list of values of same dimensions.\n",
    "        If None, by default the padding is 5% of the dataset range.\n",
    "        \"\"\"\n",
    "        non_empty_dataset = [data_i for data_i in self.__datasets if data_i.extension is not None]\n",
    "        if len(non_empty_dataset) == 0:\n",
    "            self.set_extension()\n",
    "            return\n",
    "        \n",
    "        if padding is None:\n",
    "            padding= self.__default_padding\n",
    "        else:\n",
    "            try: # check if padding as a dimension\n",
    "                len(padding)\n",
    "            # if not, then use it a a scaling \n",
    "            except TypeError: #just checking it is a number\n",
    "                assert type(padding) == int or type(padding) == float, \"padding should be given as a number (int or float), here: \"+type(padding)\n",
    "                # keep the padding as is in this case\n",
    "            else:# else check its dimensions are ok\n",
    "                assert len(padding) == self.dimension, \"the dimensions of the specified padding (len({})->) should match the space dimension ({})\".format(padding, len(padding), self.dimension)\n",
    "                padding= np.array(padding)\n",
    "                \n",
    "        extension = non_empty_dataset[0].extension\n",
    "        for data_i in non_empty_dataset[1:]:\n",
    "            for dim_i in self.dimension:\n",
    "                extension[dim_i,0] = min(extension[dim_i,0], data_i.extension[dim_i])\n",
    "                extension[dim_i,1] = max(extension[dim_i,1], data_i.extension[dim_i])\n",
    "        \"\"\":todo: use projected coordinates instead of source coordinates, might fail if 3D data projected on a map\"\"\"\n",
    "                \n",
    "        # in any cases, except when padding and data is None\n",
    "        self.center= np.mean(extension, axis= 1)\n",
    "        diff= extension.T - self.center\n",
    "        diff= diff.T\n",
    "        \n",
    "        # padding is multiplied by the width along each axis and added to the extension\n",
    "        # check if each dimension is shrinked (ie. min == max -> diff == 0), takes the average extension of the other dimensions,\n",
    "        # unless all are shrinked, in which case [[-1,1]] * self.dimension is set\n",
    "        if np.all(diff == 0):\n",
    "            diff = np.array([[-1,1]] * self.dimension)\n",
    "        else:\n",
    "            zero_diff = np.where(~diff.any(axis=1))\n",
    "            non_zero_diff = np.where(diff.any(axis=1))\n",
    "            mean_diff = np.mean(diff[non_zero_diff], axis=0)\n",
    "            diff[zero_diff] = mean_diff\n",
    "        \n",
    "        extension= np.repeat([self.center],2,axis=0).T + (1+2*padding)*diff \n",
    "        \n",
    "        self.set_extension(extension)\n",
    "        \n",
    "    def filter_qualities(self,**qualities):\n",
    "        \"\"\"removes the named arguments corresponding to this space coordinates from qualities.\n",
    "        \n",
    "        Return:\n",
    "        - a copy with the passed parameters except for the ones correpsonding to the space coordinates\"\"\"\n",
    "        return {key:val for key, val in qualities.items() if key not in self.coordinate_labels}\n",
    "    \n",
    "    def prepare_coordinate_qualities(self, **qualities):\n",
    "        \"\"\"transforms the coordinates passed in qualities into an appropriate format\n",
    "        \n",
    "        Return:\n",
    "        - a dict with quality keys and values for setting coordinates\"\"\"\n",
    "        coordinates = {key:val for key, val in qualities.items() if key in self.coordinate_labels}\n",
    "        coord_qualities = {}\n",
    "        for i,key in enumerate(self.coordinate_labels):\n",
    "            if key in coordinates:\n",
    "                coord_qualities[\"coord{}\".format(i+1)] = coordinates[key]\n",
    "                coord_qualities[\"coord{}_label\".format(i+1)] = [key]\n",
    "        return coord_qualities\n",
    "    \n",
    "    def label_map(self, object):\n",
    "        \"\"\"creates of mapping of coordinate labels\"\"\"\n",
    "        coord_label_params = [\"coord{}_label\".format(i) for i in range(1,4) if hasattr(object, \"coord{}_label\".format(i))]\n",
    "        label_map = {getattr(object,param)[0]:param for param in coord_label_params if getattr(object,param)[0] in self.coordinate_labels}\n",
    "        return label_map\n",
    "        \n",
    "    def set_object_coordinates(self, object, **kargs):\n",
    "        \"\"\"Sets the coordinates corresponding to this space into the given object\n",
    "        \n",
    "        Parameters:\n",
    "        - object: the object whose coordinates needs to be set\n",
    "        - kargs: keyword argugments corresponding to the name and values of the coordinates.\n",
    "        They must match this space coordinate names, extra names will be ignored\"\"\"\n",
    "        \n",
    "        # check if the object has coord{i}_label set, else set it\n",
    "        if mogi.has_qualities(object, [\"coord{}_label\".format(i) for i in range(1,len(self.coordinate_labels)+1)]):\n",
    "            label_map = self.label_map(object)\n",
    "            coordinate_values = {label_map[key].split(\"_\")[0]:kargs[key] for key in self.coordinate_labels if key in kargs}\n",
    "        else:\n",
    "            coordinate_values = self.prepare_coordinate_qualities(**kargs)\n",
    "        for param_name, val in coordinate_values.items():\n",
    "            setattr(object, param_name, val)\n",
    "            \n",
    "    def get_object_coordinates(self, object):\n",
    "        \"\"\"Gets the coordinates corresponding to this space from the given object\"\"\"\n",
    "        label_map = self.label_map(object)\n",
    "        coord = np.full_like(self.coordinate_labels, np.nan, dtype= float)\n",
    "        for i, key in enumerate(self.coordinate_labels):\n",
    "            if key in label_map:\n",
    "                coord_param = label_map[key].split(\"_\")[0]\n",
    "                coord[i] = getattr(object,coord_param)[0]\n",
    "        return coord\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Description of the physical space parameters and data\n",
    "        \"\"\"\n",
    "        desc= [\"Representation space of type: {:s}\".format(type(self).__name__)]\n",
    "        desc+= [\"- Number of dimension(s): {}\".format(self.dimension)]\n",
    "        desc+= [\"- Coordinate label(s): {}\".format(self.coordinate_labels)]\n",
    "        desc+= [\"- Space extension:\"]\n",
    "        for dim_i, lim_i in zip(self.coordinate_labels,self.extension):\n",
    "            desc+= [\" |- Coord {}: {}\".format(dim_i, lim_i)]  \n",
    "        return \"\\n\".join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(2)\n",
    "\n",
    "space.set_extension_from_data(padding= None)\n",
    "\n",
    "print(space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhysicalRepresentationSpace(coordinate_label= \"depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: PhysicalRepresentationSpace()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(coordinate_label=[\"X\",\"Y\"])\n",
    "print(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(coordinate_label=[\"X\",\"Z\"])\n",
    "print(space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "* visualisation of given data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class GeologicalDataset(object):\n",
    "    \"\"\"A GeologicalDataset gathers information about geological data to be interpreted.\n",
    "    \n",
    "    This class is a hybrid ontology&python class. It is providing pythonic algorithm and high level interface,\n",
    "    while the data is actually stored in an ontology.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, physical_space= None, time_space= None, representation_spaces= None, knowledge_framework= None):\n",
    "        \"\"\"Initialises a `GeologicalDataset`\n",
    "        \n",
    "        Parameters:\n",
    "        - physical_space: a `PhysicalRepresentationSpace`,  which defines the spatial coordinates of this dataset\n",
    "        - time_space: a `TemporalRepresentationSpace`,  which defines the time coordinates of this dataset\n",
    "        - representation_spaces: list of `Representationspace`s to which the dataset must be attached\n",
    "        Note: datasets can be created without representation space and attached later on by using `RepresentationSpace.attach_dataset`\n",
    "        or `GeologicalDataset.setup_representation_space`.\n",
    "        Alternativelly, a single `PhysicalRepresentationspace` and or `TemporalRepresentationspace` can be given here if `physical_space` and `time_space` are None.\n",
    "        - default_representation_space: the main RepresentationSpace to which this dataset is attached.\n",
    "        If None and representation_spaces are provided, then the first one will be taken.\n",
    "        If the default one is not initially in the full list, then it is added to it.\n",
    "        - ontology: the name of the ontology to be used for storing the data.\n",
    "        If None, the default will be taken from `the GeologicalKnowledgeManager`.\n",
    "        \n",
    "        Internals: this method initialises several internal attributes:\n",
    "        - extension: represents the extension of the dataset in the attached representation space\n",
    "        (i.e., the default on if this dataset is represented in several representation spaces\n",
    "        - representation_spaces: the dataset can be attached to and represented into several representation spaces,\n",
    "        `physical_space` and `time_space` are included into this list.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.knowledge_framework= knowledge_framework if knowledge_framework is not None else GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "        \n",
    "        self.extension = None\n",
    "        \n",
    "        # setup representation spaces\n",
    "        self.representation_spaces= set()\n",
    "        if physical_space is None:\n",
    "            # try to initialise the physical space with first existing data\n",
    "            observations = self.get_observations()\n",
    "            if len(observations) > 0:\n",
    "                coord_labels = [getattr(observations[0],\"coord{}_label\".format(i)) for i in range(1,4)]\n",
    "                coord_labels = [label[0] for label in coord_labels if len(label)>0]\n",
    "                physical_space = PhysicalRepresentationSpace(coordinate_label=coord_labels)\n",
    "        self.setup_representation_space(physical_space, time_space, representation_spaces)\n",
    "        \n",
    "        # register the datast in the listed representation spaces\n",
    "        for space_i in self.representation_spaces:\n",
    "            space_i.attach_dataset(self)\n",
    "        \n",
    "        # initialize extension of the dataset\n",
    "        self.update_extension()\n",
    "            \n",
    "    def __dell__(self):\n",
    "        self.remove_all_observations()\n",
    "\n",
    "    def __setup_space(self, space, space_type):\n",
    "        \"\"\"Check if space of given type is in list or parameter and return the appropriate value.\n",
    "        \n",
    "        Take the given physical/time space, or if None use the first one in the list, and if none just leave None.\n",
    "        Adds the space to the `self.representation_spaces` set.\"\"\"\n",
    "        if space is not None: \n",
    "            self.representation_spaces.add(space)\n",
    "            return space\n",
    "        if len(self.representation_spaces) == 0: return None\n",
    "        space_list= [space_i for space_i in self.representation_spaces if isinstance(space_i, space_type)]\n",
    "        return space_list[0] if len(space_list) > 0 else None\n",
    "    \n",
    "    def update_extension(self, update_representation_spaces= True):\n",
    "        \"\"\"Sets the extension of the dataset in the physical space from existing observations\n",
    "        \n",
    "        If there is no observation, `self.extension` is set to None\"\"\"\n",
    "        observations = self.get_observations()\n",
    "        if (len(observations) == 0) or (self.physical_space is None):\n",
    "            self.extension = None \n",
    "            return\n",
    "            \n",
    "        di = observations[0]\n",
    "        coord = self.physical_space.get_object_coordinates(di)\n",
    "        self.extension = np.repeat([coord],2,axis=0).T\n",
    "        for di in observations[1:]:\n",
    "            coord = self.physical_space.get_object_coordinates(di)\n",
    "            for j, val in enumerate(coord):\n",
    "                self.extension[j,0] = min(self.extension[j,0], val)\n",
    "                self.extension[j,1] = max(self.extension[j,1], val)\n",
    "                \n",
    "        if update_representation_spaces:\n",
    "            if self.physical_space is not None:\n",
    "                self.physical_space.set_extension_from_data()\n",
    "        \n",
    "    def setup_representation_space(self, physical_space= None, time_space= None, representation_spaces= None):\n",
    "        \"\"\"Setup the representation space list and default\n",
    "        \n",
    "        Parameters:\n",
    "        - physical_space: a `PhysicalRepresentationSpace`,  which defines the spatial coordinates of this dataset\n",
    "        - time_space: a `TemporalRepresentationSpace`,  which defines the time coordinates of this dataset\n",
    "        - representation_spaces: list of `Representationspace`s to which the dataset must be attached\n",
    "        Note: datasets can be created without representation space and attached later on by using `RepresentationSpace.attach_dataset`\n",
    "        or `GeologicalDataset.setup_representation_space`.\n",
    "        Alternativelly, a single `PhysicalRepresentationspace` and or `TemporalRepresentationspace` can be given here if `physical_space` and `time_space` are None.\n",
    "        \"\"\"\n",
    "        if representation_spaces is not None: self.representation_spaces = self.representation_spaces.union(representation_spaces)\n",
    "        self.physical_space = self.__setup_space(physical_space, PhysicalRepresentationSpace)\n",
    "        self.time_space = self.__setup_space(time_space, TemporalRepresentationSpace)\n",
    "        \n",
    "        for space in self.representation_spaces:\n",
    "            space.attach_dataset(self)\n",
    "        \n",
    "    def get_observations(self, observation_type= None, qualities= None, name= None):\n",
    "        \"\"\"Accessor to the observations stored in the internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - observation_type: the type of the searched observations as defined by the internal ontology\n",
    "        - qualities: qualities to filter the observations (c.f. `KnowledgeFramework.search`)\n",
    "        - name: name of the serached observation (c.f. `KnowledgeFramework.search`)\"\"\"\n",
    "        observation_type = observation_type if observation_type is not None else self.knowledge_framework().Ponctual_Observation\n",
    "        return self.knowledge_framework.search(type= observation_type, qualities= qualities, name= name)\n",
    "    \n",
    "    def get_occurrence_observations(self):\n",
    "        \"\"\"helper method to access occurrence data, i.e., those having a occurrence quality\n",
    "        \n",
    "        :todo: for now the occurrence quality doesn't exist so all the observations are occurrence by default\"\"\"\n",
    "        return self.get_observations(qualities= \"occurrence\")\n",
    "    \n",
    "    def get_orientation_observations(self):\n",
    "        return self.get_observations(qualities= \"dip\")\n",
    "    \n",
    "    def remove_observation(self, observations, update_extension= True):\n",
    "        \"\"\"Removes the given observations stored in this dataset and internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - observations: an iterable containing objects of the internal ontology.\n",
    "        Note that you can use the `search`method to get such a list\n",
    "        - update_extension: if True (default) the extension will be updated\"\"\"\n",
    "        self.knowledge_framework.remove_entities(observations)\n",
    "        if update_extension: self.update_extension()\n",
    "            \n",
    "    def remove_observation_by_name(self, name:str, update_extension= True):\n",
    "        \"\"\"Removes the given observations stored in this dataset and internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation to be removed\n",
    "        - update_extension: if True (default) the extension will be updated\"\"\"\n",
    "        self.remove_observation(self.get_observations(name= name), update_extension)\n",
    "        \n",
    "    def remove_all_observations(self, update_extension= True):\n",
    "        \"\"\"Removes all the observations stored in this dataset and internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        Note: the update is performed only once at the end.\"\"\"\n",
    "        self.remove_observation(self.get_observations(), update_extension= False)\n",
    "        if update_extension: self.update_extension()\n",
    "        \n",
    "    def add_observation(self, name: str, update_extension= True, **kargs):\n",
    "        \"\"\"creates a new observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\n",
    "          |   Also Note: the arguments with None value are removed from the dict\"\"\"\n",
    "        if name == \"\":\n",
    "            logging.warning(\"Trying to create an observation with a blank name, passing None instead.\")\n",
    "            name = None\n",
    "        if not isinstance(name,str):\n",
    "            logging.warning(\"Trying to create an observation with a name that is not a string, passing None instead.\")\n",
    "            name = None\n",
    "        assert self.physical_space is not None, \"Trying to add observation while physical space is not set. Please setup_representation_space first\"\n",
    "        \n",
    "        qualities = {key:[val] for key, val in kargs.items() if val is not None}\n",
    "        non_coordinate_qualities = self.physical_space.filter_qualities(**qualities)\n",
    "        \n",
    "        new_observation = self.knowledge_framework().Ponctual_Observation(name= name, **non_coordinate_qualities)\n",
    "        self.physical_space.set_object_coordinates(new_observation,**qualities)\n",
    "        if update_extension: self.update_extension()\n",
    "        \n",
    "    def add_occurrence_observation(self, name: str, observed_object:str, occurrence= True, update_extension= True, **kargs):\n",
    "        \"\"\"creates a new occurrence observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - observed_object: the name of the observed object\n",
    "        - occurrence: True (default) if the object was observed here, False if it was observed that it is not there.\n",
    "        Note that this is different from not having observed that it is here, in which case there should not be an observation.\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\"\"\"\n",
    "        if observed_object is not None: kargs[\"geology\"]= observed_object\n",
    "        if occurrence is not None: kargs[\"occurrence\"]= occurrence\n",
    "        self.add_observation(name, update_extension= update_extension, **kargs)\n",
    "    \n",
    "    def add_orientation_observation(self, name: str, observed_object:str, dip, dip_dir, occurrence= True, update_extension= True, **kargs):\n",
    "        \"\"\"creates a new orientation observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - observed_object: the name of the observed object\n",
    "        - dip: the value of the measured dip (in degrees, 0-90)\n",
    "        - dip_dir: the value of the dip direction (in degrees, 0-360, from North towards the East)\n",
    "        - occurrence: True (default) if the object was observed here.\n",
    "        This is the default behaviour because if the measurement was made here, we assume that the object actually existed\n",
    "        so this is in itself a proof ox occurrence. However, one might want to record the orientation without specifically attaching any\n",
    "        observation of occurrence, in which case None should be given for occurrence and the quality won't be set.\n",
    "        False, would not make much sense as it would imply that the orientation was measured but we observed that the object wasn't there.\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\"\"\"\n",
    "        if occurrence is not None: kargs[\"occurrence\"]= occurrence\n",
    "        if occurrence == False: logging.warning(\"occurrence parameter was set to False while adding an orientation observation.\"\\\n",
    "            \"This is weird because it would imply the measure was taken but the rock couldn't be observed.\"\\\n",
    "            \"Did you intend to avoid recording the occurrence, in which case you should prefer None isntead of False.\")\n",
    "        \n",
    "        if observed_object is not None: kargs[\"geology\"]= observed_object\n",
    "        if (dip is not None) and (dip_dir is not None):\n",
    "            kargs[\"dip\"]= dip\n",
    "            kargs[\"dip_dir\"]= dip_dir\n",
    "        self.add_observation(name, update_extension= update_extension, **kargs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.get_observations())\n",
    "         \n",
    "    def __str__(self):\n",
    "        \"\"\"Description of the dataset\n",
    "        \"\"\"\n",
    "        desc= [\"A dataset of type: {:s}\".format(type(self).__name__)]\n",
    "        n = len(self)\n",
    "        if n == 0:\n",
    "            desc+= [\"- The dataset is empty\"]\n",
    "        else:\n",
    "            desc+= [\"- Size of dataset: {:d}\".format(n)]\n",
    "            desc+= [\"- Types of data:\"]\n",
    "            desc+= [\" |- occurrence:\\t{} entries\".format(len(self.get_occurrence_observations()))]\n",
    "            desc+= [\" |- orientation:\\t{} entries\".format(len(self.get_orientation_observations()))]\n",
    "        if self.extension is None:\n",
    "            desc+= [\"- Extension: \"+str(self.extension)]\n",
    "        else:\n",
    "            desc+= [\"- Extension:\"]\n",
    "            labels = self.physical_space.coordinate_labels if self.physical_space is not None else [\"Coord_{}\".format(i) for i in range(3)][:len(self.extension)]\n",
    "            for dim_i, lim_i in zip(labels,self.extension):\n",
    "                desc+= [\" |- Coord {}: [{}, {}]\".format(dim_i, *lim_i)]  \n",
    "        desc+= [\"- Number of spaces this dataset is attached to: {:d}\".format(len(self.representation_spaces))]\n",
    "        if self.physical_space is None:\n",
    "            desc+= [\" |- Physical space: None\"]\n",
    "        else:\n",
    "            desc+= [\" |- Physical space:\"+\"\\n | |\".join(self.physical_space.__str__().split(\"\\n\"))]\n",
    "        if self.time_space is None:\n",
    "            desc+= [\" |- Temporal space: None\"]\n",
    "        else:\n",
    "            desc+= [\" |- Temporal space:\"+\"\\n | |\".join(self.time_space.__str__().split(\"\\n\"))]\n",
    "        return \"\\n\".join(desc) \n",
    "    \n",
    "    def head(self, n:int= 5):\n",
    "        \"\"\"Returns the `n`first data in the dataset\"\"\"\n",
    "        return self.to_dataframe(max_rows=n)\n",
    "    \n",
    "    def info(self):\n",
    "        \"\"\"Returns a description of the dataset\"\"\"\n",
    "        return self.__str__()\n",
    "    \n",
    "    def to_dataframe(self, max_rows:int= None):\n",
    "        \"\"\"Creates a `pandas.DataFrame` showing the data in this dataset\n",
    "        \n",
    "        Parameters:\n",
    "        - max_rows: limits the number of rows in the output, unless None is given (default).\n",
    "        Note: interanlly, all the observations are still recovered from the internal ontology, \n",
    "        but only the `max_rows`first ones are show for consision.\n",
    "        \"\"\"\n",
    "        columns = [\"name\"]\n",
    "        if self.physical_space is not None:\n",
    "            columns += self.physical_space.coordinate_labels\n",
    "        if self.time_space is not None:\n",
    "            columns += self.time_space.coordinate_labels\n",
    "        columns += [\"dip_dir\",\"dip\",'geology', 'occurrence']\n",
    "        output_frame = pd.DataFrame(columns= columns)\n",
    "        output_frame.set_index(\"name\",inplace=True)\n",
    "        \n",
    "        observations = self.get_observations() \n",
    "        observations = observations if max_rows is None else observations[:max_rows]\n",
    "        for di in observations:\n",
    "            for prop in di.get_properties():\n",
    "                if prop.name == '': continue\n",
    "                if \"coord\" in prop.name: continue\n",
    "                output_frame.loc[di.name,prop.name] = prop[di][0]\n",
    "                \n",
    "            if self.physical_space is not None:\n",
    "                for label, val in zip(self.physical_space.coordinate_labels, self.physical_space.get_object_coordinates(di)):\n",
    "                    output_frame.loc[di.name,label] = val\n",
    "            if self.time_space is not None:\n",
    "                for label, val in zip(self.time_space.coordinate_labels, self.time_space.get_object_times(di)):\n",
    "                    output_frame.loc[di.name,label] = val\n",
    "        coordinate_types = {label:float for label in self.physical_space.coordinate_labels} if self.physical_space is not None else {}\n",
    "        time_types = {label:float for label in self.physical_space.coordinate_labels} if self.time_space is not None else {}\n",
    "        other_types = {'dip_dir':float, 'dip':float, 'geology':str, \"occurrence\": bool}\n",
    "        output_frame = output_frame.astype({**coordinate_types, **other_types})\n",
    "        return output_frame\n",
    "    \n",
    "def load_dataset_from_csv(source:str, dataset:GeologicalDataset = None, coordinate_labels = [\"X\",\"Y\",\"Z\"], **kargs) ->GeologicalDataset:\n",
    "    \"\"\"Loads a dataset from a csv file\n",
    "    \n",
    "    Parameters:\n",
    "    - source(str): the source file from which the data should be loaded\n",
    "    - dataset: the `GeologicalDataset` in which the data will be loaded. If None, the dataset will be created.\n",
    "    - **arkgs: passed to pandas.read_csv\n",
    "    \n",
    "    Return:\n",
    "    - the `GeologicalDataset` with the newly loaded data (a new `GeologicalDataset` is created if needed).\"\"\"\n",
    "    try:\n",
    "        dataframe = pd.read_csv(source, **filter_kargs(pd.read_csv,**kargs))\n",
    "    except Exception as e:\n",
    "        raise( Exception(\"This error occurred while loading a dataset from: {}\\nAdditional arguments were given: {}\".format(source,\n",
    "                            \",\".join([\"{}:{}\".format(key,val) for key, val in kargs.items()]))))\n",
    "        \n",
    "    coordinate_labels = [label for label in coordinate_labels if label in dataframe.columns]\n",
    "    if len(coordinate_labels) == 0:\n",
    "        logging.warning(\"There isn't any coordinate column in the loaded dataset.\\nCheck the output and consider changing the separator with sep keyword or cahnge 'coordinate_label' parameter.\")\n",
    "    return load_dataset_from_dataframe(dataframe, dataset, coordinate_labels= coordinate_labels, **filter_kargs(load_dataset_from_dataframe,**kargs) )\n",
    "\n",
    "def load_dataset_from_dataframe(dataframe, dataset:GeologicalDataset = None, coordinate_labels = [\"X\",\"Y\",\"Z\"], labels= None, index= None, dtypes= None, clear_existing_data= True):\n",
    "    \"\"\"Loads a dataset from a `pandas.DataFrame`\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe(`pandas.DataFrame`): the source dataframe from which the data should be loaded\n",
    "    - dataset: the `GeologicalDataset` in which the data will be loaded. If None, the dataset will be created.\n",
    "    - coordinate_labels: the labels to be used as coordinates of the physical space\n",
    "    - labels: a dict to relabel the dataframe columns prior to loading in the dataset.\n",
    "    This is usefull for example when the coordinates in the source aren't labelled the same as in the internal ontology.\n",
    "    The format is {\"old_label\":\"new_label\", ...}.\n",
    "    - index: the label of the column (in original DataFrame, i.e., before renaming), which is to be used as index\n",
    "    - dtypes: a dict containing a mapping between column name and type\n",
    "    - clear_existing_data: if set (default), any oservation stored in the internal ontology is removed prior to loading the new dataset \n",
    "    \n",
    "    Return:\n",
    "    - the `GeologicalDataset` with the newly loaded data (a new `GeologicalDataset` is created if needed).\"\"\"\n",
    "    if dataset is None:\n",
    "        physical_space = PhysicalRepresentationSpace(coordinate_label= coordinate_labels)\n",
    "        dataset = GeologicalDataset(physical_space = physical_space)\n",
    "    else:\n",
    "        if dataset.physical_space is None or clear_existing_data:\n",
    "            physical_space = PhysicalRepresentationSpace(coordinate_label= coordinate_labels)\n",
    "            dataset.setup_representation_space(physical_space = physical_space)\n",
    "        else:\n",
    "            if dataset.physical_space.coordinate_labels != coordinate_labels:\n",
    "                raise MalissiaBaseError(\"Trying to add data into an existing dataset with different coordiante labels\")\n",
    "                \n",
    "    if clear_existing_data:\n",
    "        dataset.remove_all_observations()\n",
    "\n",
    "    # declare default dtypes here, in case they should be relabelled\n",
    "    default_coord_types = {key:float for key in dataset.physical_space.coordinate_labels}\n",
    "    other_types = {'dip_dir':float, 'dip':float, 'geology':str, 'observed_object':str, \"occurrence\":bool}\n",
    "    dtypes= dtypes if dtypes is not None else {**default_coord_types, **other_types}\n",
    "    assert isinstance(dtypes, dict), \"dtypes for type management should be given as a dict\"\n",
    "    \n",
    "    # relabelling\n",
    "    if labels is not None:\n",
    "        dataframe = dataframe.rename(columns= labels)\n",
    "        index= labels[index] if index in labels else index\n",
    "        dtypes= {labels[key] if key in labels else key: value for key, value in dtypes.items()}\n",
    "    \n",
    "    # setting the index\n",
    "    if index is not None:\n",
    "        # if already set, reset it\n",
    "        if type(dataframe.index) != pd.core.indexes.base.Index:\n",
    "            dataframe = dataframe.reset_index()\n",
    "        # then set the index\n",
    "        try:\n",
    "            dataframe = dataframe.set_index(index)\n",
    "        except Exception as e:\n",
    "            raise( Exception(\"Error while setting the index of the loaded dataset:\\nThis is how the dataframe looks like:\\n\"+dataframe.head().to_string()))\n",
    "    \n",
    "    for extra_type_i in dtypes.keys() - set(dataframe.columns):\n",
    "        del dtypes[extra_type_i]\n",
    "    dataframe = dataframe.astype(dtypes)\n",
    "        \n",
    "    for name_i, values_i in dataframe.iterrows():\n",
    "        # drop nan values and filter None objects\n",
    "        values_i = {key:val for key, val in values_i.dropna().items() if val is not None}\n",
    "        dataset.add_observation(name_i, **values_i)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def filter_kargs(target_function,**kargs):\n",
    "    \"\"\"Helper function to filter keyword arguments and only pass the needed ones in a function signature\"\"\"\n",
    "    sig = inspect.signature(target_function)\n",
    "    # check if there is a **kargs in the signature of the function, if yes it is ok as it will take care of the passed extra kargs\n",
    "    if not any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values()):\n",
    "        extra_args = kargs.keys() - sig.parameters.keys()\n",
    "        for args in extra_args:\n",
    "            del kargs[args]\n",
    "    return kargs\n",
    "\n",
    "def debuf_karg_filter(target_function, **kargs):\n",
    "    print(\"Kargs filtering:\")\n",
    "    print(\"before: {\",*[\"{}:{}\".format(key,val) for key, val in kargs.items()],\"}\")\n",
    "    print(\"after: {\",\",\".join([\"{}:{}\".format(key,val) for key, val in filter_kargs(target_function,**kargs).items()]),\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filter_kargs(pd.read_csv, **{\"sep\":\";\", \"truc\":\"test\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= GeologicalDataset()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_occurrence_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_entity_qualities(dataset.get_observations(name=\"Dtest\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_observation_by_name(\"D8\")\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_all_observations()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(coordinate_label= [\"x\",\"y\",\"z\"])\n",
    "dataset.setup_representation_space(physical_space= physical_space)\n",
    "dataset.add_occurrence_observation(name=\"DD\", observed_object= \"Keuper\", occurrence= True, x= 1, z= 2, y= 0)\n",
    "dataset.add_occurrence_observation(name=\"DN\", observed_object= \"Keuper\", occurrence= False, x= 3, z= 2, y= 0)\n",
    "dataset.update_extension()\n",
    "print(dataset)\n",
    "\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_orientation_observation(name=\"DO\", observed_object= \"Trias\", dip= 30, dip_dir= 270, x= 2, z= 2, y= 0)\n",
    "print(dataset)\n",
    "\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_from_csv(\"../inputs/data_for_paper.csv\", sep=\";\", index= \"Id\", coordinate_labels=[\"x\",\"y\",\"z\"], labels={\"ID\":\"Id\", \"strike\":\"dip_dir\",\"name\":\"observed_object\"})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrates errors because of wrong separator\n",
    "try:\n",
    "    dataset2= load_dataset_from_csv(\"../inputs/data_for_paper.csv\", labels={\"ID\":\"Id\", \"strike\":\"dip_dir\",\"name\":\"observed_object\"})\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    gkf = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    observations = gkf.search(type=gkf().Ponctual_Observation)\n",
    "    gkf.remove_entities(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrates errors when leaving wrong labels\n",
    "try:\n",
    "    dataset3 = load_dataset_from_csv(\"../inputs/data_for_paper.csv\", sep=\";\", index= \"ID\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    gkf = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    observations = gkf.search(type=gkf().Ponctual_Observation)\n",
    "    gkf.remove_entities(observations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset\n",
    "\n",
    "Data are actually described within the ontology, here thanks to the *Data* class.<br>\n",
    "Adding new data points calls for creating new *Data* individuals (i.e., instances in the ontology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_head = np.array(['name', 'x', 'y', 'z', 'dip_dir', 'dip', 'observed_object','occurrence'])\n",
    "data_array = np.array([['D1', 15, 20, 35, 270, 45, 'Trias_Base',True],\n",
    "                       ['D2', 30, 25, 50, 270, 45, 'Trias_Base',True],\n",
    "                       ['D3', 60, 30, 40, 90, 45, 'Trias_Base',True],\n",
    "                       ['D4', 75, 15, 25, 90, 45, 'Trias_Base',True],\n",
    "                       ['D5', 110, 20, 40, 270, 63, 'Trias_Base',True],\n",
    "                       ['D6', 120, 20, 60, 270, 64, 'Trias_Base',True],\n",
    "                       ['D7', 155, 20, 60, 89, 39, 'Trias_Base',True],\n",
    "                       ['D8', 190, 20, 30, 91, 40, 'Trias_Base',True],\n",
    "                       ['D11', 25, 22, 45, np.nan, np.nan, None ,True],\n",
    "                       ['D22', 50, 22, 50, np.nan, np.nan, None,True],\n",
    "                       ['D44', 100, 30, 20, np.nan, np.nan, None,True],\n",
    "                       ['D77', 168, 30, 47, np.nan, np.nan, None,True]]\n",
    ")\n",
    "data_test = pd.DataFrame(data = data_array, columns = data_head)\n",
    "data_test = data_test.astype({'name':str, 'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'observed_object':str, 'occurrence':bool})\n",
    "data_test.set_index(\"name\", inplace = True)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset_from_dataframe(data_test, coordinate_labels= [\"x\",\"y\",\"z\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "chart = HTML(dataset.info().replace(\"\\n\",\"<br>\"))\n",
    "display(chart)\n",
    "\n",
    "chart = HTML(dataset.to_dataframe().to_html())\n",
    "display(chart)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(center, dip, dir, length= 1, ax= None, color = \"black\", **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "\n",
    "    center = np.array(center)\n",
    "    dip_rad = np.deg2rad(dip)\n",
    "    vec_x =  np.cos(dip_rad)\n",
    "    if dir == \"left\": vec_x *= -1\n",
    "    vec_z = -np.sin(dip_rad)\n",
    "    vect = 0.5 * length * np.array([vec_x,vec_z])\n",
    "    start = center - vect\n",
    "    end = center + vect\n",
    "    ax_plt.plot([start[0],end[0]],[start[1],end[1]], color = color, **kargs)\n",
    "    \n",
    "    return vect\n",
    "    \n",
    "def draw_dip_symbol(center, dip, dir, length= 1, polarity= None, ax= None, color = \"black\", polarity_ratio= 0.4, **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "    \n",
    "    vect = draw_line(center= center, dip= dip, dir= dir, length= length, ax= ax_plt, color = color, **kargs)\n",
    "    \n",
    "    if polarity is not None:\n",
    "        vect_pol = polarity_ratio * np.array([-vect[1],vect[0]])\n",
    "        if (dir == \"left\" and polarity == \"up\") or (dir == \"right\" and polarity == \"down\") : vect_pol *= -1\n",
    "        ax_plt.arrow(*center,*vect_pol, width=length/100, color = color, **kargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_line([0,0],30, \"left\")\n",
    "draw_dip_symbol([0,1],60, \"right\", polarity= \"up\", color= \"red\" )\n",
    "plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class Rendering(object):\n",
    "    \"\"\"Class dedicated to rendering a `RepresentationSpace`\"\"\"\n",
    "    \n",
    "    def __init__(self, space:RepresentationSpace):\n",
    "        \"\"\"Creating a rendering for the given `RepresentationSpace`\"\"\"\n",
    "        self.space = space\n",
    "\n",
    "class AxisAlignedCrossSection(Rendering):\n",
    "    \"\"\"A specialised `Rendering` that procudes a cross-section.\n",
    "    \n",
    "    The cross-section is defined by two coordinates of the rendered `PhysicalRepresentationSpace`\"\"\"\n",
    "    \n",
    "    def __init__(self, space:PhysicalRepresentationSpace, u= None, v= None, ax= None):\n",
    "        \"\"\"Creates a cross section through the given physical space\n",
    "        \n",
    "        Parameters:\n",
    "        - space: the space through which the cross section is going.\n",
    "        Note: the given space must be a `PhysicalRepresentationSpace` and have two or more coordinates\n",
    "        - u: the label of the abscissa axis among the physical space coordinates.\n",
    "        By default, if None is given, the first axis of the space is used.\n",
    "        - v: the label of the ordinate axis among the physical space coordinates.\n",
    "        By default, if None is given, the last axis of the space is used,\n",
    "        effectively using both coordinates if the space is 2D, but a vertical cross-section if it is 3D.\n",
    "        - ax: the matplotlib axis in which the space is to be rendered.\n",
    "        If None (default), then a new axis will be created.\"\"\"\n",
    "        assert isinstance(space,PhysicalRepresentationSpace), \"The given representation space must be a PhysicalRepresentationSpace, here: \"+str(type(space))\n",
    "        assert space.dimension > 1, \"Cross sections are only possible through spaces of dimensions >=2, here: \"+str(space.dimension)\n",
    "        self.space = space\n",
    "        \n",
    "        if u is None:\n",
    "            self.u = self.space.coordinate_labels[0]\n",
    "            self.u_index = 0\n",
    "        else:\n",
    "            assert u in self.space.coordinate_labels, \"The first given coordinate (u={}) is not in the represented space ({})\".format(u, \",\".join(space.coordinate_labels))\n",
    "            self.u = u\n",
    "            self.u_index = np.argwhere(self.space.coordinate_labels == self.u)[0,0]\n",
    "        \n",
    "        if v is None:\n",
    "            self.v = space.coordinate_labels[-1]\n",
    "            self.v_index = len(space.coordinate_labels) - 1\n",
    "        else:\n",
    "            assert v in space.coordinate_labels, \"The second given coordinate (v={}) is not in the represented space ({})\".format(v, \",\".join(space.coordinate_labels))\n",
    "            self.v = v\n",
    "            self.v_index = np.argwhere(self.space.coordinate_labels == self.v)[0,0]\n",
    "            \n",
    "        self.plt_ax = None\n",
    "        \n",
    "    def setup_drawing(self, ax= None):\n",
    "            if ax is not None:\n",
    "                self.plt_ax = ax\n",
    "            elif self.plt_ax is None:\n",
    "                self.plt_ax = plt\n",
    "            \n",
    "            ax = self.plt_ax.gca() if self.plt_ax == plt else self.plt_ax\n",
    "            ax.set_aspect(\"equal\")\n",
    "            ax.set_xlim( *self.space.extension[self.u_index])\n",
    "            ax.set_xlabel(self.u)\n",
    "            ax.set_ylim( *self.space.extension[self.v_index])\n",
    "            ax.set_ylabel(self.v)\n",
    "    \n",
    "    def draw_line(self, center, dip, dir, length= 1, color = \"black\", setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        center = np.array(center)\n",
    "        dip_rad = np.deg2rad(dip)\n",
    "        vec_x =  np.cos(dip_rad)\n",
    "        if dir == \"left\": vec_x *= -1\n",
    "        vec_z = -np.sin(dip_rad)\n",
    "        vect = 0.5 * length * np.array([vec_x,vec_z])\n",
    "        start = center - vect\n",
    "        end = center + vect\n",
    "        self.plt_ax.plot([start[0],end[0]],[start[1],end[1]], color = color, **kargs)\n",
    "        \n",
    "        return vect        \n",
    "    \n",
    "    def draw_dip_symbol(self, center, dip, dir, length= None, polarity= None, color = \"black\", polarity_ratio= 0.4, setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        if length is None:\n",
    "            length = np.abs(np.max(self.space.extension[:,1] - self.space.extension[:,0])) / 20\n",
    "        \n",
    "        vect = self.draw_line(center= center, dip= dip, dir= dir, length= length, color = color, setup_drawing=False, ax=ax, **kargs)\n",
    "        \n",
    "        if polarity is not None:\n",
    "            vect_pol = polarity_ratio * np.array([-vect[1],vect[0]])\n",
    "            if (dir == \"left\" and polarity == \"up\") or (dir == \"right\" and polarity == \"down\") : vect_pol *= -1\n",
    "            self.plt_ax.arrow(*center,*vect_pol, width=length/100, color = color,  **kargs)\n",
    "        \n",
    "    def get_center(self,di):\n",
    "        coord = self.space.get_object_coordinates(di)\n",
    "        return coord[[self.u_index,self.v_index]]\n",
    "    \n",
    "    def draw_dip_data(self, ax= None, polarity= \"up\", setup_drawing= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        for dataset_i in self.space.get_datasets():\n",
    "            for di in dataset_i.get_orientation_observations():\n",
    "                dir = \"right\" if di.dip_dir[0] < 180 else \"left\"\n",
    "                center = self.get_center(di)\n",
    "                self.draw_dip_symbol(center, di.dip[0], dir, polarity= polarity, setup_drawing= False, ax=ax, **kargs) \n",
    "    \n",
    "    def draw_point(self, center, color = \"black\", marker=\"*\", edge_color= \"black\", zorder= 10, setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        center = np.array(center)\n",
    "        self.plt_ax.scatter(*center, color = color, marker= marker, edgecolors= edge_color, zorder=zorder, **kargs)\n",
    "        \n",
    "    def draw_occurrence_symbol(self, center, color = \"lightblue\", marker=\"*\", edge_color= \"black\", zorder= 10, setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        self.draw_point( center, color, marker, edge_color= edge_color, zorder=zorder, setup_drawing= False, ax=ax, **kargs) \n",
    "        \n",
    "    def draw_occurrence_data(self, ax= None, polarity= \"up\", setup_drawing= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        for dataset_i in self.space.get_datasets():\n",
    "            for di in dataset_i.get_occurrence_observations():\n",
    "                center = self.get_center(di)\n",
    "                self.draw_occurrence_symbol(center, setup_drawing= False, ax=ax, **kargs) \n",
    "    \n",
    "    def show(self, ax= None, setup_drawing= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        self.draw_occurrence_data(ax=ax, setup_drawing= False, **kargs)\n",
    "        self.draw_dip_data(ax=ax, setup_drawing= False, **kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(coordinate_label=[\"x\",\"y\",\"z\"])\n",
    "dataset= GeologicalDataset(physical_space=physical_space)\n",
    "print(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = AxisAlignedCrossSection(physical_space)\n",
    "section.draw_dip_symbol([1,30],30, \"right\", polarity= \"up\", length=10 )\n",
    "section.draw_occurrence_symbol([100,40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = AxisAlignedCrossSection(physical_space)\n",
    "section.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tasks in the algorithm of interpretation are not directly specified in the interpretation process but are made abstract to make it easier to implement alternative ways to proceed.\n",
    "\n",
    "Our implementation separates two aspects of the problem:\n",
    "1. **Task**: providing an interface for describing the task to be achieved\n",
    "2. **Strategy**: providing the implementation for achieving the task.\n",
    "\n",
    "Both are represented by a separate interface; only derived classes of Task that have a defined strategy are actually applicable.\n",
    "\n",
    "There is a weak distinction between two kind of tasks:\n",
    "- Strategies: for implementing parts of the overall algorithm that are fully dependent on the user's choice. In other terms, \n",
    "there is no obvious or ultimately best strategy for achieving a task, just actions that can be carried out in different ways.\n",
    "- Heuristics: for providing quick and/or easy approximate solutions to a given problem. Here, there is in theory a correct answer\n",
    "but it might be to difficult or expansive to infer it\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from enum import Enum, Flag, auto\n",
    "import random, string\n",
    "\n",
    "class MalissiaBaseError(Exception):\n",
    "    \"\"\"This is the base class for all exceptions of this package\"\"\"\n",
    "    \n",
    "class MissingTaskImplementationError(MalissiaBaseError):\n",
    "    \"\"\"Exception generated when trying to run a task for which no implementation was selected.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__ (\"This error occurred because a task has been executed without a proper implementation defined by a strategy.\\n\"\\\n",
    "            \"Make sure to use fully implemented Task class (ie. not the base ones) and that self.strategy is set\")\n",
    "        \n",
    "\n",
    "class StrategyType(Flag):\n",
    "    \"\"\"Defines flags for specifying behaviour of the Strategy.\n",
    "    \n",
    "    DEFAULT: no specific behaviour\n",
    "    USER: it will ask user for some input\n",
    "    RANDOM: it sill set some results in a random way\n",
    "    BRUTE_FORCE: it will try all options and select the best result\"\"\"\n",
    "    DEFAULT = 0\n",
    "    USER = auto()\n",
    "    RANDOM = auto()\n",
    "    BRUTE_FORCE = auto()\n",
    "\n",
    "class Strategy(object):\n",
    "    \"\"\"Strategies are pieces of algorithm that are implementing a specific task.\n",
    "    \n",
    "    Strategy provides the generic interface of algorithms implementing a `Task`\n",
    "    and determines which tasks are actually concrete. In other words, a `Task` that does not inherit from `Strategy`\n",
    "    can not be executed.\n",
    "    \n",
    "    `Strategy` provides the following interface (which has to be implemented in inheriting classes):\n",
    "    - class attributes:\n",
    "      - name: the human-readable name of the strategy\n",
    "      - short_desc: a one-line description of the strategy\n",
    "      - full_desc: a complete description of the strategy\n",
    "    - attributes initialised by __init__:\n",
    "      - strategy_type: the type of strategy (cf. `StrategyType`)\n",
    "    - methods:\n",
    "      - check_applicability(): which checks if the algorithm can be applied in the context defined by the task\n",
    "      - execute(): which effectively runs the algorithm\n",
    "    \n",
    "    Strategies differ from Heuristics, in that the implemented task does not have an obvious/natural/true result or way to proceed,\n",
    "    but it rather a matter of choice, hence a strategy.\"\"\"\n",
    "    \n",
    "    name = \"Strategy\"\n",
    "    short_desc = \"Generic Abstract Strategy\"\n",
    "    full_desc = \"This is not supposed to be implemented.\"\n",
    "    strategy_type= StrategyType.DEFAULT\n",
    "    \n",
    "    def __init__(self, type= None, **kargs):\n",
    "        \"\"\"Initialises common attributes for the `Strategy` class\n",
    "        \n",
    "        Parameters:\n",
    "        - type (`StrategyType`): the type of strategy implement, mostly to specify if user defined or random\"\"\"\n",
    "        if type is not None:\n",
    "            self.strategy_type = type\n",
    "        \n",
    "    def check_applicability(task = None):\n",
    "        \"\"\"Checks if this algorithm can be applied in the current context\n",
    "        \n",
    "        This base class method actually triggers an Exception to make sure the methods is implemented in inheriting class\"\"\"\n",
    "        raise MalissiaBaseError(\"Trying to check applicability of the {} strategy but this class doesn't implement \"\\\n",
    "            \"the check_applicability method or run Strategy.check_applicability() instead.\".format(__class__.__name__))\n",
    "        \n",
    "    def execute(self, task):\n",
    "        \"\"\"runs the algorithm\n",
    "        \n",
    "        This base class method actually triggers an Exception to make sure the methods is implemented in inheriting class.\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        raise MalissiaBaseError(\"Trying to execute the {} strategy but this class doesn't implement the execute method \"\\\n",
    "            \"or run Strategy.execute() instead.\".format(self.__class__.__name__))\n",
    "\n",
    "class UserInputStrategy(Strategy):\n",
    "    \"\"\"Strategy based on interaction with the user.\"\"\"\n",
    "    name = \"UserInputStrategy\"\n",
    "    short_desc = \"Generic User Strategy\"\n",
    "    full_desc = \"This is implementing generic user interaction.\"\n",
    "    strategy_type= StrategyType.USER\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        \"\"\"Initialises a User based strategy\"\"\"\n",
    "        super().__init__(**kargs)\n",
    "        \n",
    "    def check_applicability(task):\n",
    "        \"\"\"Checks if user interaction is possible (based on sys.ps1)\n",
    "        \n",
    "        This kind of algorithm is also restricted to result tasks because actions would be too complex to ask to users.\n",
    "        \"\"\"\n",
    "        applicable = isinstance(task, ResultTask) \n",
    "        applicable &= UserInputStrategy.check_user_interface()\n",
    "        return applicable\n",
    "    \n",
    "    def check_user_interface():\n",
    "        return hasattr(sys, \"ps1\")\n",
    "    \n",
    "    def execute(self, task):\n",
    "        \"\"\"Asks user for the result to give.\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        task.result = task.result_type(input(\"Please give in an input that can be used as a '{}':\".format(task.result_type.__name__)))\n",
    "    \n",
    "class RandomStrategy(Strategy):\n",
    "    \"\"\"Strategy based on random generator.\"\"\"\n",
    "    name = \"RandomStrategy\"\n",
    "    short_desc = \"Generic Random Strategy\"\n",
    "    full_desc = \"This is implementing generic random generator.\"\n",
    "    strategy_type= StrategyType.RANDOM\n",
    "    \n",
    "    supported_types = (str, float, int)\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        \"\"\"Initialises a Random based strategy\"\"\"\n",
    "        super().__init__(**kargs)\n",
    "        \n",
    "    def check_applicability(task):\n",
    "        \"\"\"Checks if applicable\n",
    "        \n",
    "        i.e., for a ResultTask and result_type being in supported_types\n",
    "        \"\"\"\n",
    "        applicable = isinstance(task, ResultTask)\n",
    "        applicable &= task.result_type in __class__.supported_types\n",
    "        return applicable\n",
    "    \n",
    "    def execute(self, task):\n",
    "        \"\"\"apply the strategy.\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        if task.result_type == str:\n",
    "            task.result = \"\".join(random.choices(string.ascii_lowercase, k=10))\n",
    "        elif task.result_type == float:\n",
    "            task.result = random.random()\n",
    "        elif task.result_type == int:\n",
    "            task.result = random.randint(0,10)\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"Unsupported type for RandomStrategy. Request: {}, Available: {}.\".format(task.result_type,\", \".join(__class__.supported_types)))\n",
    "        \n",
    "class Task(object):\n",
    "    \"\"\"Task is an abstract base class to provide the generic interface for any tasks of the `GeologicalInterpretationProcess`\n",
    "    \n",
    "    Task provides the following interface (which has to be implemented in inheriting classes):\n",
    "    - class attributes:\n",
    "      - name: the human-readable name of the task\n",
    "      - short_desc: a one-line description of the task\n",
    "      - full_desc: a complete description of the task\n",
    "      - available_strategies: a list of available strategy types for implementing the `Task`.\n",
    "        NB: strategies are related to `Task` by composition to make it possible to derive the tasks and still inherit from the base class strategies.\n",
    "    - attributes initialised by __init__:\n",
    "      - context: a context in which the task must be executed, typically the `GeologicalInterpretationProcess` instance\n",
    "      - strategy: the selected strategy instance for implementing the task if any.\n",
    "    \"\"\"\n",
    "    \n",
    "    name = \"Task\"\n",
    "    short_desc = \"Generic Abstract Task\"\n",
    "    full_desc = \"This is not supposed to be implemented.\"\n",
    "    available_strategies = set()\n",
    "    \n",
    "    def __init__(self, context, **kargs):\n",
    "        \"\"\"Initialises common attributes for the `Task` class\n",
    "        \n",
    "        Parameters:\n",
    "        - task_name: the human-readable name of the task\n",
    "        - task_short_desc: a one-line description of the task\n",
    "        - task_full_desc: a complete description of the task\n",
    "        - context: the context of the task, this should provide the adapted interface for giving the required information\n",
    "        \"\"\"\n",
    "        self.context = context\n",
    "\n",
    "    def set_strategy(self, strategy):\n",
    "        \"\"\"Setter for the strategy, this ensures everything is set properly\"\"\"\n",
    "        assert not ( (strategy is not None) and (not isinstance(strategy, Strategy))), \"strategy must be either None or an instance of a Strategy (not a class)\"\n",
    "        self.strategy = strategy\n",
    "        \n",
    "    def check_applicability(self):\n",
    "        \"\"\"Checks that the task can be executed (ie. a strategy is set and applicable)\"\"\"\n",
    "        if self.strategy is None: return False\n",
    "        return self.strategy.__class__.check_applicability(self)\n",
    "    \n",
    "    def execute(self, bypass_applicability_check = False):\n",
    "        \"\"\"Method to run the selected implementation of the task.\n",
    "        \n",
    "        If no strategy is implemented/selected, this triggers an exception (`MissingTaskImplementationError`).\n",
    "        \n",
    "        parameters:\n",
    "        - bypass_applicability_check (bool): if True, the applicability of the task won't be checked beforehand. Default: False\"\"\"\n",
    "        if (not hasattr(self, \"strategy\")) or (self.strategy is None):\n",
    "            raise MissingTaskImplementationError()\n",
    "        elif (not bypass_applicability_check) and (not self.check_applicability()):\n",
    "            raise MalissiaBaseError(\"executed a Task with a strategy that was not applicable in the current context.\")\n",
    "        else: self.strategy.execute(self)\n",
    "        \n",
    "class ActionTask(Task): \n",
    "    \"\"\"Task in charge of applying some actions.\n",
    "    \n",
    "    Such tasks do not typically store a result but instead interact and possibly modify the context.\n",
    "    \"\"\"\n",
    "    name = \"ActionTask\"\n",
    "    short_desc = \"Generic Abstract Task for actions\"\n",
    "    full_desc = \"This is a generic action, meaning it doesn't hold a result but instead affects the context directly. This is not supposed to be implemented directly.\"\n",
    "    available_strategies = Task.available_strategies\n",
    "    \n",
    "    def __init__(self, context, **kargs):\n",
    "        \"\"\"Initialises an ActionTask\n",
    "        \n",
    "        Parameter:\n",
    "        - context: provides a context with the appropriate (task specific) interface. This parameter is compulsory.\"\"\"\n",
    "        super().__init__(context= context, **kargs)\n",
    "        \n",
    "class ResultTask(Task):\n",
    "    \"\"\"`Task` in charge of creating a result.\n",
    "    \n",
    "    Such tasks should not modify the context but only generate a result stored the `self.result` attribute.\n",
    "    \n",
    "    `UserInputStrategy` and `RandomStrategy` are automaticall added as available strategies\n",
    "    \"\"\"\n",
    "    name = \"ResultTask\"\n",
    "    short_desc = \"Generic Task for generating results\"\n",
    "    full_desc = \"This is a generic result generator, meaning it doesn't affect the context but holds a result instead.\"\n",
    "    available_strategies = Task.available_strategies | set((UserInputStrategy, RandomStrategy))\n",
    "    \n",
    "    def __init__(self, result_type, context = None, **kargs):\n",
    "        \"\"\"Initialises a ResultTask\n",
    "                \n",
    "        Additional parameters (for others see `Task`):\n",
    "        - result_type: the type of expeted result\n",
    "        - context: provides a context with the appropriate (task specific) interface. This parameter is compulsory.\"\"\"\n",
    "        super().__init__(context= context, **kargs)\n",
    "        self.result_type = result_type\n",
    "        self.result = None\n",
    "\n",
    "    def execute(self, bypass_applicability_check = False, return_result= True):\n",
    "        \"\"\"Additional behaviour for `ResultTask`\n",
    "        \n",
    "        Runs the `Task.execute()` then return result if the option is activated (default)\"\"\"\n",
    "        super().execute(bypass_applicability_check= bypass_applicability_check)\n",
    "        if return_result: return self.result\n",
    "        \n",
    "class StrategyFactory(object):\n",
    "    \"\"\"This class gathers all the strategies available for the `GeologicalInterpretationProcess`.\n",
    "    \n",
    "    The strategies are automatically recovered from the subclasses of Task and Strategy.\n",
    "    Three dictionaries are registering the task and corresponding strategies:\n",
    "    - self.tasks: are the Defined Tasks having defined possible Strategies\n",
    "    - self.specified_tasks: are the tasks with a specific available strategy (i.e., only one)\n",
    "    - self.implemented_tasks: tasks that have an implemented strategy\n",
    "    NB: they aren't used for generating the tasks, but just for reporting the existing tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialisation of the strategy storage.\"\"\"\n",
    "        self.tasks = {}\n",
    "        self.specified_tasks = {}\n",
    "        self.implemented_tasks = {}\n",
    "        self.tasks_without_strategy = {}\n",
    "        self.list_available_tasks()\n",
    "        self.list_specified_tasks()\n",
    "        self.list_implemented_tasks()\n",
    "        self.list_tasks_without_strategy()\n",
    "        \n",
    "    def generate_task(self, task_type, strategy_type= None, strategy_class= None, context= None, **kargs):\n",
    "        \"\"\"Creates a task of a given type and instanciate appropriate strategy\n",
    "        \n",
    "        Parameters:\n",
    "        - task_type: the class of the task to be created\n",
    "        - strategy_type: the type of strategy to be used, see `StrategyType`\n",
    "        - strategy_class: to force the strategy selection, its class can be directly given here\n",
    "        - context: a context that can be passed to the task to define it (typically the `GeologicalInterpretationProcess`)\n",
    "        - kargs: keyword arguments that can be passed to the task initialisation\"\"\"\n",
    "        \n",
    "        # Create the task\n",
    "        task = task_type(context = context, **kargs)\n",
    "        \n",
    "        if strategy_class is None:\n",
    "            # list strategies\n",
    "            strategies = np.array(list(task_type.available_strategies))\n",
    "            \n",
    "            # check applicability to the task and context\n",
    "            applicable_strategy = [candidate_strategy for candidate_strategy in strategies if candidate_strategy.check_applicability(task)]\n",
    "            \n",
    "            # filter strategy types\n",
    "            if strategy_type is not None:\n",
    "                applicable_strategy = [candidate_strategy for candidate_strategy in applicable_strategy if candidate_strategy.strategy_type == strategy_type]\n",
    "                \n",
    "            if len(applicable_strategy) == 0:\n",
    "                message = [\"No implementation of the requested task ({}) found in this context.\".format(task_type)]\n",
    "                message += [\"Note that the following strategies were available:\\n\"+\"\\n\".join(strategies)]\n",
    "                message += [\"Note that the following strategies were applicable:\\n\"+\"\\n\".join(applicable_strategy)]\n",
    "                message += [\"Note that the following strategies were filtered out:\\n\"+\"\\n\".join([candidate_strategy for candidate_strategy in applicable_strategy if candidate_strategy.strategy_type != strategy_type])]\n",
    "                raise MalissiaBaseError(\"\\n\".join(message))\n",
    "             \n",
    "            # this could be defined by a strategy as well, e.g. pick first of random\n",
    "            strategy_class = random.choice(applicable_strategy)\n",
    "            \n",
    "        else:\n",
    "            if strategy_class.check_applicability(task) == False:\n",
    "                raise MalissiaBaseError(\"The proposed implementation ({}) of the requested task ({}) is not applicable in this context.\".format(strategy_class,task_type))\n",
    "                \n",
    "        # instanciate the strategy\n",
    "        strategy_instance = strategy_class(**kargs)\n",
    "        task.set_strategy(strategy_instance)\n",
    "            \n",
    "        return task\n",
    "    \n",
    "    def list_available_tasks(self):\n",
    "        \"\"\"List the Tasks that are fully defined (ie. having attached strategies)\"\"\"\n",
    "        task_pile = list(Task.__subclasses__())\n",
    "        while len(task_pile) > 0:\n",
    "            task_i = task_pile.pop()\n",
    "            \n",
    "            strategies = task_i.available_strategies\n",
    "            if len(strategies) > 0:\n",
    "                self.tasks[task_i] = strategies \n",
    "            \n",
    "            sub_tasks = list(task_i.__subclasses__())\n",
    "            task_pile += sub_tasks\n",
    "    \n",
    "    def list_specified_tasks(self):\n",
    "        \"\"\"List the Tasks that have only one possible strategy\"\"\"\n",
    "        task_pile = list(Task.__subclasses__())\n",
    "        while len(task_pile) > 0:\n",
    "            task_i = task_pile.pop()\n",
    "            \n",
    "            strategies = task_i.available_strategies\n",
    "            if len(strategies) == 1:\n",
    "                self.specified_tasks[task_i] = list(strategies)[0]\n",
    "            \n",
    "            sub_tasks = list(task_i.__subclasses__())\n",
    "            task_pile += sub_tasks\n",
    "    \n",
    "    def list_implemented_tasks(self):\n",
    "        \"\"\"List the Tasks that are implemented (ie. having an attached strategy instance)\"\"\"\n",
    "        task_pile = list(Task.__subclasses__())\n",
    "        while len(task_pile) > 0:\n",
    "            task_i = task_pile.pop()\n",
    "            \n",
    "            if hasattr(task_i,\"strategy\") and task_i.strategy is not None:\n",
    "                self.implemented_tasks[task_i] = task_i.strategy \n",
    "            \n",
    "            sub_tasks = list(task_i.__subclasses__())\n",
    "            task_pile += sub_tasks\n",
    "            \n",
    "    def list_tasks_without_strategy(self):\n",
    "        \"\"\"Lists the tasks that have no strategies nor subclass\"\"\"\n",
    "        task_pile = list(Task.__subclasses__())\n",
    "        while len(task_pile) > 0:\n",
    "            task_i = task_pile.pop()\n",
    "            \n",
    "            strategies = task_i.available_strategies\n",
    "            if (len(strategies) == 0) and (len(task_i.__subclasses__()) == 0):\n",
    "                self.tasks_without_strategy[task_i] = ()\n",
    "            \n",
    "            sub_tasks = list(task_i.__subclasses__())\n",
    "            task_pile += sub_tasks\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Describe the stored strategies\"\"\"\n",
    "        self.list_available_tasks()\n",
    "        self.list_specified_tasks()\n",
    "        self.list_implemented_tasks()\n",
    "        self.list_tasks_without_strategy()\n",
    "        \n",
    "        desc = [\"Strategy Factory:\"]\n",
    "        desc+= [\"- Available Tasks:\"]\n",
    "        for task, strat_list in self.tasks.items():\n",
    "            desc += [\"  |- {}:\".format(task.name)]\n",
    "            if len(strat_list) == 0:\n",
    "                desc += [\"  |[Warning]: no strategy provided for this task\"] \n",
    "            for strat_i in strat_list:\n",
    "                desc += [\"  |  |- {}{}: {}\".format(strat_i.__class__.__name__, \"\" if strat_i.strategy_type is StrategyType.DEFAULT else \"[\"+strat_i.strategy_type.name+\"]\", strat_i.short_desc)]\n",
    "        \n",
    "        if len(self.specified_tasks) > 0:\n",
    "            desc+= [\"- Specified Tasks:\"]\n",
    "            for task, strat in self.specified_tasks.items():\n",
    "                desc += [\"  |- {}: {}{}({})\".format(task.name, strat.name, \"\" if strat.strategy_type is StrategyType.DEFAULT else \"[\"+strat.strategy_type.name+\"]\", strat.short_desc)]\n",
    "        if len(self.implemented_tasks) > 0:\n",
    "            desc+= [\"- Implemented Tasks:\"]\n",
    "            for task, strat in self.implemented_tasks.items():\n",
    "                desc += [\"  |- {}: {}{}({})\".format(task.name, strat.name, \"\" if strat.strategy_type is StrategyType.DEFAULT else \"[\"+strat.strategy_type.name+\"]\", strat.short_desc)]\n",
    "        if len(self.tasks_without_strategy) > 0:\n",
    "            desc+= [\"- Tasks without strategy:\"]\n",
    "            for task, strat in self.tasks_without_strategy.items():\n",
    "                desc += [\"  |- {}: [Warning]: no strategy provided for this task\".format(task.name)]\n",
    "        return \"\\n\".join(desc)\n",
    "        \n",
    "    \n",
    "__default_strategy_factory__ = StrategyFactory()\n",
    "    \n",
    "class HeuristicsFactory(object):\n",
    "    \"\"\"This class provides Heuristics for the `GeologicalInterpretationProcess`.\n",
    "    \n",
    "    Heuristics are pieces of algorithm that are implementing a specific computational or decisionnal task.\n",
    "    They differ from strategies, in that there exists a true result even if it is not know or two complex to infer accurately.\n",
    "    Heuristics try to provide acceptable approximate results with a simple algorithms to save either computation time, memory, or complexity.\n",
    "\n",
    "    The heuristics are stored in a dictionnary (`self.__heuristics`) that associates tasks with a list of possible implementation. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises the Heuristics storage.\"\"\"\n",
    "        self.__heuristics = {}\n",
    "        \n",
    "print(__default_strategy_factory__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleTask(ResultTask):\n",
    "    \"\"\"class in charge of showing how to implement new tasks\n",
    "    \n",
    "    Such task is independent of a context and ignores it if passed.\"\"\"\n",
    "    \n",
    "    name = \"ExampleTask\"\n",
    "    short_desc = \"gives an example\"\n",
    "    full_desc = \"Demonstrates the implementation of new tasks.\"\n",
    "    def __init__(self, result_type= None, **kargs):\n",
    "        \"\"\"example of initialisation of a result task\n",
    "        \n",
    "        Parameters:\n",
    "        - result_type: the type of expeted result. If None, one of the RandomStrategy supported type is used.\"\"\"\n",
    "        result_type = result_type if result_type is not None else random.choice(RandomStrategy.supported_types)\n",
    "        super().__init__(result_type= result_type)\n",
    "        \n",
    "class UserExampleTask(ExampleTask):\n",
    "\n",
    "    name = \"UserExampleTask\"\n",
    "    short_desc = \"example user strategy\"\n",
    "    full_desc = \"Gives an example of implementation of user defined algorithm.\"\n",
    "    available_strategies = {UserInputStrategy}\n",
    "    \n",
    "    def __init__(self, result_type= str, **kargs):\n",
    "        super().__init__(result_type= result_type)\n",
    "        strategy = UserInputStrategy()\n",
    "        super().set_strategy(strategy)\n",
    "    \n",
    "class RandomExampleTask(ExampleTask):\n",
    "\n",
    "    name = \"RandomExampleTask\"\n",
    "    short_desc = \"example random strategy\"\n",
    "    full_desc = \"Gives an example of implementation of random defined algorithm.\"\n",
    "    available_strategies = {RandomStrategy}\n",
    "    \n",
    "    def __init__(self, result_type= str, **kargs):\n",
    "        super().__init__(result_type= result_type)\n",
    "        strategy = RandomStrategy()\n",
    "        super().set_strategy(strategy)\n",
    "        \n",
    "print(__default_strategy_factory__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(ExampleTask, strategy_type= StrategyType.RANDOM)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(ExampleTask)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    strat = Strategy()\n",
    "    strat.check_applicability()\n",
    "except MalissiaBaseError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = RandomExampleTask()\n",
    "if task.check_applicability():\n",
    "    task.execute()\n",
    "task.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ActionTask(None)\n",
    "task.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    task.execute()\n",
    "except MalissiaBaseError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExampleTask.available_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserExampleTask.available_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActionTask.available_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = UserExampleTask()\n",
    "if task.check_applicability():\n",
    "    task.execute()\n",
    "task.result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Tasks implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserSelectStrategy(UserInputStrategy):\n",
    "    \"\"\"Strategy based on interaction with the user.\"\"\"\n",
    "    name = \"UserSelectStrategy\"\n",
    "    short_desc = \"Selection User Strategy\"\n",
    "    full_desc = \"This is implementing selection based on user interaction.\"\n",
    "    type= StrategyType.USER\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        \"\"\"Initialises a User based strategy\"\"\"\n",
    "        super().__init__(**kargs)\n",
    "        \n",
    "    def check_applicability(task, context = None):\n",
    "        \"\"\"Checks if applicable\n",
    "        \n",
    "        uses UserInputStrategy.check_applicability and SelectionTask type\n",
    "        \"\"\"\n",
    "        applicable = isinstance(task, SelectionTask) \n",
    "        applicable &= UserInputStrategy.check_user_interface()\n",
    "        return applicable\n",
    "    \n",
    "    def execute(self, task):\n",
    "        \"\"\"Asks user for the result to give.\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        message = \"Please select an option amongst the following (give its index):\\n \"+\"\\n\".join([\"{}: {}\".format(i,val) for i, val in enumerate(task.choices)])\n",
    "        try:\n",
    "            selected = int(input(message))\n",
    "            task.result = task.result_type(task.choices[selected])\n",
    "        except ValueError:\n",
    "            raise MalissiaBaseError(\"User input must be in the form of an integer corresponding to the selected index.\")\n",
    "        except IndexError:\n",
    "            raise MalissiaBaseError(\"The selected index must be within [{}, {}].\".format(0,len(task.choices)-1))\n",
    "        return task.result\n",
    "    \n",
    "class RandomSelectStrategy(RandomStrategy):\n",
    "    \"\"\"Strategy based on random generator.\"\"\"\n",
    "    name = \"RandomSelectStrategy\"\n",
    "    short_desc = \"Random Selection Strategy\"\n",
    "    full_desc = \"This is implementing a random selection.\"\n",
    "    type= StrategyType.RANDOM\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        \"\"\"Initialises a Random based strategy\"\"\"\n",
    "        super().__init__(**kargs)\n",
    "        \n",
    "    def check_applicability(task, context = None):\n",
    "        \"\"\"Checks if applicable\n",
    "        \n",
    "        Needs to be set for a SelectionTask with at least one choice\n",
    "        \"\"\"\n",
    "        applicable = isinstance(task, SelectionTask)\n",
    "        applicable &= len(task.choices) >0\n",
    "        return applicable\n",
    "    \n",
    "    def execute(self, task):\n",
    "        \"\"\"perform random selection\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        \n",
    "        if len(task.n) == 1:\n",
    "            if task.n[0] == 1:\n",
    "                task.result = random.choice(task.choices)\n",
    "                return task.result_type(task.result)\n",
    "            else:\n",
    "                k = task.n\n",
    "        else:\n",
    "            k = np.cumprod(task.n)[-1]\n",
    "            \n",
    "        task.result = random.choices(task.choices, k = k)\n",
    "        if len(task.n) > 1:\n",
    "            task.result.reshape(task.n)\n",
    "            \n",
    "        return task.result.astype(task.result_type)\n",
    "\n",
    "class SelectionTask(ResultTask):\n",
    "    \"\"\"SelectionTask performs a selection among a series of possible results.\"\"\"\n",
    "    \n",
    "    name = \"SelectionTask\"\n",
    "    short_desc = \"makes a selection\"\n",
    "    full_desc = \"Performs a selection among a series of possible results.\"\n",
    "    available_strategies = set((UserSelectStrategy, RandomSelectStrategy))\n",
    "    \n",
    "    def __init__(self, choices, n= 1, result_type= None, context= None, **kargs):\n",
    "        \"\"\"Initialisation of a selection task\n",
    "        \n",
    "        Parameters:\n",
    "        - choices: a series of possible values for the result. Will be transformed into a numpy array and flatten (np.array(choices).ravel())\n",
    "        - n (int): number of elements to be picked, alternatively if a shape is given, the shape of the expected result array\n",
    "        - result_type: the type of expeted result. If None, the type of the choices will be used by default.\"\"\"\n",
    "        self.choices = np.array(choices).ravel()\n",
    "        self.n = np.array(n).ravel()\n",
    "        result_type = result_type if result_type is not None else self.choices.dtype.type\n",
    "        super().__init__(result_type= result_type)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(SelectionTask, choices = [1,2,27,42])\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = dataset.get_observations()\n",
    "task = __default_strategy_factory__.generate_task(SelectionTask, choices = choices, strategy_type= StrategyType.RANDOM)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)\n",
    "print(\"Result type:\",type(task.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectObservation(SelectionTask):\n",
    "    \"\"\"Selection of an observation\"\"\"\n",
    "    name = \"SelectObservation\"\n",
    "    short_desc = \"selects observations\"\n",
    "    full_desc = \"Performs a selection among possible observations.\"\n",
    "    \n",
    "    def __init__(self, n= 1, choices = None, dataset= None, context= None, **kargs):\n",
    "        \"\"\"Initialisation of a selection task\n",
    "        \n",
    "        Parameters:\n",
    "        - choices: a series of possible observations. If None (default), the dataset in the context will be used (and therefore can't be None)\n",
    "        - dataset: a GeologicalDataset to bring the available observations unless already defined by choices \n",
    "        - context: if given a `GeologicalInterpretationProcess`, will be used to define the possible selection unless, choices or dataset are given.\n",
    "        - n (int): number of elements to be picked, alternatively if a shape is given, the shape of the expected result array\"\"\"\n",
    "        choices = choices if choices is not None else self.get_choices(dataset= dataset, context= context, **kargs)\n",
    "        super().__init__(choices = choices, n= n, context = context, **kargs)\n",
    "\n",
    "    def get_choices(self, choices = None, dataset:GeologicalDataset= None, context= None, **kargs):\n",
    "        \"\"\"generates the choices list from different possible sources\n",
    "        \n",
    "        Parameters:\n",
    "        - choices: a series of possible observations. If None (default), the dataset in the context will be used (and therefore can't be None)\n",
    "        - dataset: a GeologicalDataset to bring the available observations unless already defined by choices \n",
    "        - context: if given a `GeologicalInterpretationProcess`, will be used to define the possible selection unless, choices or dataset are given.\n",
    "        \"\"\"\n",
    "        if (context is None) and (dataset is None):\n",
    "            raise MalissiaBaseError(\"Either a series of choices, a dataset, or a context must be given.\")\n",
    "        if dataset is None:\n",
    "            dataset = context.dataset\n",
    "        choices = dataset.get_observations()\n",
    "        return choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectObservation.available_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(SelectObservation, dataset = dataset)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)\n",
    "print(\"Result type:\",type(task.result))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation process in itself is run in a **GeologicalInterpretationProcess** and follow a very simple and generic algorithm.<br>\n",
    "This algorithm implements a Deming wheel process of continual improvement:\n",
    "1. Plan:\n",
    "    1. Select a situation\n",
    "    2. Select an action\n",
    "2. Do: Implement the action (e.g., CreateInterpretationElement)\n",
    "    1. List features\n",
    "    2. Identify possible explanations\n",
    "    3. Rank/chose explanations\n",
    "    4. Instanciate individuals\n",
    "    5. Infer and set parameters\n",
    "3. Check: Evaluate consistency\n",
    "    1. Evaluate internal consistency\n",
    "    2. Evaluate relational likelihood\n",
    "    3. Evaluate feature explanation\n",
    "4. Act: Generate anomalies and report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerminationFlag(Flag):\n",
    "    \"\"\"Defines flags for specifying why the `GeologicalInterpretationProcess` stoped\n",
    "    \n",
    "    DEFAULT: it has not been terminated\n",
    "    USER: was terminated by user\n",
    "    MAX_ITER: maximum iteration number reached\n",
    "    \"\"\"\n",
    "    DEFAULT = 0\n",
    "    USER = auto()\n",
    "    MAX_ITER = auto()\n",
    "    \n",
    "class GeologicalInterpretationProcess(object):\n",
    "   \"\"\"GeologicalInterpretationProcess implements the core process of a geological intepretation.\n",
    "   \n",
    "   It connects all the required elements and resulting artefacts relatively to a given interpretation sequence:\n",
    "    - knowledge_framework: a GeologicalKnowledgeFramework\n",
    "    - dataset: a GeologicalDataSet that interfaces all the vailable data\n",
    "    - strategies: a StrategyFactory that implements various way to perform required tasks\n",
    "    - heuristics: a HeuristicFactory that provides various algorithms for computing or inferring parameters\"\"\"\n",
    "     \n",
    "   def __init__(self,\n",
    "                dataset: GeologicalDataset,\n",
    "                physical_space: PhysicalRepresentationSpace = None,\n",
    "                knowledge_framework= None,\n",
    "                strategies: StrategyFactory= None):\n",
    "      \"\"\"Creates a GeologicalInterpretationProcess\n",
    "        \n",
    "      ---------------------------\n",
    "      Parameters:\n",
    "       - dataset (GeologicalDataset): a dataset to be explained by this interpretor\n",
    "       - physical_space (PhysicalRepresentationSpace): the physical space that must be explainined by the interpretation.\n",
    "       If None (default), the space of the dataset is used instead.\n",
    "       - knowledge_framework: a GeologicalKnowledgeFramework that defines the concepts used for this interpretation.\n",
    "         If None is given, the the default knowledge framework is used (`GeologicalKnowledgeManager().get_knowledge_framework()`)\n",
    "       - strategies: pointer to the factory to be used for accessing strategies for the various tasks in this process\n",
    "         If None (default), the default factory is used as defined by __default_strategy_factory__\n",
    "      \"\"\"\n",
    "      self.dataset = dataset\n",
    "      self.physical_space = physical_space if physical_space is not None else self.dataset.physical_space\n",
    "      self.strategies = strategies if strategies is not None else __default_strategy_factory__\n",
    "      self.knowledge_framework= GeologicalKnowledgeManager().get_knowledge_framework() if knowledge_framework is None else knowledge_framework\n",
    "      \n",
    "      self.update_status()\n",
    "   \n",
    "   def update_status(self, init= False):\n",
    "      \"\"\"Performs status update operations\n",
    "      \n",
    "      If the status is not set yet, this method will also create it and initialise it.\n",
    "      \n",
    "      Parameters:\n",
    "      - init (Bool): if True, the status will be reset to initial values. Default is False.\"\"\"\n",
    "      \n",
    "      if not hasattr(self, \"status\"):\n",
    "         self.status = {}\n",
    "         init = True\n",
    "      if init:\n",
    "         self.status[\"epoch\"] = None # index of the current iteration\n",
    "      \n",
    "      self.status[\"coverage\"] = self.evaluate_coverage() # ratio of representation space covered by an explanation\n",
    "      self.status[\"data_explanation_ratio\"] = self.evaluate_data() # ratio of explained observations\n",
    "      self.status[\"anomaly_explanation_ratio\"] =  self.evaluate_anomalies() # ratio of explained observations\n",
    "      self.status[\"termination_criterion\"] = TerminationFlag.DEFAULT # gives explanation about why it terminated\n",
    "      \n",
    "   def evaluate_coverage(self):\n",
    "      \"\"\"Evaluates the amount of physical space that is explained\n",
    "      \n",
    "      Returns:\n",
    "      - a float between 0 and 1, representing the ratio of covered space\"\"\"\n",
    "      return 0\n",
    "      \n",
    "   def evaluate_data(self):\n",
    "      \"\"\"Evaluates the amount of data that is explained\n",
    "      \n",
    "      Returns:\n",
    "      - a float between 0 and 1, representing the ratio of explained data\"\"\"\n",
    "      return 0\n",
    "   \n",
    "   def evaluate_anomalies(self):\n",
    "      \"\"\"Evaluates the amount of raised anomalies that are now explained\n",
    "      \n",
    "      Returns:\n",
    "      - a float between 0 and 1, representing the ratio of explained anomalies\"\"\"\n",
    "      return 0\n",
    "   \n",
    "   def __str__(self):\n",
    "      \"\"\"Return a report describing the interpretation process\"\"\"\n",
    "      desc = [\"Geological Interpretation Process:\"]\n",
    "      if self.dataset is None:\n",
    "         desc+= [\"|- Dataset: None\"]\n",
    "      else:\n",
    "         desc+= [\"|- Dataset: \"+\"\\n| |\".join(self.dataset.__str__().split(\"\\n\"))]\n",
    "         \n",
    "      if self.knowledge_framework is None:\n",
    "         desc+= [\"|- Geological Knowledge Framework: None\"]\n",
    "      else:\n",
    "         desc+= [\"|- \"+\"\\n| |\".join(self.knowledge_framework.__str__().split(\"\\n\"))]\n",
    "         \n",
    "      if self.strategies is None:\n",
    "         desc+= [\"|- Tasks with strategies: None\"]\n",
    "      else:\n",
    "         desc+= [\"|- \"+\"\\n| |\".join(self.strategies.__str__().split(\"\\n\"))]\n",
    "         \n",
    "      return \"\\n\".join(desc)\n",
    "   \n",
    "   def run(self, max_iter= None):\n",
    "      \"\"\"Runs the iterative interpretation process\n",
    "      \n",
    "      Parameters:\n",
    "      - max_iter (int): if set, it specifies the maximum number of iterations to run before terminating.\n",
    "      Iteration (epoch) are numbered starting at 0, this way the counter also represents the number of passed iterations.\n",
    "      \"\"\"\n",
    "      \n",
    "      # run the iterative process as long as a termination criterion is not reached\n",
    "      \n",
    "      #first update the status to make sure it is up to date, and reset the termination criteria\n",
    "      self.update_status()\n",
    "      \n",
    "      # new epoch increments the iteration count and check termination\n",
    "      try:\n",
    "         while self.new_epoch(max_iter = max_iter):\n",
    "         \n",
    "            # 1. Plan\n",
    "            # -----------------------------\n",
    "            # 1.1 chose a type of action\n",
    "            \n",
    "            # 1.2 select a situation (ie. features to be explained in a context)\n",
    "            situation = self.strategies.generate_task(SelectSituation, context = self).execute(return_result= True)\n",
    "            print(\"{}: {}\".format(self.status[\"epoch\"],situation))\n",
    "            \n",
    "            # 2. Do\n",
    "            # -----------------------------\n",
    "            # 2.1 execute the specified action\n",
    "            \n",
    "            # 3. Check\n",
    "            # -----------------------------\n",
    "            #  3.1 Evaluate internal consistency\n",
    "            #  3.2 Evaluate relational likelihood\n",
    "            #  3.3 Evaluate feature explanation\n",
    "            \n",
    "            # 4. Act\n",
    "            # -----------------------------\n",
    "            # 4.1 Generate anomalies and report\n",
    "            # 4.2 update status\n",
    "            \n",
    "      except KeyboardInterrupt:\n",
    "         self.status[\"termination_criterion\"] = TerminationFlag.USER\n",
    "         \n",
    "      return self.status[\"termination_criterion\"]\n",
    "         \n",
    "   def new_epoch(self, max_iter= None):\n",
    "      \"\"\"Starts a new epoch (iteration) unless termination criteria were reached\n",
    "      \n",
    "      Returns:\n",
    "      - True if the new epoch should start, False if iterations should stop\n",
    "      - max_iter (int): if set, it specifies the maximum number of iterations to run before terminating\"\"\"\n",
    "      keep_going = True\n",
    "      \n",
    "      self.increment_epoch()\n",
    "      if (max_iter is not None) and (self.status[\"epoch\"] >= max_iter):\n",
    "         self.status[\"termination_criterion\"] = TerminationFlag.MAX_ITER\n",
    "         return False\n",
    "      \n",
    "      return keep_going\n",
    "      \n",
    "   def increment_epoch(self):\n",
    "      \"\"\"Initialize or increment the epoch count\"\"\"\n",
    "      if self.status[\"epoch\"] is None:\n",
    "         self.status[\"epoch\"] = 0\n",
    "      else:\n",
    "         self.status[\"epoch\"] += 1\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation Process Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretationSituation(object):\n",
    "    \"\"\"Defines a situation of interpretation.\n",
    "    \n",
    "    A situation is gathering:\n",
    "    - self.features: features to be explained\n",
    "    - self.context: an interpretation context, ie. elements to be considered during the interpretation\n",
    "    - self.process: the `GeologicalInterpretationProcess` in which this situation takes place\"\"\"\n",
    "    \n",
    "    def __init__(self, features, context, process:GeologicalInterpretationProcess) -> None:\n",
    "        \"\"\"Initialises the situation\"\"\"\n",
    "        self.features = np.array(features).ravel()\n",
    "        self.context = np.array(context).ravel()\n",
    "        self.process = process\n",
    "\n",
    "class RandomSituationStrategy(Strategy):\n",
    "    \"\"\"Strategy for randomly selecting a situation\"\"\"\n",
    "    name = \"RandomSituationStrategy\"\n",
    "    short_desc = \"Random Situation Selection\"\n",
    "    full_desc = \"This is implementing random selection of situation to be explained.\"\n",
    "    strategy_type= StrategyType.RANDOM\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        \"\"\"Initialisation of radom situation selection\"\"\"\n",
    "        super().__init__(**kargs)\n",
    "        \n",
    "    \n",
    "    def check_applicability(task):\n",
    "        \"\"\"Checks if applicable\n",
    "        \n",
    "        i.e., for a SelectSituation\n",
    "        \"\"\"\n",
    "        applicable = isinstance(task, SelectSituation)\n",
    "        applicable &= task.context is not None\n",
    "        return applicable\n",
    "    \n",
    "    def execute(self, task):\n",
    "        \"\"\"apply the strategy.\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        \n",
    "        feature_selection_task = task.context.strategies.generate_task(SelectObservation, strategy_type= StrategyType.RANDOM, context= task.context)\n",
    "        selected_feature = feature_selection_task.execute(return_result= True)\n",
    "        \n",
    "        task.result = InterpretationSituation( selected_feature, context= [], process= task.context)\n",
    "  \n",
    "class SelectSituation(ResultTask):\n",
    "    \"\"\"Task for selecting a situation to be explained (feature to be explained + interpretation context)\"\"\"\n",
    "    \n",
    "    name = \"SelectSituation\"\n",
    "    short_desc = \"selects a situation to ne explained\"\n",
    "    full_desc = \"Selects features to be explained and an interpretation context.\"\n",
    "    result_type = InterpretationSituation\n",
    "    available_strategies = set((RandomSituationStrategy,))\n",
    "    \n",
    "    def __init__(self, context:GeologicalInterpretationProcess, feature_choices = None, n= None, **kargs):\n",
    "        \"\"\"Initialisation of a situation selection task\n",
    "        \n",
    "        Parameters:\n",
    "        - feature_choices: a preselected list of features to be explained. If None (default), the dataset in the context will be used \n",
    "        - context: a `GeologicalInterpretationProcess`, as this is an action task, this context will be modified by the task.\n",
    "        It will also be used to define the possible selection unless feature_choices is given.\n",
    "        - n (int): number of features to be selected, if None, this is inferred from the strategies in the context.\"\"\"\n",
    "        self.feature_choices = feature_choices\n",
    "        self.n = n\n",
    "        super().__init__(context = context, result_type= SelectSituation.result_type, **kargs)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip = GeologicalInterpretationProcess(dataset)\n",
    "print(gip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(SelectSituation, context = gip)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)\n",
    "print(\"Result type:\",type(task.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.run(max_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
