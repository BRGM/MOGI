{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geological Interpretor Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for testing and developping some of the basic code in this package."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ontology manipulation\n",
    "\n",
    "The knowledge manipulated in this package is formalised in an ontology,<br>\n",
    "which is store in a *.owl* file.\n",
    "\n",
    "It is named **MOGI** for **M**inimal **O**ntology for **G**eological **I**nterpretation\n",
    "\n",
    "To manipulated this ontology, we use the package **owlready2** available from here: https://owlready2.readthedocs.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import owlready2 as owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owl.onto_path.append(\"../ontologies/\")\n",
    "mogi = owl.get_ontology(\"mogi.owl\").load()\n",
    "mogi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ontology provides access to its components, e.g.:\n",
    "* classes\n",
    "* properties\n",
    "* individuals\n",
    "* rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(mogi.classes()))\n",
    "print(list(mogi.properties()))\n",
    "print(list(mogi.individuals()))\n",
    "print(list(mogi.rules()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specific elements can be searched through simple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(iri = \"*Surface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mogi.Geologic_Context('Data_properties')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.INDIRECT_get_properties()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoner\n",
    "\n",
    "Ontologies are even more powerful thansk to their capabilities to use reasoning for infering types, properties, and relationships that were not explicitly stated.\n",
    "This is usefull for obtaining results implied by the already stated information.\n",
    "\n",
    "This is achieved by running a *reasoner* on the ontology as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owl.sync_reasoner(infer_property_values=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geological Knowledge Manager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GeologicalKnowledgeManager** may know different instances of **GeologicalKnowledgeFramework**,<br>\n",
    "for example to allow differenciating scenarios or for allowing customisation of knowledge and its formalisation.\n",
    "\n",
    "**GeologicalKnowledgeFramework** provides access to concept definitions for providing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class GeologicalKnowledgeManager(object):\n",
    "    \"\"\"GeologicalKnowledgeManager is managing one or several GeologicalKnowledgeFramework.\n",
    "    \n",
    "    The GeologicalKnowledgeManager is typically a singleton, so there is always one and only one instance of it.\n",
    "    \n",
    "    The GeologicalKnowledgeManager may know different instances of GeologicalKnowledgeFramework,\n",
    "    for example to allow different interpretation scenarios or for allowing user-specific customisation\n",
    "    of knowledge and its formalisation.\n",
    "    \n",
    "    GeologicalKnowledgeFramework are typically ontologies and extensions defined in this package or elsewhere.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __new__(cls):\n",
    "        \"\"\"Method to access (and create if needed) the only allowed instance of this class.\n",
    "        \n",
    "        Returns:\n",
    "        - an instance of GeologicalKnowledgeManager\"\"\"\n",
    "        if not hasattr(cls, 'instance'):\n",
    "            cls.instance = super(GeologicalKnowledgeManager, cls).__new__(cls)\n",
    "            cls.initialised= False\n",
    "            print(\"DEBUG::creates new manager\")\n",
    "        return cls.instance\n",
    "        \n",
    "    def __init__(self, default= \"mogi\", default_source_directory= \"../ontologies/\", default_source_file= \"mogi.owl\", default_ontology_backend= \"owlready2\"):\n",
    "        \"\"\"Initializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        print(\"DEBUG::__init__\")\n",
    "        if not self.initialised:\n",
    "            self._initialise(default= default, default_source_directory= default_source_directory, default_source_file= default_source_file, default_ontology_backend= default_ontology_backend)\n",
    "            \n",
    "    def _initialise(self, default, default_source_directory, default_source_file, default_ontology_backend):\n",
    "        \"\"\"Initializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        print(\"DEBUG::initialize manager\")\n",
    "        self.default= default\n",
    "        self.default_source_directory= default_source_directory\n",
    "        self.default_source_file= default_source_file\n",
    "        self.default_ontology_backend= default_ontology_backend\n",
    "        \n",
    "        self.knowledge_framework_dict = {}\n",
    "        \n",
    "        self.initialised= True\n",
    "        \n",
    "    def reset(self, default= \"mogi\", default_source_directory= \"../ontologies/\", default_source_file= \"mogi.owl\", default_ontology_backend= \"owlready2\"):\n",
    "        \"\"\"Reinitializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        print(\"DEBUG::reset manager\")\n",
    "        self._initialise(default= default, default_source_directory= default_source_directory, default_source_file= default_source_file, default_ontology_backend= default_ontology_backend)\n",
    "             \n",
    "    def load_knowledge_framework(self, name=None, source= None, source_directory= None, backend= None):\n",
    "        \"\"\"Gets and initilises the ontology from the specified source.\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name to be given to the knowledge framework. If None (default) the file name will be used.\n",
    "        - source: filename to the ontology source. If None(default) the default ontology is used.\n",
    "        - source_directory: where the system should look for ontology definition files. If None, the `GeologicalKnowledgeFramework` will decide.\n",
    "        - backend: the ontology backend to be used. If None, the `GeologicalKnowledgeFramework` will decide.\"\"\"\n",
    "        source = source if source is not None else self.default_source_file\n",
    "        name = name if name is not None else os.path.basename(source).split(os.path.extsep)[0]\n",
    "        self.knowledge_framework_dict[name] = GeologicalKnowledgeFramework(name= name, source= source, source_directory= source_directory, backend= backend)\n",
    "    \n",
    "    def get_knowledge_framework(self,name= \"default\"):\n",
    "        \"\"\"Accessor to knowledge frameworks.\"\"\"\n",
    "        name = self.default if name == \"default\" else name\n",
    "        assert len(self.knowledge_framework_dict) > 0, \"No ontology has been loaded yet. Please use GeologicalKnowledgeManager().load_knowledge_framework() first\"\n",
    "        assert name in self.knowledge_framework_dict.keys(), \"The specified ontology hasn't been loaded: \"+name+\\\n",
    "            \"\\navailable ontology names are: \"+\"\\n\".join(self.knowledge_framework_dict.keys())\n",
    "        return self.knowledge_framework_dict[name]\n",
    "    \n",
    "class GeologicalKnowledgeFramework(object):\n",
    "    \"\"\"A GeologicalKnowledgeFramework holds the definition of concepts and relationships describing knowledge.\n",
    "    \n",
    "    This is typically an overlay around a formal ontology definition, which also brings additional capabilities,\n",
    "    such as algorithms and factories to achieve specific tasks and create objects.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, source, source_directory= None, backend= None):\n",
    "        \"\"\"Initialise a KnowledgeFramework form a given ontology file (source).\n",
    "        \n",
    "        Parameters:\n",
    "        - name: should be the name under which this KnowledgeFramework is known in the manager\n",
    "        - source: the source file for the ontology definition\n",
    "        - source_directory: the directory where the source files for the ontology definition are looked for.\n",
    "        If None (default) the default path provided by the `KnowledgeManager` is used.\n",
    "        - backend: the ontology backend to be used for this knwoledge framework.\n",
    "        If None (default) the default ontology backend provided by the `KnowledgeManager` is used.\"\"\"\n",
    "        self.name= name\n",
    "        print(source)\n",
    "        self.__source_directory= None\n",
    "        self.init_source_directory(source_directory)\n",
    "        self.initialise_ontology_backend(backend)\n",
    "        print(source)\n",
    "        self.load_ontology(source)\n",
    "    \n",
    "    def init_source_directory(self, source_directory):\n",
    "        \"\"\"Initialises the folder where source files are searched.\n",
    "        \n",
    "        Parameters:\n",
    "        - source_directory: if None, the previous value is used if it wasn't None, else the `GeologicalKnowledgeManager`default is used.\"\"\"\n",
    "        if source_directory is not None:\n",
    "            self.__source_directory= source_directory\n",
    "        elif self.__source_directory is None:\n",
    "            self.__source_directory= GeologicalKnowledgeManager().default_source_directory\n",
    "    \n",
    "    def initialise_ontology_backend(self, backend_name:str= None):\n",
    "        \"\"\"Initializes the ontology package used as a backend to access ontologies.\n",
    "        \n",
    "        This will:\n",
    "        - try to import the backend as onto\n",
    "        - set the default path for ontologies\"\"\"\n",
    "                \n",
    "        self.__ontology_backend = None\n",
    "        backend_name= GeologicalKnowledgeManager().default_ontology_backend if backend_name is None else backend_name\n",
    "        if backend_name == \"owlready2\":\n",
    "            try:\n",
    "                import owlready2 as owl2 \n",
    "                self.__ontology_backend = owl2\n",
    "                if self.__source_directory not in self.__ontology_backend.onto_path:\n",
    "                    self.__ontology_backend.onto_path.append(self.__source_directory)\n",
    "            except ImportError:\n",
    "                raise ImportError(\"Your are trying to use Owlready2 as a backend for ontology management, but it doesn't appear to be installed.\"\\\n",
    "                \"This is either because OwlReady2 is given as default option or because you asked for it.\"\\\n",
    "                \"Please install the OwlReady2 package from https://owlready2.readthedocs.io\"\\\n",
    "                \"or give another backend through GeologicalKnowledgeManager().initialise_ontology_backend()\")\n",
    "                \n",
    "            # also test if java is correctly installed & accessible, as it is used by owlready2 for reasoning\n",
    "            try:\n",
    "                os.system(\"java -version\")\n",
    "            except:\n",
    "                raise ImportError(\"Java doesn't appear to be installed properly as the command `java -version` returned an error.\"\\\n",
    "                    \"This error occured while loading owlready2 package as an ontology backend, because java is used for the reasoning engine.\")\n",
    "        else:\n",
    "            raise Exception(\"The specified backed for ontology is not supported: \"+backend_name)\n",
    "          \n",
    "        \n",
    "    def load_ontology(self, source):\n",
    "        \"\"\"Loads the ontology specified by source.\n",
    "        \n",
    "        Parameters:\n",
    "        - source: the source file for the ontology definition\n",
    "        - source_directory: the directory where the source files for the ontology definition are looked for.\n",
    "        If None (default) the default path provided by the `KnowledgeManager` is used.\"\"\"\n",
    "        self.__source= source\n",
    "        print(source)\n",
    "        try:\n",
    "            self.__onto = self.__ontology_backend.get_ontology(self.__source).load()\n",
    "        except Exception as err:\n",
    "            raise Exception(\"Unexpected exception received while loading ontology:\\n - source: {}\\n - onto_path: {}\".format(self.__source, self.__ontology_backend.onto_path))\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.__onto\n",
    "        \n",
    "    def get_ontology_backend(self):\n",
    "        \"\"\"Gets the ontology backend\"\"\"\n",
    "        assert self.__ontology_backend is not None, \"Trying to access the ontology backend without initialising it.\"\n",
    "        return self.__ontology_backend\n",
    "    \n",
    "    def search(self, name= None, type= None, qualities= None, prepend_star=True) -> list:\n",
    "        \"\"\"Search function to interface the serach capabilities of the internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the search object (you can use * to replace any set of characters and ? to replace any single character)\n",
    "        Note: if `prepend_star` a * is always prepended to allows the search to work because of the internal prefix names\n",
    "        - type: the type of the searched observations as defined by the internal ontology\n",
    "        - qualities: qualities to filter the observations.\n",
    "        If `qualities` is a :\n",
    "         * `str`: a single quality will be searched for with any value (\"*\"),\n",
    "         * `list`: a list of qualities will be searched for with any values (\"*\")\n",
    "         * `dict`: a list of qualities defined by the keys and with the associated values will be searched for\"\"\"\n",
    "        if name is None:\n",
    "            name = \"*\"\n",
    "        elif prepend_star:\n",
    "            name = \"*\"+name\n",
    "        \n",
    "        if isinstance(qualities,list):\n",
    "            kargs = {quality_i: \"*\" for quality_i in qualities} \n",
    "        elif isinstance(qualities,str):\n",
    "            kargs = {qualities: \"*\"}\n",
    "        elif isinstance(qualities,dict):\n",
    "            kargs = qualities\n",
    "        else:\n",
    "            kargs = {}\n",
    "            assert (qualities is None) or isinstance(qualities,dict), \"qualities should be given as either None, a str, a list, or a dict\"\n",
    "        if type is not None: kargs[\"type\"] = type\n",
    "        return self.__onto.search(iri= name, **kargs)\n",
    "    \n",
    "    def sync_reasoner(self, **kargs):\n",
    "        \"\"\"Synchronise the reasoner.\n",
    "        \n",
    "        Parameters:\n",
    "        - **kargs:\n",
    "        |-infer_property_values\"\"\"\n",
    "        self.__ontology_backend.sync_reasoner(**kargs)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our approach, geological datasets will be progressively interpreted in terms of structural objects,<br>\n",
    "based on a formal definition of concepts own by a **GeologicalKnowledgeManager**.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "GeologicalKnowledgeManager().get_knowledge_framework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeFramework(\"mogi\",\"mogi.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().knowledge_framework_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(type= mogi().Ponctual_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().D1.dip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(type= mogi().Ponctual_Observation, dip= \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[obs_i.dip[0] for obs_i in mogi().search(type= mogi().Ponctual_Observation, dip= \"*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[obs_i.dip[0] for obs_i in mogi().search(type= mogi().Ponctual_Observation, dip= 45)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{obs_i.name: obs_i.dip[0] for obs_i in mogi().search(type= mogi().Ponctual_Observation, dip= \"*\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(type= mogi().Ponctual_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(type= mogi().Ponctual_Observation, qualities= \"dip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(type= mogi().Ponctual_Observation, qualities= [\"dip\",\"occurrence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(type= mogi().Ponctual_Observation, qualities= {\"dip\":45})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(qualities= {\"dip\":45})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(name=\"*D1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(name=\"D1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distinguish two kind of operations here:\n",
    "* representation\n",
    "* visualisation\n",
    "\n",
    "A representation is a formal description of how something appears in a given representation space, but it doesn't have to be visualised.<br>\n",
    "A visualisation takes care of the rendering of a representation with a given support (image, screen)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation should also be made a bit more abstract.<br>\n",
    "1. There is a variety of object that can be rendered in a representation space (typically, different kinds of a dataset components)\n",
    "2. Several kinds of representation spaces could be envisionned (e.g., spatial 1D,2D,3D, or temporal, or just an abstract text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RepresentationSpace(object):\n",
    "    \"\"\"A general framework for Representating geological objects\"\"\"\n",
    "    \n",
    "class TemporalRepresentationSpace(RepresentationSpace):\n",
    "    \"\"\"A `RepresentationSpace` representing temporal apsects of represented objects.\"\"\"\n",
    "    \n",
    "class PhysicalRepresentationSpace(RepresentationSpace):\n",
    "    \"\"\"A type of `RepresentationSpace` representing physical aspects of the represented objects.\"\"\"\n",
    "    \n",
    "    __default_coordinate_labels = [\"X\",\"Y\",\"Z\"]\n",
    "    \n",
    "    def __init__(self, dimension: int=None, coordinate_label: str|list= None, dataset= None, **kargs):\n",
    "        \"\"\"Initialisation of the representation space.\n",
    "        \n",
    "        Parameters:\n",
    "        - dimension (int): specify the number of dimensions of the representation space, typically 1D, 2D, or 3D (i.e., 1, 2, or 3),\n",
    "        NB: larger dimension spaces are not supported. At least either the `dimension` parameter or `coordinate_label` parameter should be given.\n",
    "        - coordinate_label(str|list(str)): gives the label(s) of the coordinates. If given, the number of dimensions is deduced from the size of the list\n",
    "        and `dimensions`is ignored, otherwise, the labels are taken from the `__default_coordinate_labels` based on the number of `dimension`s. \n",
    "        At least either the `dimension` parameter or `coordinate_label` parameter should be given.\n",
    "        - dataset: a Dataset object containing the data to be attached to this representation space.\n",
    "        Note that the RepresentationSpace can be created first and then updated automatically when creating the dataset attached to this space.\n",
    "        - **kargs:\n",
    "            - use_extension: if True, uses the extension of the dataset, else keeps the current ones\n",
    "            - padding: if use_extension is True, the given paddign will be used to keep a space around the dataset\n",
    "        \"\"\"\n",
    "        assert not (coordinate_label is None and dimension is None), \"At least one of the parameters should be specified\"\n",
    "        if coordinate_label is None:\n",
    "            assert isinstance(dimension, int),\"dimension parameter must be an integer\"\n",
    "            assert dimension in [1,2,3], \"The specified number of dimensions ({:d}) is not supported, should be 1, 2 or 3.\".format(dimension)\n",
    "            self.dimension= dimension\n",
    "            self.coordinate_labels= PhysicalRepresentationSpace.__default_coordinate_labels[:self.dimension]\n",
    "        elif isinstance(coordinate_label,str):\n",
    "            self.dimension= 1\n",
    "            self.coordinate_labels=  [coordinate_label]\n",
    "        elif isinstance(coordinate_label, list):\n",
    "            self.dimension= len(coordinate_label)\n",
    "            self.coordinate_labels= coordinate_label\n",
    "        else:\n",
    "            raise(\"Unsupported initialisation of representation space: dimension({}) and coordinate_label ({}).\\n At least one of the parameters shoudl be specified.\".format(dimension, coordinate_label))\n",
    "    \n",
    "        self.__default_padding= 0.05\n",
    "        self.__datasets= []\n",
    "        self.set_extension()\n",
    "        self.attach_dataset(dataset,**kargs)\n",
    "        \n",
    "    def attach_dataset(self, dataset, use_extension= True, padding= None, **kargs):\n",
    "        \"\"\"Attach a dataset to the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - dataset: a Dataset object to be attached\n",
    "        - use_extension: if True, uses the extension of the dataset, else keeps the current ones\n",
    "        - padding: if use_extension is True, the given paddign will be used to keep a space around the dataset\"\"\"\n",
    "        if dataset == None: return\n",
    "        if dataset in self.__datasets: return\n",
    "        self.data += [dataset]\n",
    "        \n",
    "        if use_extension:\n",
    "            self.set_extension_from_data(padding= padding)\n",
    "        \n",
    "        dataset.register_representation_space(self)\n",
    "        \n",
    "    def set_extension(self, extension:list= None):\n",
    "        \"\"\"Setter for the extension (min,max) of the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - extension: a list containing a pair of min and max value for each dimension of the space.\n",
    "        If None, the default will be set, i.e., [[0,1]] * dimension\"\"\"\n",
    "        if extension is None: \n",
    "            self.extension = [[0,1]]*self.dimension\n",
    "            return\n",
    "        extension= np.array(extension)\n",
    "        assert extension.shape[0] == self.dimension, \"The specified extension ({}) do not match the space dimensions ({})\".format(extension, self.dimension)\n",
    "        assert extension.shape[1] == 2, \"The specified extension should provide both lower andupper bounds for each dimension, given: {}\".format(extension)\n",
    "        self.extension= extension\n",
    "        \n",
    "    def set_extension_from_data(self, padding= None):\n",
    "        \"\"\"Sets the extension of the space from the attached dataset\n",
    "        \n",
    "        If no dataset is attached yet, then default extension are used instead (min:0,max:1).\n",
    "        \n",
    "        Parameters:\n",
    "        - padding: a space that is left around the dataset, either a value compatible with the coordinates, or a list of values of same dimensions.\n",
    "        If None, by default the padding is 5% of the dataset range.\n",
    "        \"\"\"\n",
    "        non_empty_dataset = [data_i for data_i in self.__datasets if data_i.extension is not None]\n",
    "        if len(non_empty_dataset) == 0:\n",
    "            self.set_extension()\n",
    "            return\n",
    "        \n",
    "        if padding is None:\n",
    "            padding= self.__default_padding\n",
    "        else:\n",
    "            try: # check if padding as a dimension\n",
    "                len(padding)\n",
    "            # if not, then use it a a scaling \n",
    "            except TypeError: #just checking it is a number\n",
    "                assert type(padding) == int or type(padding) == float, \"padding should be given as a number (int or float), here: \"+type(padding)\n",
    "                # keep the padding as is in this case\n",
    "            else:# else check its dimensions are ok\n",
    "                assert len(padding) == self.dimension, \"the dimensions of the specified padding (len({})->) should match the space dimension ({})\".format(padding, len(padding), self.dimension)\n",
    "                padding= np.array(padding)\n",
    "                \n",
    "        extension = non_empty_dataset[0].extension\n",
    "        for data_i in non_empty_dataset[1:]:\n",
    "            for dim_i in self.dimension:\n",
    "                extension[dim_i,0] = min(extension[dim_i,0], data_i.extension[dim_i])\n",
    "                extension[dim_i,1] = max(extension[dim_i,1], data_i.extension[dim_i])\n",
    "        \"\"\":todo: use projected coordinates instead of source coordinates, might fail if 3D data projected on a map\"\"\"\n",
    "                \n",
    "        # in any cases, except when padding and data is None\n",
    "        self.center= np.mean(extension, axis= 1)\n",
    "        diff= extension.T - self.center\n",
    "        extension= self.center + (1+2*padding)*diff\n",
    "        self.set_extension(extension.T)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Description of the physical space parameters and data\n",
    "        \"\"\"\n",
    "        desc= [\"Representation space of type: {:s}\".format(type(self).__name__)]\n",
    "        desc+= [\"- Number of dimension(s): {}\".format(self.dimension)]\n",
    "        desc+= [\"- Coordinate label(s): {}\".format(self.coordinate_labels)]\n",
    "        desc+= [\"- Space extension:\"]\n",
    "        for dim_i, lim_i in zip(self.coordinate_labels,self.extension):\n",
    "            desc+= [\" - Coord {}: {}\".format(dim_i, lim_i)]  \n",
    "        return \"\\n\".join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(2)\n",
    "\n",
    "space.set_extension_from_data(padding= None)\n",
    "\n",
    "print(space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhysicalRepresentationSpace(\"depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhysicalRepresentationSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(coordinate_label=[\"X\",\"Y\"])\n",
    "print(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(coordinate_label=[\"X\",\"Z\"])\n",
    "print(space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "* replace storage of data from data_map to ontology directly\n",
    "* function to create and add data to ontology\n",
    "* read table -> data in ontology\n",
    "* visualisation of given data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class GeologicalDataset(object):\n",
    "    \"\"\"A GeologicalDataset gathers information about geological data to be interpreted.\n",
    "    \n",
    "    This class is a hybrid ontology&python class. It is providing pythonic algorithm and high level interface,\n",
    "    while the data is actually stored in an ontology.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, physical_space= None, time_space= None, representation_spaces= None, knowledge_framework= None):\n",
    "        \"\"\"Initialises a `GeologicalDataset`\n",
    "        \n",
    "        Parameters:\n",
    "        - physical_space: a `PhysicalRepresentationSpace`,  which defines the spatial coordinates of this dataset\n",
    "        - time_space: a `TemporalRepresentationSpace`,  which defines the time coordinates of this dataset\n",
    "        - representation_spaces: list of `Representationspace`s to which the dataset must be attached\n",
    "        Note: datasets can be created without representation space and attached later on by using `RepresentationSpace.attach_dataset`\n",
    "        or `GeologicalDataset.register_representation_space`.\n",
    "        Alternativelly, a single `PhysicalRepresentationspace` and or `TemporalRepresentationspace` can be given here if `physical_space` and `time_space` are None.\n",
    "        - default_representation_space: the main RepresentationSpace to which this dataset is attached.\n",
    "        If None and representation_spaces are provided, then the first one will be taken.\n",
    "        If the default one is not initially in the full list, then it is added to it.\n",
    "        - ontology: the name of the ontology to be used for storing the data.\n",
    "        If None, the default will be taken from `the GeologicalKnowledgeManager`.\n",
    "        \n",
    "        Internals: this method initialises several internal attributes:\n",
    "        - extension: represents the extension of the dataset in the attached representation space\n",
    "        (i.e., the default on if this dataset is represented in several representation spaces\n",
    "        - representation_spaces: the dataset can be attached to and represented into several representation spaces,\n",
    "        `physical_space` and `time_space` are included into this list.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.knowledge_framework= knowledge_framework if knowledge_framework is not None else GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "        \n",
    "        self.extension= None\n",
    "        \"\"\"This attribute stores extension of the dataset in its default representation space\"\"\"\n",
    "        \n",
    "        # setup representation spaces\n",
    "        self.representation_spaces= set()\n",
    "        self.setup_representation_space(physical_space, time_space, representation_spaces)\n",
    "        \n",
    "        # register the datast in the listed representation spaces\n",
    "        for space_i in self.representation_spaces:\n",
    "            space_i.attach_dataset(self)\n",
    "\n",
    "    def __setup_space(self, space, space_type):\n",
    "        \"\"\"Check if space of given type is in list or parameter and return the appropriate value.\n",
    "        \n",
    "        Take the given physical/time space, or if None use the first one in the list, and if none just leave None.\n",
    "        Adds the space to the `self.representation_spaces` set.\"\"\"\n",
    "        if space is not None: \n",
    "            self.representation_spaces.add(space)\n",
    "            return space\n",
    "        if len(self.representation_spaces) == 0: return None\n",
    "        space_list= [space_i for space_i in self.representation_spaces if isinstance(space_i, space_type)]\n",
    "        return space_list[0] if len(space_list) > 0 else None\n",
    "        \n",
    "    def setup_representation_space(self, physical_space= None, time_space= None, representation_spaces= None):\n",
    "        \"\"\"Setup the representation space list and default\n",
    "        \n",
    "        Parameters:\n",
    "        - physical_space: a `PhysicalRepresentationSpace`,  which defines the spatial coordinates of this dataset\n",
    "        - time_space: a `TemporalRepresentationSpace`,  which defines the time coordinates of this dataset\n",
    "        - representation_spaces: list of `Representationspace`s to which the dataset must be attached\n",
    "        Note: datasets can be created without representation space and attached later on by using `RepresentationSpace.attach_dataset`\n",
    "        or `GeologicalDataset.register_representation_space`.\n",
    "        Alternativelly, a single `PhysicalRepresentationspace` and or `TemporalRepresentationspace` can be given here if `physical_space` and `time_space` are None.\n",
    "        \"\"\"\n",
    "        if representation_spaces is not None: self.representation_spaces = self.representation_spaces.union(representation_spaces)\n",
    "        self.physical_space = self.__setup_space(physical_space, PhysicalRepresentationSpace)\n",
    "        self.time_space = self.__setup_space(time_space, TemporalRepresentationSpace)\n",
    "                \n",
    "    def register_representation_space(self, space):\n",
    "        \"\"\"Register the given representation space as a space of representation of this dataset\"\"\"\n",
    "        if space is None: return \n",
    "        self.representation_spaces.add(space)\n",
    "        if (self.physical_space is None) and isinstance(space, PhysicalRepresentationSpace):\n",
    "            self.physical_space = space\n",
    "        if (self.time_space is None) and isinstance(space, TemporalRepresentationSpace):\n",
    "            self.time_space = space\n",
    "        space.attach_dataset(self)\n",
    "        \n",
    "    def get_observations(self, observation_type= None, qualities= None, name= None):\n",
    "        \"\"\"Accessor to the observations stored in the internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - observation_type: the type of the searched observations as defined by the internal ontology\n",
    "        - qualities: qualities to filter the observations (c.f. `KnowledgeFramework.search`)\n",
    "        - name: name of the serached observation (c.f. `KnowledgeFramework.search`)\"\"\"\n",
    "        observation_type = observation_type if observation_type is not None else self.knowledge_framework().Ponctual_Observation\n",
    "        return self.knowledge_framework.search(type= observation_type, qualities= qualities, name= name)\n",
    "    \n",
    "    def get_occurrence_observations(self):\n",
    "        \"\"\"helper method to access occurrence data, i.e., those having a occurrence quality\n",
    "        \n",
    "        :todo: for now the occurrence quality doesn't exist so all the observations are occurrence by default\"\"\"\n",
    "        return self.get_observations() # qualities= \"occurrence\")\n",
    "    \n",
    "    def get_orientation_observations(self):\n",
    "        return self.get_observations(qualities= \"dip\")\n",
    "    \n",
    "    def remove_observation(self, observations):\n",
    "        \"\"\"Removes the given observations stored in this dataset and internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - observations: an iterable containing objects of the internal ontology.\n",
    "        Note that you can use the `search`method to get such a list\"\"\"\n",
    "        for data_i in observations:\n",
    "            self.knowledge_framework.get_ontology_backend().destroy_entity(data_i)\n",
    "            \n",
    "    def remove_observation_by_name(self, name:str):\n",
    "        \"\"\"Removes the given observations stored in this dataset and internal ontology\"\"\"\n",
    "        self.remove_observation(self.get_observations(name= name))\n",
    "        \n",
    "    def remove_all_observations(self):\n",
    "        \"\"\"Removes all the observations stored in this dataset and internal ontology\"\"\"\n",
    "        self.remove_observation(self.get_observations())\n",
    "        \n",
    "    def add_observation(self, name: str, **kargs):\n",
    "        \"\"\"creates a new observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\"\"\"\n",
    "        self.knowledge_framework().Ponctual_Observation(name= name, **{key:[val] for key, val in kargs.items()})\n",
    "        \n",
    "    def add_occurrence_observation(self, name: str, observed_object:str, occurrence= True, **kargs):\n",
    "        \"\"\"creates a new occurrence observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - observed_object: the name of the observed object\n",
    "        - occurrence: True (default) if the object was observed here, False if it was observed that it is not there.\n",
    "        Note that this is different from not having observed that it is here, in which case there should not be an observation.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\"\"\"\n",
    "        if observed_object is not None: kargs[\"geology\"]= observed_object\n",
    "        if occurrence is not None: kargs[\"occurrence\"]= occurrence\n",
    "        self.add_observation(name,**kargs)\n",
    "    \n",
    "    def add_orientation_observation(self, name: str, observed_object:str, dip, dip_dir, occurrence= True, **kargs):\n",
    "        \"\"\"creates a new orientation observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - observed_object: the name of the observed object\n",
    "        - dip: the value of the measured dip (in degrees, 0-90)\n",
    "        - dip_dir: the value of the dip direction (in degrees, 0-360, from North towards the East)\n",
    "        - occurrence: True (default) if the object was observed here.\n",
    "        This is the default behaviour because if the measurement was made here, we assume that the object actually existed\n",
    "        so this is in itself a proof ox occurrence. However, one might want to record the orientation without specifically attaching any\n",
    "        observation of occurrence, in which case None should be given for occurrence and the quality won't be set.\n",
    "        False, would not make much sense as it would imply that the orientation was measured but we observed that the object wasn't there.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\"\"\"\n",
    "        if occurrence is not None: kargs[\"occurrence\"]= occurrence\n",
    "        if occurrence == False: logging.warning(\"occurrence parameter was set to False while adding an orientation observation.\"\\\n",
    "            \"This is weird because it would imply the measure was taken but the rock couldn't be observed.\"\\\n",
    "            \"Did you intend to avoid recording the occurrence, in which case you should prefer None isntead of False.\")\n",
    "        \n",
    "        if observed_object is not None: kargs[\"geology\"]= observed_object\n",
    "        if (dip is not None) and (dip_dir is not None):\n",
    "            kargs[\"dip\"]= dip\n",
    "            kargs[\"dip_dir\"]= dip_dir\n",
    "        self.add_observation(name,**kargs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.get_observations())\n",
    "         \n",
    "    def __str__(self):\n",
    "        \"\"\"Description of the dataset\n",
    "        \"\"\"\n",
    "        desc= [\"A dataset of type: {:s}\".format(type(self).__name__)]\n",
    "        n = len(self)\n",
    "        if n == 0:\n",
    "            desc+= [\"The dataset is empty\"]\n",
    "        else:\n",
    "            desc+= [\"Size of dataset: {:d}\".format(n)]\n",
    "            desc+= [\"Extension: \"+str(self.extension)]\n",
    "            desc+= [\"- types of data:\"]\n",
    "            desc+= [\" | occurrence:\\t{} entries\".format(len(self.get_occurrence_observations()))]\n",
    "            desc+= [\" | orientation:\\t{} entries\".format(len(self.get_orientation_observations()))]  \n",
    "        return \"\\n\".join(desc) \n",
    "    \n",
    "def load_dataset_from_csv(source:str, dataset:GeologicalDataset = None, **kargs) ->GeologicalDataset:\n",
    "    \"\"\"Loads a dataset from a csv file\n",
    "    \n",
    "    Parameters:\n",
    "    - source(str): the source file from which the data should be loaded\n",
    "    - dataset: the `GeologicalDataset` in which the data will be loaded. If None, the dataset will be created.\n",
    "    - **arkgs: passed to pandas.read_csv\n",
    "    \n",
    "    Return:\n",
    "    - the `GeologicalDataset` with the newly loaded data (a new `GeologicalDataset` is created if needed).\"\"\"\n",
    "    try:\n",
    "        dataframe = pd.read_csv(source, **filter_kargs(pd.read_csv,**kargs))\n",
    "    except Exception as e:\n",
    "        e.add_note(\"This error occurred while loading a dataset from: \", source)\n",
    "        e.add_note(\"Additional arguments were given: \", *[\"{}:{}\".format(key,val) for key, val in kargs.items()])\n",
    "        \n",
    "    if len(dataframe.columns) < 3: logging.warning(\"There are less than 3 columns in the loaded dataset.\\nCheck the output and consider changing the separator with sep keyword.\")\n",
    "    return load_dataset_from_dataframe(dataframe, dataset)\n",
    "\n",
    "def load_dataset_from_dataframe(dataframe, dataset:GeologicalDataset = None, labels= None, index= \"Id\", dtypes= None):\n",
    "    \"\"\"Loads a dataset from a `pandas.DataFrame`\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe(`pandas.DataFrame`): the source dataframe from which the data should be loaded\n",
    "    - dataset: the `GeologicalDataset` in which the data will be loaded. If None, the dataset will be created.\n",
    "    - labels: a dict to relabel the dataframe columns prior to loading in the dataset.\n",
    "    This is usefull for example when the coordinates in the source aren't labelled the same as in the internal ontology.\n",
    "    The format is {\"old_label\":\"new_label\", ...}.\n",
    "    - index: the label of the column (in original DataFrame, i.e., before renaming), which is to be used as index\n",
    "    - dtypes: a dict containing a mapping between column name and type\n",
    "    ...\n",
    "    \n",
    "    Return:\n",
    "    - the `GeologicalDataset` with the newly loaded data (a new `GeologicalDataset` is created if needed).\"\"\"\n",
    "    if dataset is None:\n",
    "        dataset = GeologicalDataset()\n",
    "\n",
    "    dtypes= dtypes if dtypes is not None else {'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'geology':str, 'observed_object':str, \"occurrence\":bool}\n",
    "    if labels is not None:\n",
    "        dataframe = dataframe.rename(columns= labels)\n",
    "        index= labels[index] if index in labels else index\n",
    "        dtypes= {labels[key] if key in labels else key: value for key, value in dtypes.items()}\n",
    "    dataframe = dataframe.set_index(index)\n",
    "    output_frame = output_frame.astype(dtypes)\n",
    "        \n",
    "    for name_i, values_i in dataframe.iterrows():\n",
    "        dataset.add_observation(name_i, **values_i)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def filter_kargs(target_function,**kargs):\n",
    "    \"\"\"Helper function to filter keyword arguments and only pass the needed ones in a function signature\"\"\"\n",
    "    sig = inspect.signature(target_function)\n",
    "    # check if there is a **kargs in the signature of the function, if yes it is ok as it will take care of the passed extra kargs\n",
    "    if not any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values()):\n",
    "        extra_args = kargs.keys() - sig.parameters.keys()\n",
    "        for args in extra_args:\n",
    "            del kargs[args]\n",
    "    return kargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = inspect.signature(pd.read_csv)\n",
    "sign.parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = inspect.signature(load_dataset_from_csv)\n",
    "[p.kind for p in sign.parameters.values()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= GeologicalDataset()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_observations(name=\"D8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_observation_by_name(\"D8\")\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_all_observations()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_occurrence_observation(name=\"DD\", observed_object= \"Keuper\", occurrence= True, x= 1, z= 2, y= 0)\n",
    "print(dataset)\n",
    "\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_orientation_observation(name=\"DO\", observed_object= \"Trias\", dip= 30, dip_dir= 270, x= 1, z= 2, y= 0)\n",
    "print(dataset)\n",
    "\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = mogi().DD\n",
    "for prop in di.get_properties():\n",
    "    for value in prop[di]:\n",
    "        print(prop,\":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = mogi().DO\n",
    "for prop in di.get_properties():\n",
    "    for value in prop[di]:\n",
    "        print(prop,\":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(di.get_properties())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = load_dataset_from_csv(\"../inputs/data_for_paper.csv\", labels={\"ID\":\"Id\", \"X\":\"x\",\"Y\":\"y\",\"Z\":\"z\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kargs = {\"labels\":{\"ID\":\"Id\", \"X\":\"x\",\"Y\":\"y\",\"Z\":\"z\"}}\n",
    "filter_kargs(pd.read_csv,**kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = load_dataset_from_csv(\"../inputs/data_for_paper.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset\n",
    "\n",
    "Data are actually described within the ontology, here thanks to the *Data* class.<br>\n",
    "Adding new data points calls for creating new *Data* individuals (i.e., instances in the ontology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_head = np.array(['name', 'x', 'y', 'z', 'dip_dir', 'dip', 'observed_object','occurrence'])\n",
    "data_array = np.array([['D1', 15, 20, 35, 270, 45, 'Trias_Base',True],\n",
    "                       ['D2', 30, 25, 50, 270, 45, 'Trias_Base',True],\n",
    "                       ['D3', 60, 30, 40, 90, 45, 'Trias_Base',True],\n",
    "                       ['D4', 75, 15, 25, 90, 45, 'Trias_Base',True],\n",
    "                       ['D5', 110, 20, 40, 270, 63, 'Trias_Base',True],\n",
    "                       ['D6', 120, 20, 60, 270, 64, 'Trias_Base',True],\n",
    "                       ['D7', 155, 20, 60, 89, 39, 'Trias_Base',True],\n",
    "                       ['D8', 190, 20, 30, 91, 40, 'Trias_Base',True],\n",
    "                       ['D11', 25, 22, 45, np.nan, np.nan, np.nan,True],\n",
    "                       ['D22', 50, 22, 50, np.nan, np.nan, np.nan,True],\n",
    "                       ['D44', 100, 30, 20, np.nan, np.nan, np.nan,True],\n",
    "                       ['D77', 168, 30, 47, np.nan, np.nan, np.nan,True]]\n",
    ")\n",
    "data_test = pd.DataFrame(data = data_array, columns = data_head)\n",
    "data_test = data_test.astype({'name':str, 'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'observed_object':str, 'occurrence':bool})\n",
    "data_test.set_index(\"name\", inplace = True)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_all_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the dataset in the ontology by creating individuals\n",
    "for name_i, values_i in data_test.iterrows():\n",
    "    mogi().Ponctual_Observation(name_i, **{key:[val] for key, val in values_i.items()})\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mogi().D1.get_properties())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading dataset from the ontology\n",
    "output_frame = pd.DataFrame(columns=[\"name\",\"x\",\"y\",\"z\",\"dip_dir\",\"dip\",'geology'])\n",
    "output_frame.set_index(\"name\",inplace=True)\n",
    "for di in mogi.search(type = mogi().Ponctual_Observation):\n",
    "    for prop in di.get_properties():\n",
    "        for value in prop[di]:\n",
    "            output_frame.loc[di.name,prop.name] = value\n",
    "output_frame = output_frame.astype({'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'geology':str})\n",
    "output_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(center, dip, dir, length= 1, ax= None, color = \"black\", **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "\n",
    "    center = np.array(center)\n",
    "    dip_rad = np.deg2rad(dip)\n",
    "    vec_x =  np.cos(dip_rad)\n",
    "    if dir == \"left\": vec_x *= -1\n",
    "    vec_z = -np.sin(dip_rad)\n",
    "    vect = 0.5 * length * np.array([vec_x,vec_z])\n",
    "    start = center - vect\n",
    "    end = center + vect\n",
    "    ax_plt.plot([start[0],end[0]],[start[1],end[1]], color = color, **kargs)\n",
    "    \n",
    "    return vect\n",
    "    \n",
    "def draw_dip_symbol(center, dip, dir, length= 1, polarity= None, ax= None, color = \"black\", polarity_ratio= 0.4, **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "    \n",
    "    vect = draw_line(center= center, dip= dip, dir= dir, length= length, ax= ax_plt, color = color, **kargs)\n",
    "    \n",
    "    if polarity is not None:\n",
    "        vect_pol = polarity_ratio * np.array([-vect[1],vect[0]])\n",
    "        if (dir == \"left\" and polarity == \"up\") or (dir == \"right\" and polarity == \"down\") : vect_pol *= -1\n",
    "        ax_plt.arrow(*center,*vect_pol, width=length/100, color = color, **kargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_line([0,0],30, \"left\")\n",
    "draw_dip_symbol([0,1],60, \"right\", polarity= \"up\", color= \"red\" )\n",
    "plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dataset( dataset, ax= None, **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "    \n",
    "    for data_i in dataset.itertuples():\n",
    "        if (data_i.dip != np.nan) and (data_i.dip_dir != np.nan):\n",
    "            dir = \"right\" if data_i.dip_dir < 180 else \"left\"\n",
    "            draw_dip_symbol( center= [data_i.x,data_i.z], dip= data_i.dip, dir= dir, **kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dataset(dataset, length=10, polarity=\"up\")\n",
    "plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(dataset.itertuples()).dip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the dataset in the ontology by creating individuals\n",
    "for name_i, values_i in dataset.iterrows():\n",
    "    mogi.Ponctual_Observation(name_i, **{key:[val] for key, val in values_i.items()})\n",
    "mogi.search(type = mogi.Ponctual_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading dataset from the ontology\n",
    "dataset = pd.DataFrame(columns=[\"name\",\"x\",\"y\",\"z\",\"dip_dir\",\"dip\",'geology'])\n",
    "dataset.set_index(\"name\",inplace=True)\n",
    "for di in mogi.search(type = mogi.Ponctual_Observation):\n",
    "    for prop in di.get_properties():\n",
    "        for value in prop[di]:\n",
    "            dataset.loc[di.name,prop.name] = value\n",
    "dataset = dataset.astype({'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'geology':str})\n",
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation process in itself is run in a **GeologicalInterpretationProcess** and follow a very simple and generic algorithm.<br>\n",
    "This algorithm implements a Deming wheel process of continual improvement:\n",
    "1. Plan:\n",
    "    1. Select a situation\n",
    "    2. Select an action\n",
    "2. Do: Implement the action (e.g., CreateInterpretationElement)\n",
    "    1. List features\n",
    "    2. Identify possible explanations\n",
    "    3. Rank/chose explanations\n",
    "    4. Instanciate individuals\n",
    "    5. Infer and set parameters\n",
    "3. Check: Evaluate consistency\n",
    "    1. Evaluate internal consistency\n",
    "    2. Evaluate relational likelihood\n",
    "    3. Evaluate feature explanation\n",
    "4. Act: Generate anomalies and report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeologicalInterpretationProcess(object):\n",
    "    \"\"\"GeologicalInterpretationProcess implements the core process of a geological intepretation.\n",
    "    \n",
    "    It connects all the required elements and resulting artefacts relatively to a given interpretation sequence:\n",
    "     - a GeologicalKnowledgeFramework\"\"\"\n",
    "     \n",
    "    def __init__(self, dataset: GeologicalDataset, knowledge_framework= None):\n",
    "         \"\"\"Creates a GeologicalInterpretationProcess\n",
    "         \n",
    "         ---------------------------\n",
    "         Parameters:\n",
    "         - dataset (GeologicalDataset): a dataset to be explained by this interpretor\n",
    "         - knowledge_framework: a GeologicalKnowledgeFramework that defines the concepts used for this interpretation.\n",
    "            If None is given, the the default knowledge framework is used (`GeologicalKnowledgeManager().get_knowledge_framework()`)\n",
    "         \"\"\"\n",
    "         self.knowledge_framework= GeologicalKnowledgeManager().get_knowledge_framework() if knowledge_framework is None else knowledge_framework\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
