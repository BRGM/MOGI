{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geological Interpretor Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for testing and developping some of the basic code in this package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import owlready2 as owl\n",
    "class MalissiaBaseError(Exception):\n",
    "    \"\"\"This is the base class for all exceptions of this package\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owl.onto_path.append(\"../ontologies/\")\n",
    "mogi = owl.get_ontology(\"mogi.owl\").load()\n",
    "mogi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geological Knowledge Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GeologicalKnowledgeManager** may know different instances of **GeologicalKnowledgeFramework**,<br>\n",
    "for example to allow differenciating scenarios or for allowing customisation of knowledge and its formalisation.\n",
    "\n",
    "**GeologicalKnowledgeFramework** provides access to concept definitions for providing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class GeologicalKnowledgeManager(object):\n",
    "    \"\"\"GeologicalKnowledgeManager is managing one or several GeologicalKnowledgeFramework.\n",
    "    \n",
    "    The GeologicalKnowledgeManager is typically a singleton, so there is always one and only one instance of it.\n",
    "    \n",
    "    The GeologicalKnowledgeManager may know different instances of GeologicalKnowledgeFramework,\n",
    "    for example to allow different interpretation scenarios or for allowing user-specific customisation\n",
    "    of knowledge and its formalisation.\n",
    "    \n",
    "    GeologicalKnowledgeFramework are typically ontologies and extensions defined in this package or elsewhere.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __new__(cls):\n",
    "        \"\"\"Method to access (and create if needed) the only allowed instance of this class.\n",
    "        \n",
    "        Returns:\n",
    "        - an instance of GeologicalKnowledgeManager\"\"\"\n",
    "        if not hasattr(cls, 'instance'):\n",
    "            cls.instance = super(GeologicalKnowledgeManager, cls).__new__(cls)\n",
    "            cls.initialised= False\n",
    "        return cls.instance\n",
    "        \n",
    "    def __init__(self, default= \"mogi\", default_source_directory= \"../ontologies/\", default_source_file= \"mogi.owl\", default_ontology_backend= \"owlready2\"):\n",
    "        \"\"\"Initializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        if not self.initialised:\n",
    "            self._initialise(default= default, default_source_directory= default_source_directory, default_source_file= default_source_file, default_ontology_backend= default_ontology_backend)\n",
    "            \n",
    "    def _initialise(self, default, default_source_directory, default_source_file, default_ontology_backend):\n",
    "        \"\"\"Initializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        self.default= default\n",
    "        self.default_source_directory= default_source_directory\n",
    "        self.default_source_file= default_source_file\n",
    "        self.default_ontology_backend= default_ontology_backend\n",
    "        \n",
    "        self.knowledge_framework_dict = {}\n",
    "        \n",
    "        self.initialised= True\n",
    "        \n",
    "    def reset(self, default= \"mogi\", default_source_directory= \"../ontologies/\", default_source_file= \"mogi.owl\", default_ontology_backend= \"owlready2\"):\n",
    "        \"\"\"Reinitializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        self._initialise(default= default, default_source_directory= default_source_directory, default_source_file= default_source_file, default_ontology_backend= default_ontology_backend)\n",
    "             \n",
    "    def load_knowledge_framework(self, name=None, source= None, source_directory= None, backend= None):\n",
    "        \"\"\"Gets and initilises the ontology from the specified source.\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name to be given to the knowledge framework. If None (default) the file name will be used.\n",
    "        - source: filename to the ontology source. If None(default) the default ontology is used.\n",
    "        - source_directory: where the system should look for ontology definition files. If None, the `GeologicalKnowledgeFramework` will decide.\n",
    "        - backend: the ontology backend to be used. If None, the `GeologicalKnowledgeFramework` will decide.\"\"\"\n",
    "        source = source if source is not None else self.default_source_file\n",
    "        name = name if name is not None else os.path.basename(source).split(os.path.extsep)[0]\n",
    "        self.knowledge_framework_dict[name] = GeologicalKnowledgeFramework(name= name, source= source, source_directory= source_directory, backend= backend)\n",
    "    \n",
    "    def get_knowledge_framework(self,name= \"default\"):\n",
    "        \"\"\"Accessor to knowledge frameworks.\"\"\"\n",
    "        name = self.default if name == \"default\" else name\n",
    "        assert len(self.knowledge_framework_dict) > 0, \"No ontology has been loaded yet. Please use GeologicalKnowledgeManager().load_knowledge_framework() first\"\n",
    "        assert name in self.knowledge_framework_dict.keys(), \"The specified ontology hasn't been loaded: \"+name+\\\n",
    "            \"\\navailable ontology names are: \"+\"\\n\".join(self.knowledge_framework_dict.keys())\n",
    "        return self.knowledge_framework_dict[name]\n",
    "    \n",
    "class GeologicalKnowledgeFramework(object):\n",
    "    \"\"\"A GeologicalKnowledgeFramework holds the definition of concepts and relationships describing knowledge.\n",
    "    \n",
    "    This is typically an overlay around a formal ontology definition, which also brings additional capabilities,\n",
    "    such as algorithms and factories to achieve specific tasks and create objects.\n",
    "    \n",
    "    This knowledge framework also holds a registry of available constructors for making new objects:\n",
    "    - registered_constructors: a dictionnary holding the constructors for a given class, sorted in order of preference,\n",
    "         structured as {class_object or key: [constructor1, constructor2,...]}\n",
    "    - constructor_conditions: conditions for the application of a constructor,\n",
    "       e.g., regarding InterpretationSituation, interpretor status, and parameters.\n",
    "        Structured as dictionnaries {constructor: function_to_evaluate_conditions}\"\"\"\n",
    "\n",
    "    # registry of object constructors and conditions\n",
    "    registered_constructors = {}\n",
    "    constructor_conditions = {}\n",
    "    \n",
    "    def __init__(self, name, source, source_directory= None, backend= None):\n",
    "        \"\"\"Initialise a KnowledgeFramework form a given ontology file (source).\n",
    "        \n",
    "        Parameters:\n",
    "        - name: should be the name under which this KnowledgeFramework is known in the manager\n",
    "        - source: the source file for the ontology definition\n",
    "        - source_directory: the directory where the source files for the ontology definition are looked for.\n",
    "        If None (default) the default path provided by the `KnowledgeManager` is used.\n",
    "        - backend: the ontology backend to be used for this knwoledge framework.\n",
    "        If None (default) the default ontology backend provided by the `KnowledgeManager` is used.\"\"\"\n",
    "        self.name= name\n",
    "        \n",
    "        self._source_directory= None\n",
    "        self.init_source_directory(source_directory)\n",
    "        self.initialise_ontology_backend(backend)\n",
    "        \n",
    "        self.load_ontology(source)\n",
    "            \n",
    "    \n",
    "    def init_source_directory(self, source_directory):\n",
    "        \"\"\"Initialises the folder where source files are searched.\n",
    "        \n",
    "        Parameters:\n",
    "        - source_directory: if None, the previous value is used if it wasn't None, else the `GeologicalKnowledgeManager`default is used.\"\"\"\n",
    "        if source_directory is not None:\n",
    "            self._source_directory= source_directory\n",
    "        elif self._source_directory is None:\n",
    "            self._source_directory= GeologicalKnowledgeManager().default_source_directory\n",
    "    \n",
    "    def initialise_ontology_backend(self, backend_name:str= None):\n",
    "        \"\"\"Initializes the ontology package used as a backend to access ontologies.\n",
    "        \n",
    "        This will:\n",
    "        - try to import the backend as onto\n",
    "        - set the default path for ontologies\"\"\"\n",
    "                \n",
    "        self._ontology_backend = None\n",
    "        backend_name= GeologicalKnowledgeManager().default_ontology_backend if backend_name is None else backend_name\n",
    "        if backend_name == \"owlready2\":\n",
    "            try:\n",
    "                import owlready2 as owl2 \n",
    "                self._ontology_backend = owl2\n",
    "                if self._source_directory not in self._ontology_backend.onto_path:\n",
    "                    self._ontology_backend.onto_path.append(self._source_directory)\n",
    "            except ImportError:\n",
    "                raise ImportError(\"Your are trying to use Owlready2 as a backend for ontology management, but it doesn't appear to be installed.\"\\\n",
    "                \"This is either because OwlReady2 is given as default option or because you asked for it.\"\\\n",
    "                \"Please install the OwlReady2 package from https://owlready2.readthedocs.io\"\\\n",
    "                \"or give another backend through GeologicalKnowledgeManager().initialise_ontology_backend()\")\n",
    "                \n",
    "            # also test if java is correctly installed & accessible, as it is used by owlready2 for reasoning\n",
    "            try:\n",
    "                os.system(\"java -version\")\n",
    "            except:\n",
    "                raise ImportError(\"Java doesn't appear to be installed properly as the command `java -version` returned an error.\"\\\n",
    "                    \"This error occured while loading owlready2 package as an ontology backend, because java is used for the reasoning engine.\")\n",
    "        else:\n",
    "            raise Exception(\"The specified backed for ontology is not supported: \"+backend_name)\n",
    "          \n",
    "        \n",
    "    def load_ontology(self, source):\n",
    "        \"\"\"Loads the ontology specified by source.\n",
    "        \n",
    "        Parameters:\n",
    "        - source: the source file for the ontology definition\n",
    "        - source_directory: the directory where the source files for the ontology definition are looked for.\n",
    "        If None (default) the default path provided by the `KnowledgeManager` is used.\"\"\"\n",
    "        self._source= source\n",
    "        try:\n",
    "            self._onto = self._ontology_backend.get_ontology(self._source).load()\n",
    "        except Exception as err:\n",
    "            raise Exception(\"Unexpected exception received while loading ontology:\\n - source: {}\\n - onto_path: {}\".format(self._source, self._ontology_backend.onto_path))\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self._onto\n",
    "        \n",
    "    def get_ontology_backend(self):\n",
    "        \"\"\"Gets the ontology backend\"\"\"\n",
    "        assert self._ontology_backend is not None, \"Trying to access the ontology backend without initialising it.\"\n",
    "        return self._ontology_backend\n",
    "    \n",
    "    def search(self, name= None, type= None, qualities= None, prepend_star=True) -> list:\n",
    "        \"\"\"Search function to interface the serach capabilities of the internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the search object (you can use * to replace any set of characters and ? to replace any single character)\n",
    "        Note: if `prepend_star` a * is always prepended to allows the search to work because of the internal prefix names\n",
    "        - type: the type of the searched observations as defined by the internal ontology\n",
    "        - qualities: qualities to filter the observations.\n",
    "        If `qualities` is a :\n",
    "         * `str`: a single quality will be searched for with any value (\"*\"),\n",
    "         * `list`: a list of qualities will be searched for with any values (\"*\")\n",
    "         * `dict`: a list of qualities defined by the keys and with the associated values will be searched for\"\"\"\n",
    "        if name is None:\n",
    "            name = \"*\"\n",
    "        elif prepend_star:\n",
    "            name = \"*\"+name\n",
    "        \n",
    "        if isinstance(qualities,list):\n",
    "            kargs = {quality_i: \"*\" for quality_i in qualities} \n",
    "        elif isinstance(qualities,str):\n",
    "            kargs = {qualities: \"*\"}\n",
    "        elif isinstance(qualities,dict):\n",
    "            kargs = qualities\n",
    "        else:\n",
    "            kargs = {}\n",
    "            assert (qualities is None) or isinstance(qualities,dict), \"qualities should be given as either None, a str, a list, or a dict\"\n",
    "        if type is not None: kargs[\"type\"] = type\n",
    "        return self._onto.search(iri= name, **kargs)\n",
    "    \n",
    "    def format_qualities(self, **qualities):\n",
    "        \"\"\"Formats the qualities for setting an instance\n",
    "\n",
    "        It will :\n",
    "         - remove qualities that are not defined in the ontology\n",
    "         - transform scalar values into vectors (for non functional properties)\n",
    "         - filter out None values\n",
    "        \"\"\"\n",
    "        \n",
    "        formated_qualities = {key:val for key, val in qualities.items() \n",
    "                 if (val is not None) and (getattr(self._onto,key) is not None)\n",
    "                 }\n",
    "        for key, val in formated_qualities.items():\n",
    "            if not getattr(self._onto,key).is_functional_for(None):\n",
    "                formated_qualities[key] = val if isinstance(val,list) else [val]\n",
    "        return formated_qualities\n",
    "\n",
    "    def create_instance(self, class_type, name = None, physical_space= None, **qualities):\n",
    "        \"\"\"Creates an instance in the ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - class_type: the type of the class of the object to be created (as in the ontology, e.g., mogi().Surface).\n",
    "        Note: the given class_type must be in the internal ontology stored in this manager.\n",
    "        - name: a string giving the name of the object. If None (default) this name is set automatically\n",
    "        - physical_space: if defined, a PhysicalSpaceRepresentation that handles the spatial qualities given in qualities\n",
    "        - qualities: a series of possible qualities to be added to the object\"\"\"\n",
    "        if class_type not in self._onto.classes():\n",
    "            raise MalissiaBaseError(\"The proposed class type ({}) does not belong to this ontology ({})\".format(class_type, self._onto))\n",
    "            \n",
    "        # checking name\n",
    "        if name == \"\":\n",
    "            logging.warning(\"Trying to create a '{}' with a blank name, passing None instead.\".format(class_type))\n",
    "            name = None\n",
    "        if not isinstance(name,str):\n",
    "            logging.warning(\"Trying to create a '{}' with a name that is not a string, using default naming instead.\".format(class_type))\n",
    "            name = None\n",
    "\n",
    "        #reformating the qualities\n",
    "        \n",
    "        formated_qualities = self.format_qualities(**qualities)\n",
    "        if physical_space:\n",
    "            non_coordinate_qualities = physical_space.filter_qualities(**formated_qualities)\n",
    "            new_individual = class_type(name= name, **non_coordinate_qualities)\n",
    "            physical_space.set_object_coordinates(new_individual, **qualities)\n",
    "        else:\n",
    "            new_individual = class_type(name= name, **formated_qualities)\n",
    "\n",
    "        return new_individual\n",
    "\n",
    "    def remove_instance(self, instance):\n",
    "        self._ontology_backend.destroy_entity(instance)\n",
    "        \n",
    "    def remove_all_instances(self, instances = None):\n",
    "        if instances is None:\n",
    "            instances = self._onto.individuals()\n",
    "        for instance in instances:\n",
    "            self.remove_instance(instance)\n",
    "        \n",
    "    def get_all_classes_of_instance(self,instance):\n",
    "        \"\"\"Returns all the classes of this instance and all its ancestors\n",
    "        \n",
    "        Parameters:\n",
    "        - instance: an instance to be investigated\n",
    "        Returns:\n",
    "        - a set containing all the classes this instance belongs to including the mother classes\n",
    "        Note: be carefull because this is returnin owl.Thing too and may cause issues if not handled\n",
    "        \"\"\"\n",
    "        instance_classes = set(instance.is_instance_of)\n",
    "        instance_classes.discard(self._ontology_backend.Thing)\n",
    "        return set.union(*[instance_class.ancestors() for instance_class in instance_classes])\n",
    "        \n",
    "    def show_instance_qualities(self,instance):\n",
    "        if instance is None: \n",
    "            print(\"The instance is None.\")\n",
    "            return\n",
    "        print(\"instance:\",instance)\n",
    "        print(\"types:\", \",\".join([str(i) for i in instance.is_a]))\n",
    "        print(\"properties:\")\n",
    "        for prop in instance.get_properties():\n",
    "            print(\"|- {}:{}\".format(prop.name, prop[instance]))\n",
    "            \n",
    "    def get_all_instances(self):\n",
    "        return list(self._onto.individuals())\n",
    "    \n",
    "    def show_all_instance_qualities(self, instances= None):\n",
    "        instances = instances if instances is not None else self.get_all_instances()\n",
    "        print(\"Number of instances:\",len(instances))\n",
    "        for i in instances:\n",
    "            self.show_instance_qualities(i)\n",
    "            \n",
    "    def has_quality(self, object, quality):\n",
    "        return getattr(self._onto, quality) in object.get_properties()\n",
    "    \n",
    "    def has_qualities(self, object, qualities):\n",
    "        qualities = np.array(qualities).ravel()\n",
    "        return np.all([self.has_quality(object, quality_i) for quality_i in qualities])\n",
    "            \n",
    "    def get_possible_interpretations_of(self, object):\n",
    "        \"\"\"Returns a list of classes potentially explaining this object\"\"\"\n",
    "        object_classes = object.is_instance_of\n",
    "        if len(object_classes) == 0:\n",
    "            raise MalissiaBaseError(\"Trying to interprete an object without class.\")\n",
    "        possible_interpretations = [interpretation for class_i in object_classes \n",
    "                                    for interpretation in class_i.has_Possible_Explanation ] \n",
    "        return possible_interpretations\n",
    "        \n",
    "    def get_objects_potentially_explained_by(self, object):\n",
    "        \"\"\"Returns the classes this object is potentially explaining.\"\"\"\n",
    "        return object.is_Possible_Explanation_Of\n",
    "    \n",
    "    def isinstance(self, candidate_object, candidate_class):\n",
    "        \"\"\"Checks whether an object is an instance of a given class\"\"\"\n",
    "        return isinstance(candidate_object, candidate_class)\n",
    "    \n",
    "    def issubclass(self, candidate_subclass, candidate_class):\n",
    "        \"\"\"Checks whether an object is an instance of a given class\"\"\"\n",
    "        return issubclass(candidate_subclass, candidate_class)\n",
    "    \n",
    "    def has_representation_of_type(self, candidate_object, representation_type, return_representations= False):\n",
    "        \"\"\"Checks if an instance has a representation of a given type\"\"\"\n",
    "        candidate_object\n",
    "        if (candidate_object.has_Representation is None) or (len(candidate_object.has_Representation) < 1):\n",
    "            return False\n",
    "        reps = np.array(candidate_object.has_Representation)\n",
    "        selected_representations = [self.isinstance(rep_i, representation_type) for rep_i in reps]\n",
    "        if return_representations:\n",
    "            return reps[selected_representations]\n",
    "        else:\n",
    "            return np.any(selected_representations)\n",
    "        \n",
    "    def has_point_representation(self, candidate_object, return_representations= False):\n",
    "        return self.has_representation_of_type(candidate_object, self._onto.Point, return_representations)\n",
    "    \n",
    "    @classmethod\n",
    "    def register_constructor(cls, object_class_name, constructor, condition = None):\n",
    "        \"\"\"Registers a constructor for the given ontology class\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class_name: the type of object to be created (typically, the ontology class name)\n",
    "        - constructor: a function that can be called to generate the given object class.\n",
    "          It will typically be method of the GeologicalKnowledgeFramework and called by\n",
    "          constructor( knowledge_framework= None, interpretation_situation = None, interpretation_status = None, **kargs)\n",
    "        \"\"\"\n",
    "        if (object_class_name is None) or (not isinstance(object_class_name, str)):\n",
    "            raise MalissiaBaseError(\"Registering constructor for an undefined object class.\")\n",
    "        if constructor is None: raise MalissiaBaseError(\"Registering  an undefined constructor for an object class.\")\n",
    "        \n",
    "        if object_class_name in cls.registered_constructors:\n",
    "            cls.registered_constructors[object_class_name] += [constructor]\n",
    "        else:\n",
    "            cls.registered_constructors[object_class_name] = [constructor]\n",
    "        cls.constructor_conditions[constructor] = condition\n",
    "            \n",
    "    def filter_explainable_features(self, object_class, features):\n",
    "        \"\"\"Selects the features that can be explained by a given ontology class.\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class [object]: a type of object used as an explanation\n",
    "        - features: the list of features to be filtered.\n",
    "        Returns:\n",
    "        - a list of features that can be explained by the proposed class\"\"\"\n",
    "        explainable_class_set = set(self.get_objects_potentially_explained_by(object_class))\n",
    "        return [feature_i for feature_i in features \n",
    "                if not self.get_all_classes_of_instance(feature_i).isdisjoint(explainable_class_set)]\n",
    "                # test wether the is an intersection between the classes of the instance and the explainable classes\n",
    "        \n",
    "    def select_object_constructor(self, object_class_name, \n",
    "                                    interpretation_situation = None,\n",
    "                                    interpretation_status = None,\n",
    "                                    random_choice = False,\n",
    "                                    debug= False,\n",
    "                                    **kargs):\n",
    "        \"\"\"Factory function that generates a constructor for the given ontology class\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class_name: the type of object to be created (typically, the ontology class name)\n",
    "        - interpretation_situation: a `InterpretationSituation` providing a context for the creation\n",
    "        - interpretation_status: the status of the current interpretation process,\n",
    "        which might provide contextual information to decide which constructor to generate\n",
    "        - random_choice: if False (default), the first available constructor is selected, else it is picked randomly\n",
    "        - debug: if True, some debug information is sent, default False. \n",
    "        - kargs: keyword arguments passing available qualities and properties.\n",
    "        This will be used to check whether required information is available.\n",
    "        \n",
    "        Return:\n",
    "        - a function that can be called for creating a new instance of the given object.\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            print(\"Looking for a constructor for:\", object_class_name)\n",
    "            print(\" - registered constructors:\\n\", \"\\n\".join([str(constructor) for constructor in self.registered_constructors[object_class_name]]))\n",
    "        if object_class_name not in self.registered_constructors:\n",
    "            raise MalissiaBaseError(\"No available constructor for {}.\".format(object_class_name))\n",
    "        candidate_constructors = [constructor for constructor in self.registered_constructors[object_class_name]\n",
    "                                  if self.constructor_conditions[constructor](\n",
    "                                        knowledge_framework = self,\n",
    "                                        interpretation_situation = interpretation_situation,\n",
    "                                        interpretation_status = interpretation_status,\n",
    "                                        **kargs)\n",
    "                                  ]\n",
    "        if debug:\n",
    "            print(\" - candidate constructors:\\n\", \"\\n\".join([str(constructor) for constructor in candidate_constructors]))\n",
    "        if len(candidate_constructors) == 0:\n",
    "            raise MalissiaBaseError(\"No available constructor for {} in these conditions.\".format(object_class_name))\n",
    "        \n",
    "        selected_constructor = random.choice(candidate_constructors) if random_choice else candidate_constructors[0]\n",
    "        if debug: print(\"Selected constructor:\",str(selected_constructor))\n",
    "        return selected_constructor\n",
    "        \n",
    "    def sync_reasoner(self, **kargs):\n",
    "        \"\"\"Synchronise the reasoner.\n",
    "        \n",
    "        Parameters:\n",
    "        - **kargs:\n",
    "        |-infer_property_values\"\"\"\n",
    "        self._ontology_backend.sync_reasoner(**kargs)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"Describe the knowledge framework\"\"\"\n",
    "        desc = [\"Geological Knowledge Framework:\"]\n",
    "        desc += [\" |- Name: {}\".format(self.name)]\n",
    "        desc += [\" |- Backend: {}\".format(self._ontology_backend)]\n",
    "        desc += [\" |- Source: {}\".format(self._source)]\n",
    "        desc += [\" |- Ontology: {}\".format(self._onto)]\n",
    "        return \"\\n\".join(desc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_coord_dicts(coord_labels, coords):\n",
    "    label_keys = [\"coord{}_label\".format(i+1) for i in range(len(coord_labels))]\n",
    "    coord_keys = [\"coord{}\".format(i+1) for i in range(len(coords))]\n",
    "    coord_label_dict = {label_key:[coord_label_i] for label_key,coord_label_i in zip(label_keys, coord_labels)}\n",
    "    coord_dict = {coord_key:[coord_i] for coord_key,coord_i in zip(coord_keys, coords)}\n",
    "    return coord_label_dict, coord_dict\n",
    "\n",
    "def constructor_point(knowledge_framework, coord_labels, coords, name= None):\n",
    "    coord_label_dict, coord_dict = format_coord_dicts(coord_labels=coord_labels, coords= coords)\n",
    "    point =knowledge_framework().Point(name= name, **coord_label_dict, **coord_dict)\n",
    "    \n",
    "    # it is represented by himself\n",
    "    point.has_Representation = [point]\n",
    "    return point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Point Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructor_vector(knowledge_framework, coord_labels, coords, name= None):\n",
    "    coord_label_dict, coord_dict = format_coord_dicts(coord_labels= coord_labels, coords= coords)\n",
    "    vector =  knowledge_framework().Vector(name= name, **coord_label_dict, **coord_dict)\n",
    "    \n",
    "    # it is represented by himself\n",
    "    vector.has_Representation = [vector]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Planar_Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coord_labels(plan):\n",
    "    node = plan.has_Node0[0]\n",
    "    return [node.coord1_label[0],node.coord2_label[0],node.coord3_label[0]]\n",
    "\n",
    "def get_nodes(plan):\n",
    "    return [plan.has_Node0[0], plan.has_Node1[0], plan.has_Node2[0], plan.has_Node3[0]]\n",
    "\n",
    "def get_coord(nodes):\n",
    "    \"\"\"returns the coordinates of a series of nodes as an array (node_index, coord_index)\"\"\"\n",
    "    return [[node_i.coord1[0],node_i.coord2[0],node_i.coord3[0]] for node_i in nodes]\n",
    "    \n",
    "def compute_center(nodes):\n",
    "    coords = get_coord(nodes= nodes)\n",
    "    center_coord = np.mean(coords,axis=0).tolist() \n",
    "    return center_coord\n",
    "\n",
    "def set_center(knowledge_framework, plan):\n",
    "    nodes = get_nodes(plan= plan)\n",
    "    center_coord = compute_center(nodes= nodes) \n",
    "    coord_labels = get_coord_labels(plan= plan)\n",
    "    \n",
    "    name = plan.name + \"_center\"\n",
    "    center = constructor_point(knowledge_framework= knowledge_framework, coord_labels= coord_labels, coords= center_coord, name= name)\n",
    "    plan.has_Center = [center]\n",
    "    return center\n",
    "\n",
    "def compute_principal_vectors_from_nodes(nodes):\n",
    "    coords = np.array(get_coord(nodes= nodes))\n",
    "    u = coords[1] - coords[0]\n",
    "    v = coords[3] - coords[0]\n",
    "    normal = np.cross(u,v)\n",
    "    normal = normal/np.linalg.norm(normal)\n",
    "    return u.tolist(), v.tolist(), normal.tolist()\n",
    "\n",
    "def compute_size_from_nodes(nodes):\n",
    "    coords = np.array(get_coord(nodes= nodes))\n",
    "    u = coords[1] - coords[0]\n",
    "    size = np.linalg.norm(u)\n",
    "    return float(size)\n",
    "\n",
    "def compute_dip_dir_from_normal(normal):\n",
    "    x,y,z = np.sign(normal[2]) * np.array(normal) / np.linalg.norm(normal)\n",
    "    if z == 1:\n",
    "        return 0.,0.\n",
    "    dip = np.rad2deg(np.arccos(z))\n",
    "    dip_dir = np.rad2deg(np.arctan2(x,y)) % 360\n",
    "    return float(np.round(dip, 3)), float(np.round(dip_dir, 3))\n",
    "\n",
    "def set_attitude(knowledge_framework, plan):\n",
    "    nodes = get_nodes(plan= plan)\n",
    "    coord_labels = get_coord_labels(plan= plan)\n",
    "    u, v, normal = compute_principal_vectors_from_nodes(nodes= nodes)\n",
    "    size = compute_size_from_nodes(nodes= nodes)\n",
    "    plan.size = [size]\n",
    "    \n",
    "    normal_name = plan.name + \"_normal\"\n",
    "    normal_entity = constructor_vector(knowledge_framework= knowledge_framework, coord_labels= coord_labels, coords= normal, name= normal_name)\n",
    "    plan.has_Normal = [normal_entity]\n",
    "    \n",
    "    dip, dip_dir = compute_dip_dir_from_normal(normal= normal)\n",
    "    plan.dip = [dip]\n",
    "    plan.dip_dir = [dip_dir]\n",
    "    return dip, dip_dir, normal, size\n",
    "\n",
    "def set_nodes(plan, nodes):\n",
    "    if(len(nodes) != 4): raise MalissiaBaseError(\"Please give 4 nodes in, {} were given.\".format(len(nodes)))\n",
    "    plan.has_Node0 = [nodes[0]]\n",
    "    plan.has_Node1 = [nodes[1]]\n",
    "    plan.has_Node2 = [nodes[2]]\n",
    "    plan.has_Node3 = [nodes[3]]\n",
    "    plan.has_Representation = nodes\n",
    "    \n",
    "def create_planar_surface(knowledge_framework, nodes, name= None):\n",
    "    plan = knowledge_framework().Planar_Surface(name= name)\n",
    "    set_nodes(plan,nodes)\n",
    "    return plan\n",
    "\n",
    "def constructor_planar_surface_from_nodes(knowledge_framework, nodes, name= None):\n",
    "    plan = create_planar_surface(knowledge_framework= knowledge_framework, nodes= nodes, name= name)\n",
    "    center = set_center(knowledge_framework= knowledge_framework, plan= plan)\n",
    "    dip, dip_dir, normal, size = set_attitude(knowledge_framework= knowledge_framework, plan= plan)\n",
    "    return plan\n",
    "\n",
    "def create_nodes_from_coords(knowledge_framework, coords, coord_labels, name= None):\n",
    "    names = [None if name is None else name + \"_N\" + str(i) for i in range(len(coords))]\n",
    "    nodes = [\n",
    "        constructor_point(knowledge_framework= knowledge_framework, coord_labels= coord_labels, coords= coord_i, name= name_i)\n",
    "        for coord_i, name_i in zip(coords,names)\n",
    "    ]\n",
    "    return nodes\n",
    "\n",
    "def constructor_planar_surface_from_coords(knowledge_framework, coords, coord_labels, name= None):\n",
    "    nodes = create_nodes_from_coords(knowledge_framework= knowledge_framework, coords= coords, coord_labels= coord_labels, name= name)\n",
    "    return constructor_planar_surface_from_nodes(knowledge_framework= knowledge_framework, nodes= nodes)\n",
    "\n",
    "def compute_normal_from_dip_dir(dip, dip_dir, polarity= 1):\n",
    "    dip_rad = np.deg2rad(dip)\n",
    "    dip_dir_rad = np.deg2rad(dip_dir)\n",
    "    z = np.cos(dip_rad)\n",
    "    h = np.sin(dip_rad)\n",
    "    y = h * np.cos(dip_dir_rad)\n",
    "    x = h * np.sin(dip_dir_rad)\n",
    "    normal = polarity * np.array([x,y,z])\n",
    "    return normal.tolist()\n",
    "\n",
    "def compute_principal_vectors_from_dip_dir(dip, dip_dir, polarity= 1):\n",
    "    dip_rad = np.deg2rad(dip)\n",
    "    dip_dir_rad = np.deg2rad(dip_dir)\n",
    "    nz =  np.cos(dip_rad)\n",
    "    h = np.sin(dip_rad)\n",
    "    hy = np.cos(dip_dir_rad)\n",
    "    hx = np.sin(dip_dir_rad)\n",
    "    ny = h * hy\n",
    "    nx = h * hx\n",
    "    normal = polarity * np.array([nx,ny,nz])\n",
    "    \n",
    "    uz = -h\n",
    "    ux = nz * hx\n",
    "    uy = nz * hy\n",
    "    dip_vector = np.array([ux,uy,uz])\n",
    "    \n",
    "    az_vector = np.cross(normal, dip_vector)\n",
    "    \n",
    "    return normal.tolist(), dip_vector.tolist(), az_vector.tolist()\n",
    "\n",
    "def compute_coords(center, dip_vector, az_vector, size):\n",
    "    center     = np.array(center)\n",
    "    dip_vector = np.array(dip_vector)\n",
    "    az_vector  = np.array(az_vector)\n",
    "    coords = np.array([\n",
    "        center - size/2 * dip_vector - size/2 * az_vector,\n",
    "        center + size/2 * dip_vector - size/2 * az_vector,\n",
    "        center + size/2 * dip_vector + size/2 * az_vector,\n",
    "        center - size/2 * dip_vector + size/2 * az_vector\n",
    "    ])\n",
    "    return coords.tolist()\n",
    "\n",
    "def constructor_planar_surface_from_center_attitude(knowledge_framework, coord_labels, center, dip, dip_dir, size= None, polarity= True, name= None):\n",
    "    normal, dip_vector, az_vector = compute_principal_vectors_from_dip_dir(dip= dip, dip_dir= dip_dir, polarity= polarity)\n",
    "    coords = compute_coords(center= center, dip_vector= dip_vector, az_vector= az_vector, size= size)\n",
    "    nodes = create_nodes_from_coords(knowledge_framework= knowledge_framework, coords= coords, coord_labels= coord_labels)\n",
    "    plan = create_planar_surface(knowledge_framework= knowledge_framework, nodes= nodes, name= name)\n",
    "    \n",
    "    plan.dip = [float(dip)]\n",
    "    plan.dip_dir = [float(dip_dir)]\n",
    "    plan.polarity = [translate_polarity(polarity)]\n",
    "    plan.size = [float(size)]\n",
    "    \n",
    "    center_name = plan.name + \"_center\"\n",
    "    center_entity = constructor_point(knowledge_framework= knowledge_framework, coord_labels= coord_labels, coords= center, name= center_name)\n",
    "    plan.has_Center= [center_entity]\n",
    "    \n",
    "    normal_name = plan.name + \"_normal\"\n",
    "    normal_entity = constructor_vector(knowledge_framework= knowledge_framework, coord_labels= coord_labels, coords= normal, name= normal_name)\n",
    "    plan.has_Normal = [normal_entity]\n",
    "    return plan\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Planar Surface anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditions_for_constructor_observation(knowledge_framework:GeologicalKnowledgeFramework, **kargs):\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for observation constructor: knowledge framework\")\n",
    "        return False\n",
    "    if (\"dataset\" not in kargs) or (kargs[\"dataset\"] is None):\n",
    "        print(\"Wrong condition for observation constructor: dataset\")\n",
    "        return False\n",
    "    dataset = kargs[\"dataset\"]\n",
    "    if (dataset.physical_space is None):\n",
    "        print(\"Wrong condition for observation constructor: physical space\")\n",
    "        return False\n",
    "    if not np.all([coord in kargs for coord in dataset.physical_space.coordinate_labels]):\n",
    "        print(\"Wrong condition for observation constructor: coord\")\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "def constructor_observation(knowledge_framework= None, name: str= None,\n",
    "                            dataset= None, physical_space= None, coord_labels= None,\n",
    "                            **kargs):\n",
    "    \"\"\"Constructor of observations\n",
    "    \n",
    "    Parameters:\n",
    "    - knowledge_framework: defining the existing objects, if None it is inferred from the dataset if given\n",
    "    - name: the name of the observation (similar to an observation id). If None a default one is given.\n",
    "    - dataset: (optional) the dataset to which this observation belongs.\n",
    "    If given, the physical space is taken from there unless it is specified\n",
    "    - physical_space: (optional) the physical space in which this observation is defined.\n",
    "    If not given, it is taken from the dataset, or if not dataset is provided, the coordinate_labels must be given\n",
    "    - coord_labels: (optional) a list of coordinate names, required only if no physical space is provided\n",
    "    - kargs: keyword arguments, containing:\n",
    "        - spatial coordinates with names matching the dataset.physical_space coordinate labels\n",
    "        - any other quality that should be associated with the observation\n",
    "         * size: should be set, this is the size of the area represented by this observation\n",
    "         * dip and dip_dir: optional, make it an orientation observation, should both be set together\n",
    "         * polarity: optional, only used if dip an ddip_dir are set, boolean indicating the polarity of the observed feature.\n",
    "         True means up (or towards the dip_dir if vertical), False is the opposite\n",
    "         * occurrence: optional make it an occurrence observation, True (thing has been observed)\n",
    "         or False (it has been observed that the thing is not here). Note: if the observation is not made on the\n",
    "         occurrence of the observed object, don't set this property, do not set it to False.\n",
    "        - qualities whose values is None are not set\n",
    "        - Note that the dataset.physical_space will be asked to filter the provided information\n",
    "        \n",
    "    Return:\n",
    "    - the created observation\n",
    "    \"\"\"\n",
    "    \n",
    "    if knowledge_framework is None:\n",
    "        if dataset is not None:\n",
    "            knowledge_framework = dataset.knowledge_framework\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"provide at least a knowledge framework or a dataset\")\n",
    "    \n",
    "    # create Observation\n",
    "    obs = knowledge_framework().PointBased_Observation(name= name)\n",
    "\n",
    "    # create a point as geometrical support\n",
    "    physical_space = physical_space if physical_space is not None else \\\n",
    "        (dataset.physical_space if dataset is not None else None)\n",
    "    coord_labels = physical_space.coordinate_labels if physical_space is not None else coord_labels\n",
    "    if coord_labels is None:\n",
    "        raise MalissiaBaseError(\"coordinate labels must be specified somehow.\")\n",
    "    coords = [kargs[i] for i in coord_labels]\n",
    "    point = constructor_point(knowledge_framework= knowledge_framework,\n",
    "                              coord_labels= coord_labels, coords= coords,\n",
    "                              name= obs.name + \"_point\")\n",
    "    obs.has_Center = [point]\n",
    "    obs.has_Representation = [point]\n",
    "    \n",
    "    # add size\n",
    "    if \"size\" in kargs:\n",
    "        obs.size = [kargs[\"size\"]]\n",
    "    elif physical_space is not None:\n",
    "        domain_size = float(max(physical_space.get_size()))\n",
    "        obs.size = [domain_size / 10]\n",
    "    else:\n",
    "        obs.size = [1]\n",
    "    \n",
    "    # add observed qualities\n",
    "    if \"occurrence\" in kargs:\n",
    "        obs.occurrence = [kargs[\"occurrence\"]]\n",
    "    if (\"dip\" in kargs) and (\"dip_dir\" in kargs):\n",
    "        obs.dip     = [kargs[\"dip\"]]\n",
    "        obs.dip_dir = [kargs[\"dip_dir\"]]\n",
    "        if \"polarity\" in kargs:\n",
    "            obs.polarity = [translate_polarity(kargs[\"polarity\"])]\n",
    "        else:\n",
    "            obs.polarity = [True]\n",
    "            \n",
    "    # todo : find or create observed object\n",
    "    \n",
    "    return obs\n",
    "\n",
    "if \"Observation\" in GeologicalKnowledgeFramework.registered_constructors:\n",
    "    del GeologicalKnowledgeFramework.registered_constructors[\"Observation\"]\n",
    "GeologicalKnowledgeFramework.register_constructor(\"Observation\", \n",
    "                                                  constructor= constructor_observation,\n",
    "                                                  condition= conditions_for_constructor_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal consistency anomaly\n",
    "# eg. dip, dip_dir out of range\n",
    "# must have at least (dip and dip_dir) or occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretationSituation(object):\n",
    "    \"\"\"Defines a situation of interpretation.\n",
    "    \n",
    "    A situation is gathering:\n",
    "    - self.features: features to be explained\n",
    "    - self.context: an interpretation context, ie. elements to be considered during the interpretation\n",
    "    - self.process: the interpretation process in which this situation arises\"\"\"\n",
    "    \n",
    "    def __init__(self, features, process= None, context = None, knowledge_framework= None) -> None:\n",
    "        \"\"\"Initialises the situation\n",
    "        \n",
    "        Parameters:\n",
    "         - self.features: features to be explained\n",
    "         - self.context: an interpretation context, ie. elements to be considered during the interpretation\n",
    "         - self.process: the interpretation process in which this situation arises\n",
    "         \"\"\"\n",
    "        self.features = np.array(features).ravel()\n",
    "        self.context = np.array(context).ravel() if context is not None else None\n",
    "        self.candidate_explaining_object = None\n",
    "        self.process = process\n",
    "        self.knowledge_framework = knowledge_framework if knowledge_framework is not None else \\\n",
    "            (process.knowledge_framework if process is not None else None)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratigraphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditions_for_constructor_surface_part_from_interpretation(\n",
    "                                            knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            interpretation_situation:InterpretationSituation, **kargs):\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for Surface Part constructor: knowledge framework\")\n",
    "        return False\n",
    "    if (interpretation_situation is None):\n",
    "        print(\"Wrong condition for Surface Part constructor: interpretation situation\")\n",
    "        return False\n",
    "    if  (interpretation_situation.process is None) or (interpretation_situation.process.physical_space):\n",
    "        print(\"Wrong condition for Surface Part constructor: physical space\")\n",
    "        return False\n",
    "    if  (interpretation_situation.features is None) or (len(interpretation_situation.features) == 0):\n",
    "        print(\"Wrong condition for Surface Part constructor: features\")\n",
    "        return False\n",
    "    \n",
    "    onto_class = knowledge_framework().Stratigraphic_Part\n",
    "    explainable = knowledge_framework.get_objects_potentially_explained_by(onto_class)\n",
    "    if not np.any([feature in explainable for feature in interpretation_situation.features]):\n",
    "        print(\"Wrong condition for Surface Part constructor: no explainable feature in situation\")\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "def get_physical_space_in_constructor(interpretation_situation, physical_space= None, **kargs):\n",
    "    \"\"\"Gets a physical representation space from an interpreted situation of context\"\"\"\n",
    "    if physical_space is not None:\n",
    "        return physical_space\n",
    "    if interpretation_situation is not None \\\n",
    "    and interpretation_situation.process is not None \\\n",
    "    and interpretation_situation.process.physical_space is not None:\n",
    "        return interpretation_situation.process.physical_space\n",
    "    else:\n",
    "        raise MalissiaBaseError(\"Wrong conditions for physical space definition.\")\n",
    "    \n",
    "def location_constructor(features, physical_space, method= \"average\", return_as_dict= False):\n",
    "    \"\"\"Estimated a location from a given situation and or space\n",
    "    \n",
    "    Parameters:\n",
    "    - features: a list of features that can inform the location, if empty the location is set from the physical_space\n",
    "    - physical_space: the space in whichthe new location is to be defined\n",
    "    - method: determines how th e new location should be estimated (pick, random, average)\n",
    "    Returns:\n",
    "    - a dict matching each coordinate label with its coordinate value\n",
    "    \"\"\" \n",
    "    if physical_space is None: raise MalissiaBaseError(\"a physical space is required, use get_physical_space_in_constructor\")\n",
    "    if len(features) == 0:\n",
    "        # set from the physical space\n",
    "        if method == \"average\":\n",
    "            location = physical_space.get_center()\n",
    "        else:\n",
    "            location = physical_space.generate_random_location()\n",
    "    else:\n",
    "        coords = np.array([physical_space.get_object_coordinates(feature_i) for feature_i in features])\n",
    "        if method == \"average\":\n",
    "            # take the average position as new position\n",
    "            location = np.mean(coords, axis= 0)\n",
    "        elif method == \"pick\":\n",
    "            # pick a random location from the dataset\n",
    "            location = np.random.default_rng().choice(coords)\n",
    "        elif method == \"random\":\n",
    "            # generate a random location in the space covered by the data points\n",
    "            mean = np.mean(coords, axis= 0)\n",
    "            cov = np.cov(coords, rowvar=False)\n",
    "            location = np.random.default_rng().multivariate_normal(mean, cov)\n",
    "        return physical_space.coordinates_to_dict(location) if return_as_dict else location.tolist()\n",
    "        \n",
    "def translate_polarity(polarity):\n",
    "    return bool(polarity > 0)\n",
    "\n",
    "def format_attitude_qualities(dip, dip_dir, polarity= True, size= 1):\n",
    "    return {\"dip\":float(dip),\"dip_dir\":float(dip_dir), \"polarity\": translate_polarity(polarity), \"size\": float(size)}\n",
    "\n",
    "def attitude_constructor_from_space(physical_space):\n",
    "    \"\"\"Estimated attitude from a list of points\"\"\"\n",
    "    dip = physical_space.generate_random_dip()\n",
    "    dip_dir = physical_space.generate_random_dip_dir()\n",
    "    polarity = physical_space.generate_random_polarity()\n",
    "    size = physical_space.generate_random_size()\n",
    "    return format_attitude_qualities(dip= dip, dip_dir= dip_dir, polarity= polarity, size= size)\n",
    "    \n",
    "def attitude_constructor_from_attitude_features(features, physical_space, method= \"average\"):\n",
    "    \"\"\"Estimated attitude from a list of features having attitudes\"\"\"\n",
    "    if method == \"pick\":\n",
    "        picked_feature = np.random.default_rng().choice(features)\n",
    "        dip = picked_feature.dip[0]\n",
    "        dip_dir = picked_feature.dip_dir[0]\n",
    "        polarity = picked_feature.polarity[0]\n",
    "        size = picked_feature.size[0]\n",
    "    elif method == \"average\":\n",
    "        dip = [feature_i.dip[0] for feature_i in features]\n",
    "        dip_dir = [feature_i.dip_dir[0] for feature_i in features]\n",
    "        polarities = [(1 if feature_i.polarity[0] else -1) for feature_i in features]\n",
    "        vectors = [compute_normal_from_dip_dir(dip_i, dip_dir_i, polarity_i) for dip_i, dip_dir_i, polarity_i in zip(dip, dip_dir, polarities) ]\n",
    "        normal = physical_space.compute_average_vector(vectors)\n",
    "        dip, dip_dir, polarity = physical_space.compute_dip_dir_from_normal(normal)\n",
    "        size = np.mean([feature_i.size[0] for feature_i in features])\n",
    "        polarity = physical_space.generate_random_polarity()\n",
    "    else:\n",
    "        raise MalissiaBaseError(\"unimplemented method: \"+method)\n",
    "    return format_attitude_qualities(dip= dip, dip_dir= dip_dir, polarity= polarity, size= size)\n",
    "\n",
    "def attitude_constructor_from_points(points, physical_space, method= \"average\"):\n",
    "    \"\"\"Estimated attitude from a list of points\"\"\"\n",
    "    if len(points) < 2:\n",
    "        raise MalissiaBaseError(\"not enough points to estimate attitude\")\n",
    "    if len(points) == 2:\n",
    "        line = physical_space.compute_line_attitude_from_two_points(*points)\n",
    "        if method == \"average\":\n",
    "            plane = line\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"method not implemented yet: \" + method)\n",
    "    else:\n",
    "        if method == \"average\":\n",
    "            plane = physical_space.compute_attitude_from_points(points)\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"method not implemented yet: \" + method)\n",
    "        \n",
    "    polarity = physical_space.generate_random_polarity()\n",
    "    return format_attitude_qualities(dip= plane[\"dip\"], dip_dir= plane[\"dip_dir\"], size= plane[\"size\"], polarity= polarity)\n",
    "\n",
    "def attitude_constructor_from_single_feature(feature, knowledge_framework= None):\n",
    "    # if it has dip and dip_dir -> use it\n",
    "    dip = feature.dip[0] if knowledge_framework.has_quality(feature, \"dip\") else physical_space.generate_random_dip()\n",
    "    dip_dir = feature.dip_dir[0] if knowledge_framework.has_quality(feature, \"dip_dir\") else physical_space.generate_random_dip_dir()\n",
    "    return format_attitude_qualities(dip= dip, dip_dir= dip_dir)\n",
    "    \n",
    "def attitude_constructor(features, physical_space, knowledge_framework= None, method= \"average\"):\n",
    "    \"\"\"Estimated the attitude of a feature from a given situation and or space\n",
    "    \n",
    "    Parameters:\n",
    "    - features: a list of features that can inform the location\n",
    "    - physical_space: the space in whichthe new location is to be defined\n",
    "    - method: determines how the new attitude should be estimated (pick, random, average)\"\"\" \n",
    "    if physical_space is None: raise MalissiaBaseError(\"a physical space is required, use get_physical_space_in_constructor\")\n",
    "    knowledge_framework = GeologicalKnowledgeManager().get_knowledge_framework() if knowledge_framework is None else knowledge_framework\n",
    "    \n",
    "    if len(features) == 0:\n",
    "        return attitude_constructor_from_space(physical_space)\n",
    "    elif len(features) == 1:\n",
    "        return attitude_constructor_from_single_feature(features[0], knowledge_framework= knowledge_framework)\n",
    "    else:\n",
    "        # filtering point features\n",
    "        features = np.array(features)\n",
    "        is_instance_of_point = [knowledge_framework.has_point_representation(feature_i) for feature_i in features]\n",
    "        point_features = features[is_instance_of_point]\n",
    "        points = [physical_space.get_object_coordinates(point_feature_i) for point_feature_i in point_features]\n",
    "        \n",
    "        is_attitude_instance = [knowledge_framework.has_qualities(feature_i,[\"dip\",\"dip_dir\"]) for feature_i in features]\n",
    "        attitude_features = features[is_attitude_instance]\n",
    "        \n",
    "        # if only points then create from them\n",
    "        if np.all(is_attitude_instance):\n",
    "            return attitude_constructor_from_attitude_features(attitude_features, physical_space= physical_space, method= method)\n",
    "        elif np.all(is_instance_of_point):\n",
    "            return attitude_constructor_from_points(points, physical_space= physical_space, method= method)\n",
    "        else:\n",
    "            print(\"Warning: surfaces are ignored while constructing from both points and surfaces. To be implemented.\")\n",
    "            return attitude_constructor_from_points(points, physical_space= physical_space, method= method)\n",
    "    \n",
    "def constructor_surface_part_from_interpretation(knowledge_framework, interpretation_situation,\n",
    "                                                 physical_space= None, name= None, **kargs):\n",
    "    \"\"\"Constructor of observations\n",
    "    \n",
    "    Parameters:\n",
    "    - knowledge_framework: the knowledge framework in which this object must be created\n",
    "    - interpretation_situation: the interpretation situation selected in the given state of interpretation process\n",
    "    - physical_space: the physical representation space where the surface is to be defined, if None the one from the interpretation_situation.process is used\n",
    "        \n",
    "    Return:\n",
    "    - the created Surface Part\n",
    "    \"\"\"\n",
    "    \n",
    "    physical_space = get_physical_space_in_constructor(interpretation_situation, physical_space, **kargs)\n",
    "    \n",
    "    constructed_class = knowledge_framework().Stratigraphic_Part\n",
    "    explainable_features = knowledge_framework.filter_explainable_features(constructed_class, interpretation_situation.features)\n",
    "    if len(explainable_features) == 0:\n",
    "        print(\"Warning: No explainable feature in the situation with this interpretation. To be implemented.\")\n",
    "        return None\n",
    "    \n",
    "    #estimate location\n",
    "    location_qualities = location_constructor(features= explainable_features, physical_space= physical_space, method= \"average\")\n",
    "    \n",
    "    # estimate attitude\n",
    "    attitude_qualities = attitude_constructor(features= explainable_features, physical_space= physical_space,\n",
    "                                              knowledge_framework= knowledge_framework, method= \"average\")\n",
    "    \n",
    "    # estimate size\n",
    "    size = attitude_qualities[\"size\"]\n",
    "    \n",
    "    # Instanciate a PlanarSurface\n",
    "    name_rep = None if name is None else name+\"_rep\"\n",
    "    surface_representation = constructor_planar_surface_from_center_attitude(\n",
    "                                    knowledge_framework= knowledge_framework,\n",
    "                                    coord_labels= physical_space.coordinate_labels,\n",
    "                                    center= location_qualities,\n",
    "                                    dip= attitude_qualities[\"dip\"],\n",
    "                                    dip_dir= attitude_qualities[\"dip_dir\"],\n",
    "                                    polarity= attitude_qualities[\"polarity\"],\n",
    "                                    size= size, \n",
    "                                    name= name_rep\n",
    "                                    )\n",
    "\n",
    "    # Instanciate a Stratigraphic_Part pointing to the planarsurface representation\n",
    "    class_type = knowledge_framework().Stratigraphic_Part\n",
    "    surface_part = knowledge_framework.create_instance(class_type, name, has_Representation= surface_representation, **kargs)\n",
    "    \n",
    "    # explanation relationship\n",
    "    surface_part.explain = explainable_features\n",
    "     \n",
    "    return surface_part\n",
    "\n",
    "if \"SurfacePart\" in GeologicalKnowledgeFramework.registered_constructors:\n",
    "    del GeologicalKnowledgeFramework.registered_constructors[\"SurfacePart\"]\n",
    "GeologicalKnowledgeFramework.register_constructor(\"SurfacePart\", \n",
    "                                                  constructor= constructor_surface_part_from_interpretation,\n",
    "                                                  condition= conditions_for_constructor_surface_part_from_interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stratigraphic part anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal consistency anomaly\n",
    "\n",
    "## DippingStratigraphyAnomaly\n",
    "## anomaly for a StratigraphicPart with a planar geometry having a dip > threshold \n",
    "\n",
    "## DipVariationAnomaly\n",
    "## anomaly for a StratigraphicParts with a ComplexSurface representation\n",
    "## when two successive PlanarSurfaces have a variation of orientation\n",
    "\n",
    "## DiscontinuousStratigaphyAnomaly\n",
    "## anomaly for a StratigraphicParts with a ComplexSurface representation\n",
    "## when two successive PlanarSurfaces do not intersect properly\n",
    "\n",
    "## StratigraphyAnomaly\n",
    "## Anomaly for a stratigraphic surface when its geometry\n",
    "## doesn't reach the border of the domain\n",
    "\n",
    "# ExplanationAnomaly\n",
    "## an explained object geometry doesn't match the explaining object geometry\n",
    "## distance between surface too big, orientation too different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anomaly:\n",
    "    \"\"\"A class for constructing anomalies\"\"\"\n",
    "    def __init__(self, knowledge_framework = None):\n",
    "        \"\"\"Initialise base class attributes\"\"\"\n",
    "        self.knowledge_framework = knowledge_framework if knowledge_framework is not None else GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "        pass\n",
    "\n",
    "\n",
    "def dippingStratigraphyAnomaly_constructor(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            related_object = []):\n",
    "    \"\"\" constructor method for creating an instance of DippingStratigraphyAnomaly\"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for DippingStratigraphyAnomaly_constructor : knowledge framework\")\n",
    "        return False\n",
    "    anomaly = knowledge_framework().DippingStratigraphyAnomaly( is_Related_To = related_object)\n",
    "    return anomaly\n",
    "\n",
    "def dipVariationAnomaly_constructor(knowledge_framework:GeologicalKnowledgeFramework, \n",
    "                                    related_objects = []):\n",
    "    \"\"\" constructor method for creating an instance of DipVariationAnomaly\"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for DipVariationAnomaly_constructor : knowledge framework\")\n",
    "        return False\n",
    "    anomaly = knowledge_framework().DipVariationAnomaly(is_Related_To = related_objects)\n",
    "    return anomaly\n",
    "\n",
    "def discontinuousStratigaphyAnomaly_constructor(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                                 discontinuous_objects = [], \n",
    "                                                 discontinuity_space = []):\n",
    "    \"\"\" constructor method for creating an instance of DiscontinuousStratigaphyAnomaly\"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for DiscontinuousStratigaphyAnomaly_constructor : knowledge framework\")\n",
    "        return False\n",
    "    anomaly = knowledge_framework().DiscontinuousStratigaphyAnomaly(is_Related_To = discontinuous_objects,\n",
    "                                                                 has_Occurrence_Space =  discontinuity_space )\n",
    "    return anomaly\n",
    "\n",
    "def stratigraphyAnomaly_constructor(knowledge_framework:GeologicalKnowledgeFramework, \n",
    "                                    stratigraphic_surface = [], occurence_spaces = []):\n",
    "    \"\"\" constructor method for creating an instance of StratigraphyAnomaly\"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for StratigraphyAnomaly_constructor : knowledge framework\")\n",
    "        return False\n",
    "    anomaly = knowledge_framework().StratigraphyAnomaly(is_Related_To = stratigraphic_surface, \n",
    "                                                      has_Occurrence_Space = occurence_spaces)\n",
    "    return anomaly\n",
    "\n",
    "\n",
    "def explanationAnomaly_constructor(knowledge_framework:GeologicalKnowledgeFramework, \n",
    "                                   Explained_Object = [],\n",
    "                                   Explaining_Object= []):\n",
    "    \"\"\" constructor method for creating an instance of ExplanationAnomaly\"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for ExplanationAnomaly_constructor : knowledge framework\")\n",
    "        return False\n",
    "    anomaly = knowledge_framework().ExplanationAnomaly( is_Related_To_Explained_Object = Explained_Object,\n",
    "                                                     is_Related_To_Explaining_Object = Explaining_Object )\n",
    "    return anomaly\n",
    "\n",
    "\n",
    "def limbsPolarityAnomaly_constructor(knowledge_framework:GeologicalKnowledgeFramework,\n",
    "                                            related_limbs = []):\n",
    "    \"\"\" constructor method for creating an instance of limbsPolarityAnomaly\"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for DippingStratigraphyAnomaly_constructor : knowledge framework\")\n",
    "        return False\n",
    "    anomaly = knowledge_framework().DippingStratigraphyAnomaly( is_Related_To = related_limbs)\n",
    "    return anomaly\n",
    "\n",
    "\n",
    "### to do define how to create the occuence space (through representational voxel ! )\n",
    "## how to related by its physical qualities noodes coord\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratigraphic_part_to_limb_constructor(knowledge_framework:GeologicalKnowledgeFramework, stratigraphic_part):\n",
    "    \"\"\" constructor method for creating an instance of a limb based on an existing stratigraphic part\"\"\"\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        print(\"Wrong condition for DippingStratigraphyAnomaly_constructor : knowledge framework\")\n",
    "        return False\n",
    "    return stratigraphic_part.is_a.append(knowledge_framework.Fold_Limb)   \n",
    "\n",
    "\n",
    "\n",
    "def fold_instance_constructor(knowledge_framework:GeologicalKnowledgeFramework):\n",
    "    \"\"\" constructor method for creating an instance of a Chevron_Fold\"\"\"\n",
    "    return knowledge_framework.Chevron_Fold()\n",
    "\n",
    "\n",
    "def fold_constructor(knowledge_framework:GeologicalKnowledgeFramework, \n",
    "                     Stratigraphic_Part1, Stratigraphic_Part2):\n",
    "    if (knowledge_framework is None) or (knowledge_framework.name != \"mogi\"):\n",
    "        raise ValueError ('Wrong condition for DippingStratigraphyAnomaly_constructor : knowledge framework')\n",
    "    # creating an instance of a Chevron_Fold\n",
    "    fold = knowledge_framework.Chevron_Fold()\n",
    "    # creating an instance of limbs\n",
    "    limb1 = stratigraphic_part_to_limb_constructor(knowledge_framework, Stratigraphic_Part1)\n",
    "    limb2 = stratigraphic_part_to_limb_constructor(knowledge_framework, Stratigraphic_Part2)\n",
    "\n",
    "    fold.has_Limb1 = limb1\n",
    "    fold.has_Limb2 = limb2\n",
    "    return fold    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from scipy.linalg import null_space\n",
    "from sympy import Point, Point3D, Line3D, Plane, Polygon, Line, Float\n",
    "import sympy as sp \n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "\n",
    "def compute_fold_axis_non_oriented(limb1_normal, limb2_normal):\n",
    "    limb1_normal = np.array(limb1_normal).astype(float)\n",
    "    limb2_normal = np.array(limb2_normal).astype(float)\n",
    "    fold_axis = np.cross(limb1_normal, limb2_normal)\n",
    "    fold_axis_norm = np.linalg.norm(fold_axis)\n",
    "    if fold_axis_norm == 0:\n",
    "        raise MalissiaBaseError(\"Limbs are parallel.\")\n",
    "    return fold_axis / fold_axis_norm\n",
    "\n",
    "def compute_axial_surface_normal(limb1_normal, limb2_normal, fold_axis):\n",
    "    limb1_normal = np.array(limb1_normal).astype(float)\n",
    "    limb2_normal = np.array(limb2_normal).astype(float)\n",
    "    axial_surface_normal = np.cross(fold_axis, limb1_normal+limb2_normal)\n",
    "    return axial_surface_normal / np.linalg.norm(axial_surface_normal)\n",
    "\n",
    "def compute_direction_toward_hinge_along_limb_1(limb_normal, fold_axis):\n",
    "    along_limb_vec = list(map(float,-1 * np.cross(limb_normal,fold_axis)))\n",
    "    return along_limb_vec / np.linalg.norm(along_limb_vec)\n",
    "\n",
    "def compute_direction_toward_hinge_along_limb_2(limb_normal, fold_axis):\n",
    "    along_limb_vec = list(map(float, np.cross(limb_normal,fold_axis)))\n",
    "\n",
    "    return along_limb_vec / np.linalg.norm(along_limb_vec)\n",
    "\n",
    "def calculate_square_normal(vertex1, vertex2, vertex3, vertex4):\n",
    "    \"\"\"\n",
    "    Calculate the normal vector of a square surface defined by its four vertices.\n",
    "    \"\"\"\n",
    "    # Choose three non-collinear vertices to define a plane (e.g., vertices 1, 2, and 3)\n",
    "    point1 = np.array(vertex1)\n",
    "    point2 = np.array(vertex2)\n",
    "    point3 = np.array(vertex3)\n",
    "\n",
    "    # Calculate two vectors in the plane\n",
    "    vector1 = point2 - point1\n",
    "    vector2 = point3 - point1\n",
    "\n",
    "    # Calculate the normal vector by taking the cross product of vector1 and vector2\n",
    "    normal_vector = np.cross(vector1, vector2)\n",
    "    \n",
    "    # Optionally, normalize the normal vector to have a unit length\n",
    "    magnitude = np.linalg.norm(normal_vector)\n",
    "    normalized_vector = normal_vector / magnitude\n",
    "\n",
    "    return normalized_vector\n",
    "\n",
    "def compute_square_center(vertex1, vertex2, vertex3, vertex4):\n",
    "    return np.array((vertex1 + vertex2 + vertex3 + vertex4) / 4)    \n",
    "\n",
    "def compute_intersection_point_beta(limb_1_nodes , limb_2_nodes):\n",
    "\n",
    "    # calculate n1, n2, c1, c2, v1, f_axe\n",
    "    n1 = sp.Point(calculate_square_normal(*limb_1_nodes))\n",
    "    n2 = sp.Point(calculate_square_normal(*limb_2_nodes))\n",
    "    c1 = sp.Point(compute_square_center(*limb_1_nodes))\n",
    "    c2 = sp.Point(compute_square_center(*limb_2_nodes))\n",
    "    f_axe = sp.Point(compute_fold_axis(n1, c1, n2, c2))\n",
    "    v1_to_hinge = sp.Point(compute_direction_toward_hinge_along_limb_1(n1 , f_axe))\n",
    "    # Define symbolic variables\n",
    "    k1 = sp.symbols('k1')\n",
    "    C1 = sp.Matrix(c1)  # Numeric values for C1\n",
    "    C2 = sp.Matrix(c2)  # Numeric values for C2\n",
    "    V1 = sp.Matrix(v1_to_hinge)  # Numeric values for V1\n",
    "    N2 = sp.Matrix(n2)  # Numeric values for n2\n",
    "    # Define the equations\n",
    "    equation1 = C1 + k1 * V1\n",
    "    equation2 = (equation1 - C2).dot(N2)\n",
    "    equation3 = (C1 + k1 * V1 - C2).dot(N2)\n",
    "    # Solve for k1\n",
    "    solution = sp.solve(equation3, k1)[0]\n",
    "\n",
    "    return np.array(list(map(float,(np.array(Point(equation1.subs(k1, solution)))))))\n",
    "    \n",
    "  \n",
    "def compute_intersection_point(plane1_center, plane1_normal, plane2_center, plane2_normal):\n",
    "    # Convert input to NumPy arrays for easier vector operations\n",
    "    plane1_center = np.array(plane1_center)\n",
    "    plane1_normal = np.array(plane1_normal)\n",
    "    plane2_center = np.array(plane2_center)\n",
    "    plane2_normal = np.array(plane2_normal)\n",
    "\n",
    "    # Find the direction vector of the line of intersection\n",
    "    fold_axis = compute_fold_axis(plane1_normal, plane1_center, plane2_normal, plane2_center)\n",
    "    v2_to_hinge = compute_direction_toward_hinge_along_limb_2(plane2_normal, fold_axis)\n",
    "    c1_c2 = plane2_center - plane1_center\n",
    "    k = -np.dot(c1_c2, plane1_normal)/np.dot(plane1_normal, v2_to_hinge)\n",
    "    intersection_point = plane2_center  + k * np.array(v2_to_hinge)\n",
    "    return intersection_point\n",
    "\n",
    "def compute_square_side(square_nodes):\n",
    "    return  np.linalg.norm(square_nodes[0] - square_nodes[1])\n",
    "\n",
    "def compute_endpoint(start_point, direction_vector, distance):\n",
    "    direction_vector = np.array(direction_vector)\n",
    "    direction_vector = direction_vector/np.linalg.norm(direction_vector)\n",
    "    endpoint = [float(start_point[i] + distance * direction_vector[i]) for i in range(len(start_point))]\n",
    "    return endpoint\n",
    "\n",
    "def create_finite_axial_surface(intersection_point,  axial_surf_normal, fold_axis, \n",
    "                                half_diag_size = None, \n",
    "                                 limb_1 = None, limb_2 = None):\n",
    "    \"\"\"method to create a finite axial surface based on a plane normal, fold axis, and if possible\n",
    "      nodes of limbs for giving it a proportional size, or it will generate a random value between 10 and 100. \n",
    "      This method could be used to create any square surface based on:\n",
    "        a plane normal and a direction vector of one of the four sides instead of the fold axis, and by a giving a specific half_diag_size \"\"\"\n",
    "    \n",
    "    if half_diag_size != None :\n",
    "        half_diag_size = half_diag_size\n",
    "    else:    \n",
    "        if (limb_1, limb_2) == (None, None):\n",
    "            half_diag_size = np.random.uniform(10, 100)\n",
    "        elif limb_1 != None and limb_2 != None:\n",
    "            half_diag_size = 5* max(compute_square_side(limb_1), compute_square_side(limb_2))\n",
    "        else:\n",
    "            half_diag_size = 5*(compute_square_side(limb_1) if \n",
    "                        limb_1 is not None else compute_square_side(limb_2))    \n",
    "\n",
    "    # Calculate the direction vectors for the sides of the square\n",
    "    side1 = np.cross(axial_surf_normal, fold_axis)\n",
    "    side2 = fold_axis\n",
    "\n",
    "    # Calculate the coordinates of the four corners\n",
    "    corner1 = intersection_point - half_diag_size * side1\n",
    "    corner3 = intersection_point + half_diag_size * side1\n",
    "    corner2 = intersection_point - half_diag_size * side2\n",
    "    corner4 = intersection_point + half_diag_size * side2\n",
    "    vertices = np.array([corner1, corner2 , corner3, corner4])\n",
    "    translated_vertices = vertices - intersection_point\n",
    "    rotation = R.from_rotvec(np.pi / 4 * axial_surf_normal)\n",
    "    #Rotate the square's vertices while keeping its center fixed\n",
    "     \n",
    "    rotated_vertices = rotation.apply(translated_vertices)\n",
    "    vertices = rotated_vertices + intersection_point\n",
    "    return  vertices\n",
    "\n",
    "def compute_s_vector(n1, c1 , n2 , c2):\n",
    "\n",
    "    pp = np.cross(n1, n2)\n",
    "    pp_norm = np.linalg.norm(pp)\n",
    "    if pp_norm == 0:\n",
    "        raise MalissiaBaseError(\"Limbs are parallel.\")\n",
    "    pp = pp/pp_norm\n",
    "    t = n1 + n2\n",
    "    t /= np.linalg.norm(t)\n",
    "    c = c2 - c1\n",
    "    s = c - np.dot(c,t)*t - np.dot(c,pp)*pp\n",
    "    s /= np.linalg.norm(s)\n",
    "    return s\n",
    "\n",
    "def compute_fold_axis(n1, c1, n2,  c2):\n",
    "    s_vector = compute_s_vector(n1, c1, n2,  c2)\n",
    "    t = n1+n2\n",
    "    t /= np.linalg.norm(t)\n",
    "    fold_axis = np.cross(t, s_vector)\n",
    "    fold_axis /= np.linalg.norm(fold_axis)\n",
    "    return fold_axis\n",
    "\n",
    "def compute_direction_toward_fold_axis(fold_axis, limb_normal):\n",
    "    # Calculate the vector connecting the point to its projection\n",
    "    direction_toward_fold_axis = np.cross(  fold_axis,  limb_normal)\n",
    "    direction_toward_fold_axis /= np.linalg.norm(direction_toward_fold_axis)\n",
    "    return direction_toward_fold_axis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cross([1,0,0], [-1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def rotate_vector_around_axis(n1, p, theta_rad ):\n",
    "    n1 = n1/ np.linalg.norm(n1)\n",
    "    # Calculate the rotation matrix\n",
    "    # Normalize the direction vector p over which the rotation will be performed\n",
    "    p = p / np.linalg.norm(p)\n",
    "    \n",
    "    # Calculate the cross product matrix of p\n",
    "    P = np.array([\n",
    "        [0, -p[2], p[1]],\n",
    "        [p[2], 0, -p[0]],\n",
    "        [-p[1], p[0], 0]\n",
    "    ])\n",
    "    \n",
    "    # Calculate the rotation matrix using the Rodriguez formula\n",
    "    R = np.eye(3) + np.sin(theta_rad) * P + (1 - np.cos(theta_rad)) * np.dot(P, P)\n",
    "\n",
    "    # Step 2: Apply the rotation to n1\n",
    "    rotated_n1 = np.round(np.array(np.dot(R, n1)),8) \n",
    "\n",
    "    return rotated_n1 / np.linalg.norm(np.dot(R, n1))\n",
    "\n",
    "def choose_square_side(limb_nodes):\n",
    "\n",
    "    z = limb_nodes[:,2]\n",
    "    dz1 = np.abs(z[1] - z[0])\n",
    "    dz2 = np.abs(z[2] - z[1])\n",
    "\n",
    "    if dz1 < dz2:\n",
    "        selected_indices = [0,2]\n",
    "    elif dz1 > dz2:\n",
    "        selected_indices = [1,3]\n",
    "    else:\n",
    "        selected_indices = np.arange(4)\n",
    "\n",
    "    return random.choice(selected_indices)\n",
    "\n",
    "def get_square_side(limb_nodes, i):\n",
    "    edges = np.array([[0,1],[1,2],[2,3],[3,0]])\n",
    "    nodes = limb_nodes[edges[i]]\n",
    "    vec = nodes[1] - nodes[0]\n",
    "    return [nodes[0], nodes[1], vec]\n",
    "\n",
    "def compute_shape_limb1(start_point, fold_axis, side_size,  limb_normal):\n",
    "\n",
    "    vertex1 = start_point\n",
    "    vertex2 = compute_endpoint(vertex1, fold_axis, side_size)\n",
    "    vertex3 = compute_endpoint(vertex2, \n",
    "                               (-compute_direction_toward_hinge_along_limb_1(limb_normal,\n",
    "                                fold_axis)), side_size)\n",
    "    vertex4 = compute_endpoint(vertex3, -fold_axis, side_size)\n",
    "    return [vertex1, vertex2, vertex3, vertex4]   \n",
    "\n",
    "def compute_shape_limb2( vertex1, vertex2 , fold_axis, side_size,  limb_normal):\n",
    "\n",
    "    vertex1 = vertex1\n",
    "    vertex2 = vertex2\n",
    "    vertex3 = compute_endpoint(vertex2, \n",
    "                               (-compute_direction_toward_hinge_along_limb_2(limb_normal,\n",
    "                                fold_axis)), side_size)\n",
    "    vertex4 = compute_endpoint(vertex3,fold_axis, side_size)\n",
    "    return [vertex1, vertex2, vertex3, vertex4]   \n",
    "\n",
    "def compute_distance_to_vector(point, vector_origin, vector_orientation):\n",
    "    # Convert input to NumPy arrays for easier vector operations\n",
    "    point = np.array(point)\n",
    "    vector_origin = np.array(vector_origin)\n",
    "    vector_orientation = np.array(vector_orientation)\n",
    "\n",
    "    # Calculate the vector from the center to the point\n",
    "    vector_to_point = point - vector_origin\n",
    "\n",
    "    # Calculate the projection of vector_to_point onto vector_orientation\n",
    "    projection = np.dot(vector_to_point, vector_orientation) / np.dot(vector_orientation, vector_orientation) * vector_orientation\n",
    "\n",
    "    # Calculate the distance between the point and the projection\n",
    "    distance = np.linalg.norm(vector_to_point - projection)\n",
    "\n",
    "    return distance\n",
    "\n",
    "def compute_point_projection_onto_line(point_to_project, line_origin, line_vector):\n",
    "    # Normalize the line vector\n",
    "    line_normalized = np.array(line_vector / np.linalg.norm(line_vector))\n",
    "    # Calculate the vector from the origin to the point\n",
    "    w = np.array(np.array(point_to_project) - line_origin)\n",
    "    # Project w onto the line\n",
    "    projection_length = np.dot(w, line_normalized)\n",
    "    # Find the projected point on the line\n",
    "    projected_point = line_origin + projection_length * line_normalized\n",
    "    return np.array(projected_point)\n",
    "\n",
    "def compute_projection_info (projected_points,  fold_axis):\n",
    "\n",
    "    # Initialize variables to store extreme points and distances\n",
    "    max_distance = -1\n",
    "    extreme_points_max = None\n",
    "    # Calculate pairwise distances and the extreme points\n",
    "    for pair in itertools.combinations(projected_points, 2):\n",
    "        point1, point2 = pair\n",
    "        distance = np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "        if distance > max_distance:\n",
    "            max_distance = distance\n",
    "            extreme_points_max = [point1, point2]\n",
    "    point0, point1 = np.array(extreme_points_max[0]), np.array(extreme_points_max[1])\n",
    "    if all(point0 == point1):\n",
    "        raise MalissiaBaseError ('the projected points coincide')\n",
    "    side_size = max_distance\n",
    "    vec = np.array([point1[0] - point0[0], \n",
    "           point1[1] - point0[1],\n",
    "             point1[2] - point0[2]])\n",
    "    parallelism = np.rad2deg(np.arccos(np.dot(vec, fold_axis) / (np.linalg.norm(vec) \n",
    "                                                        * np.linalg.norm(fold_axis))))\n",
    "\n",
    "    if  parallelism != 0 and parallelism != 180:\n",
    "        raise MalissiaBaseError('projected points on the fold axis are not collinear')\n",
    "    if np.dot(vec, fold_axis) > 0:\n",
    "        start_point = point0\n",
    "        end_point = point1\n",
    "    else: \n",
    "        start_point = point1\n",
    "        end_point = point0\n",
    "\n",
    "    return {'start_point': start_point, 'end_point': end_point,\n",
    "             'side_size': side_size}\n",
    "\n",
    "\n",
    "\n",
    "def compute_fold_geometry_from_two_limbs(limb1, limb2):\n",
    "    n1 = calculate_square_normal(*limb1)\n",
    "    n2 = calculate_square_normal(*limb2)\n",
    "    c1 = compute_square_center(*limb1)\n",
    "    c2 = compute_square_center(*limb2)\n",
    "    fold_axis = compute_fold_axis(n1, c1, n2, c2)\n",
    "    intersection_point = compute_intersection_point(c1, n1,c2, n2)\n",
    "    all_surfaces_nodes = [v for v in limb1]\n",
    "    for v in limb2:\n",
    "        all_surfaces_nodes.append(v) \n",
    "    projected_nodes = []\n",
    "    for node_i in all_surfaces_nodes:\n",
    "        projected_nodes.append(compute_point_projection_onto_line(node_i, \n",
    "                intersection_point, fold_axis))\n",
    "    information = compute_projection_info (projected_nodes,  fold_axis)\n",
    "    all_distances = [compute_distance_to_vector(n_i, intersection_point, fold_axis) \n",
    "                    for n_i in all_surfaces_nodes]\n",
    "    max_distance = np.max(all_distances)\n",
    "    side_size =  information['side_size'] if information['side_size']>= max_distance else  max_distance\n",
    "    \n",
    "    new_limb1 = compute_shape_limb1(information['start_point'],\n",
    "                                    fold_axis, \n",
    "                                    side_size,\n",
    "                                    n1)\n",
    "    vertex1_for_2limb, vertex2_for_2limb = new_limb1[1],new_limb1[0]\n",
    "    new_limb2 = compute_shape_limb2(vertex1= vertex1_for_2limb,\n",
    "                                    vertex2= vertex2_for_2limb,\n",
    "                                    fold_axis= fold_axis, \n",
    "                                    side_size = side_size,\n",
    "                                    limb_normal= n2)    \n",
    "    axial_surf_normal = compute_axial_surface_normal(n1 ,n2, fold_axis)\n",
    "    midpoint = np.array([(i1 + i2) / 2 for i1, i2 in zip(new_limb1[1],new_limb1[0])]) \n",
    "    finite_axial_surf = create_finite_axial_surface(midpoint,  axial_surf_normal, \n",
    "                                                    fold_axis, \n",
    "                                                    half_diag_size = information['side_size']*5)\n",
    "    intersection_point = midpoint\n",
    "    return {'fold_axis': fold_axis, \n",
    "            'axial_surf_normal': axial_surf_normal, \n",
    "            'finite_axial_surf': finite_axial_surf,\n",
    "            'limb1': new_limb1,\n",
    "            'limb2': new_limb2, \n",
    "            'intersection_point': intersection_point}\n",
    "\n",
    "\n",
    "\n",
    "def compute_complement_angle(angle_radians):\n",
    "\n",
    "    # Calculate the complement in radians\n",
    "    complement_radians = math.pi - angle_radians\n",
    "    \n",
    "    return  complement_radians\n",
    "\n",
    "\n",
    "def compute_fold_geometry_from_one_limb(candidate_limb_nodes,  \n",
    "                                                 force_limb_assignment = None , \n",
    "                                                 force_fold_type = None , \n",
    "                                                 chosen_side_index = None,\n",
    "                                                 fold_opening_angle_degree  = 90. ):\n",
    "    \n",
    "    \"\"\"method for building a fold based on a candidatefold limb. \n",
    "    for all arguments  if none is chosen, the algorithm will make a random generation of the paramters: \n",
    "    - force_limb_assignment must be int 1 or 2  \n",
    "    - fold type must be string anticline or aynscline \n",
    "    - chosen_side_index must be an int from 0 to 3 \n",
    "    - fold_opening_angle_degree must be flaot from 0.0001 to 179.9999\n",
    "    \"\"\"\n",
    "    \n",
    "    candidate_limb_nodes = np.array(candidate_limb_nodes)\n",
    "    initial_limb = force_limb_assignment if force_limb_assignment is not None else random.choice([1,2])\n",
    "    if initial_limb not in (1,2): \n",
    "        raise MalissiaBaseError('The choice of which limb could not be be made, try giving force_limb_assignment a value of 1 or 2')\n",
    "\n",
    "    fold_type = force_fold_type if force_fold_type is not None else random.choice(['anticline','syncline'])\n",
    "    if fold_type not in ['anticline','syncline']: \n",
    "        raise MalissiaBaseError('The choice of fold type could not be made, try giving force_limb_assignment a value of anticline syncline')\n",
    "\n",
    "\n",
    "    chosen_side_index = chosen_side_index if chosen_side_index is not None else choose_square_side(candidate_limb_nodes)\n",
    "    chosen_side = get_square_side(candidate_limb_nodes, chosen_side_index)\n",
    "    if len (chosen_side) == 0 :\n",
    "        raise MalissiaBaseError('the algorithm could not chose a square side')\n",
    "    \n",
    "    midpoint_side = np.mean((chosen_side[0], chosen_side[1]), axis=0)\n",
    "    vector_to_axis = midpoint_side - np.array(compute_square_center(*candidate_limb_nodes))\n",
    "\n",
    "\n",
    "    initial_normal = calculate_square_normal(*candidate_limb_nodes)\n",
    "    \n",
    "    proposed_fold_axis = np.cross(initial_normal, vector_to_axis) / np.linalg.norm(np.cross(initial_normal, vector_to_axis))\n",
    "\n",
    "\n",
    "\n",
    "    if initial_limb == 1:\n",
    "        fold_opening_angle_degree = fold_opening_angle_degree if fold_type == 'anticline' else -fold_opening_angle_degree\n",
    "\n",
    "    if initial_limb == 2:\n",
    "        proposed_fold_axis *= -1\n",
    "        fold_opening_angle_degree = fold_opening_angle_degree if fold_type == 'syncline' else -fold_opening_angle_degree\n",
    "\n",
    "    #fold_opening_angle_degree = 2*dip_compliment_from_self.object if fold_opening_angle_degree is None else fold_opening_angle_degree\n",
    "    angle_of_normal_rotation_rad =  compute_complement_angle(np.deg2rad(fold_opening_angle_degree))\n",
    "    \n",
    "\n",
    "    computed_rotated_normal = rotate_vector_around_axis(initial_normal, \n",
    "                                                        proposed_fold_axis, \n",
    "                                                        theta_rad= angle_of_normal_rotation_rad)\n",
    "    side_size  = np.linalg.norm(chosen_side[1] - chosen_side[0])\n",
    "\n",
    "    if initial_limb == 1:\n",
    "        limb1_nodes = candidate_limb_nodes\n",
    "        limb2_v1 = chosen_side[1]\n",
    "        limb2_v2 = chosen_side[0]\n",
    "        direction_toward_hinge_2 = compute_direction_toward_hinge_along_limb_2(computed_rotated_normal, proposed_fold_axis)\n",
    "        limb2_v3 = compute_endpoint(limb2_v2, -direction_toward_hinge_2, side_size)\n",
    "        limb2_v4 = compute_endpoint(limb2_v3, proposed_fold_axis, side_size)\n",
    "        limb2_nodes = np.array([limb2_v1, limb2_v2, limb2_v3, limb2_v4])\n",
    "\n",
    "    if initial_limb == 2:\n",
    "        limb2_nodes = candidate_limb_nodes\n",
    "        limb1_v1 = chosen_side[1]\n",
    "        limb1_v2 =chosen_side[0]\n",
    "        direction_toward_hinge_1 = compute_direction_toward_hinge_along_limb_1(computed_rotated_normal, proposed_fold_axis)\n",
    "        limb1_v3 = compute_endpoint(limb1_v2, -direction_toward_hinge_1, side_size)\n",
    "        limb1_v4 = compute_endpoint(limb1_v3, -proposed_fold_axis, side_size)\n",
    "        limb1_nodes = np.array([limb1_v1, limb1_v2, limb1_v3, limb1_v4]) \n",
    "    # now generate fold paramters    \n",
    "    n1 = calculate_square_normal(*limb1_nodes)\n",
    "    n2 = calculate_square_normal(*limb2_nodes)\n",
    "    c1 = compute_square_center(*limb1_nodes)\n",
    "    c2 = compute_square_center(*limb2_nodes)\n",
    "    intersection_point = compute_intersection_point(c1, n1,c2, n2)  \n",
    "    axial_surf_normal = compute_axial_surface_normal(n1 ,n2, proposed_fold_axis)\n",
    "    finite_axial_surf = create_finite_axial_surface(intersection_point,  axial_surf_normal, \n",
    "                                                    proposed_fold_axis, \n",
    "                                                    half_diag_size = side_size*1.5)\n",
    "    return {'fold_axis': proposed_fold_axis, \n",
    "            'axial_surf_normal': axial_surf_normal, \n",
    "            'finite_axial_surf': finite_axial_surf,\n",
    "            'limb1': limb1_nodes,\n",
    "            'limb2': limb2_nodes, \n",
    "            'intersection_point': intersection_point}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_1= [\n",
    "    np.array([-6, 0, 0]),\n",
    "    np.array([-10, 0, 4]),\n",
    "    np.array([-10.0, 6.0, 4.0]),\n",
    "    np.array([-6.0, 6.0, 0.0])\n",
    "]\n",
    "vertices_2 = [\n",
    "    np.array([10, -3.0, -4]),\n",
    "    np.array([10, 0, -4]),\n",
    "    np.array([15.0, 0.0, 0.0]),\n",
    "    np.array([15, -3.0, 0.0])\n",
    "]\n",
    "\n",
    "\n",
    "x = compute_fold_geometry_from_two_limbs(vertices_1, vertices_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_1= [\n",
    "    np.array([0., 0, 0]),\n",
    "    np.array([0., -5, 0]),\n",
    "    np.array([4., -5, 4]),\n",
    "    np.array([4., 0, 4])\n",
    "]\n",
    "theta = 5\n",
    "\n",
    "x = compute_fold_geometry_from_one_limb(candidate_limb_nodes = vertices_1, \n",
    "                                        force_limb_assignment = 1, \n",
    "                                        force_fold_type= 'syncline',\n",
    "                                        fold_opening_angle_degree= theta,\n",
    "                                          chosen_side_index= 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyvista  as pv\n",
    "\n",
    "\n",
    "#plane = pv.Plane()\n",
    "#line = pv.Line(np.array([1,5,1]), np.array([0,1,0]))\n",
    "#line1 = pv.Line(np.array([0,0,0]), np.array([0,0,5]))\n",
    "#poly = pv.Polygon()\n",
    "#points = np.array([np.array([1,1,1]), np.array([5,1,1])])\n",
    "#p.add_points( points, render_points_as_spheres=True, point_size=5.0 )\n",
    "\n",
    "\n",
    "vertices_x = np.array(x['limb1'])\n",
    "arrow1 = pv.Arrow(compute_square_center(*x['limb1']), calculate_square_normal(*x['limb1']))\n",
    "faces1 = np.hstack([ [ 4, 0, 1, 2, 3]])\n",
    "surf1 = pv.PolyData(vertices_x, faces1)\n",
    "\n",
    "\n",
    "vertices_y = np.array(x['limb2'])\n",
    "arrow2 = pv.Arrow(compute_square_center(*x['limb2']), calculate_square_normal(*x['limb2']))\n",
    "faces2 = np.hstack([[ 4, 0, 1, 2, 3]])\n",
    "surf2 = pv.PolyData(vertices_y, faces2)\n",
    "\n",
    "\n",
    "vertices_ax = np.array(x['finite_axial_surf'])\n",
    "arrow3 = pv.Arrow(compute_square_center(*x['finite_axial_surf']), x['axial_surf_normal'])\n",
    "faces3 = np.hstack([ [ 4, 0, 1, 2, 3]])\n",
    "surf3 = pv.PolyData(vertices_ax, faces3)\n",
    "\n",
    "\n",
    "\n",
    "p = pv.Plotter()\n",
    "\n",
    "#arrow_d1= pv.Arrow(compute_square_center(*x['limb1']), x['d1'])\n",
    "#arrow_d2 = pv.Arrow(compute_square_center(*x['limb2']), x['d2'])\n",
    "#computed_rotated_normal = pv.Arrow([-3,-3,-3], x['computed_rotated_normal'])\n",
    "\n",
    "p.add_mesh(arrow1, color='red', show_edges=True)\n",
    "p.add_mesh(arrow2, color='red', show_edges=True)\n",
    "#p.add_mesh(arrow3, color='red', show_edges=True)\n",
    "####\n",
    "#p.add_mesh(arrow_d1, color='red', show_edges=True)\n",
    "#p.add_mesh(arrow_d2, color='red', show_edges=True)\n",
    "#p.add_mesh(computed_rotated_normal, color='black', show_edges=True)\n",
    "####\n",
    "\n",
    "p.add_mesh(surf1, color='grey', show_edges=True)\n",
    "p.add_mesh(surf2, color='yellow', show_edges=True)\n",
    "#p.add_mesh(surf3, color='black', show_edges=True)\n",
    "\n",
    "# Render all of them\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## useful methods \n",
    "\n",
    "\n",
    "def check_sides_parallel_to_fold_axis(limb1_nodes, limb2_nodes):\n",
    "    \"\"\"Method to check if at least one side of the two limbs sqare surfaces is\n",
    "      parallel to the fold axis or not\"\"\"\n",
    "\n",
    "    n1 = calculate_square_normal(*limb1_nodes)\n",
    "    n2 = calculate_square_normal(*limb2_nodes)\n",
    "    fold_axis = compute_fold_axis(n1, n2)\n",
    "    axial_surf_normal = compute_axial_surface_normal(n1, n2, fold_axis)\n",
    "    p_intersection = compute_intersection_point(limb1_nodes, limb2_nodes)\n",
    "    axial_plane = sp.Plane(sp.Point(p_intersection), sp.Point(axial_surf_normal))\n",
    "    \n",
    "    distance1 = np.sort([float(axial_plane.distance(sp.Point(node_i))) for node_i in limb1_nodes])\n",
    "    distance2 = np.sort([float(axial_plane.distance(sp.Point(node_i))) for node_i in limb2_nodes])\n",
    "    limbs_not_parallel_to_fold_axis = []\n",
    "    if distance1[0] == distance1[1]:\n",
    "        limbs_not_parallel_to_fold_axis.append(limb1_nodes)\n",
    "    if distance2[0] == distance2[1]:\n",
    "        limbs_not_parallel_to_fold_axis.append(limb2_nodes)\n",
    "    #limbs_not_parallel_to_fold_axis = [limb for limb in []]\n",
    "    return limbs_not_parallel_to_fold_axis\n",
    "\n",
    "\n",
    "def compute_point_project_onto_plane(point_to_project, vector1, vector2, point_on_plane):\n",
    "    # Calculate the unit normal vector\n",
    "    unit_normal = np.cross(vector1,\n",
    "                            vector2) / np.linalg.norm(np.cross(vector1, vector2))\n",
    "    # Calculate the vector from the given point to the point on the plane\n",
    "    vector_to_plane = point_to_project - point_on_plane\n",
    "    # Calculate the projection of vector_to_plane onto the unit_normal vector\n",
    "    projection = np.dot(vector_to_plane, unit_normal)\n",
    "    # Calculate the projected point on the plane\n",
    "    return point_to_project - projection * unit_normal\n",
    "    \n",
    "\n",
    "def compute_most_distant_point_limbs(limb1_nodes, limb2_nodes):\n",
    "    most_distant_point_list1 = None\n",
    "    most_distant_point_list2 = None\n",
    "    max_distance = -1  # Initialized to a negative value, so any distance will be greater\n",
    "    most_distant_point_list1 = None\n",
    "    most_distant_point_list2 = None\n",
    "    max_distance = -1  # Initialized to a negative value, so any distance will be greater\n",
    "\n",
    "    # Iterate over each combination of points from list1 and list2\n",
    "    for point1 in limb1_nodes:\n",
    "        for point2 in limb2_nodes:\n",
    "            distance = np.linalg.norm(point1 - point2)  # Calculate the distance between the points\n",
    "            if distance > max_distance:\n",
    "                max_distance = distance\n",
    "                most_distant_point_list1 = point1\n",
    "                most_distant_point_list2 = point2\n",
    "    return list([most_distant_point_list1, most_distant_point_list2])\n",
    "    \n",
    "\n",
    "def compute_s_vector_limb1_toward_limb2(limb1_normal, point_on_limb1,\n",
    "                                         limb2_normal, point_on_limb2):\n",
    "    t = (limb1_normal +limb2_normal) / np.linalg.norm(limb1_normal +limb2_normal)\n",
    "    \n",
    "    # use for now 'compute_fold_axis()' but need making it compute p prime\n",
    "    pp = compute_fold_axis_non_oriented(limb1_normal, limb2_normal)\n",
    "    # use the point of the second limb as a center of the plane that is parallel to the axial plane\n",
    "    # project the point on the limb1 on this plane\n",
    "    projected_point = compute_point_project_onto_plane(point_to_project = point_on_limb1 ,\n",
    "                                                        vector1 = t , \n",
    "                                                        vector2 = pp, \n",
    "                                                        point_on_plane = point_on_limb2)\n",
    "    # return the vector between the projection and the inital point on limb1 as the vector pointing to the limb2\n",
    "    return np.array((projected_point-point_on_limb1 ) / np.linalg.norm(projected_point-point_on_limb1))\n",
    "     \n",
    "\n",
    "def compute_plane(point, normal):\n",
    "    return Plane(Point(point), Point(normal))\n",
    "\n",
    "\n",
    "def compute_working_direction(limb_nodes,  preferential_diretion = 'x'):\n",
    "    if preferential_diretion == 'x':\n",
    "        return np.array([1,0,0])\n",
    "    else:\n",
    "        diag_distance = np.linalg.norm((np.array(limb_nodes[0])-np.array(limb_nodes[2])))\n",
    "        diag_distance = diag_distance*1.3\n",
    "        random_direction = np.random.rand(3) - 0.5  # Create a random vector in the range [-0.5, 0.5] for each component\n",
    "        random_direction /= np.linalg.norm(random_direction)  \n",
    "        center = compute_square_center(*limb_nodes)\n",
    "        new_point = center + diag_distance * random_direction\n",
    "        plane_normal = calculate_square_normal(*limb_nodes)\n",
    "        while np.abs(np.dot(new_point - center, plane_normal)) < 1e-6:\n",
    "        # Adjust the new point if it's still on the plane (within a small tolerance)\n",
    "            new_point += 1e-5 * random_direction\n",
    "        working_direction = np.array(new_point-center)\n",
    "        return working_direction\n",
    "    \n",
    "\n",
    "def generate_point_away_from_plane(original_point, plane_normal, distance, direction_vector):\n",
    "    # Ensure the direction vector is normalized\n",
    "    direction_vector = direction_vector / np.linalg.norm(direction_vector)\n",
    "\n",
    "    # Calculate the new point\n",
    "    new_point = original_point + distance * direction_vector\n",
    "\n",
    "    # Ensure the new point is not on the plane\n",
    "    if np.abs(np.dot(new_point - original_point, plane_normal)) < 1e-6:\n",
    "        # Adjust the new point if it's still on the plane (within a small tolerance)\n",
    "        new_point += 1e-6 * direction_vector\n",
    "\n",
    "    return new_point\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distinguish two kind of operations here:\n",
    "* representation\n",
    "* visualisation\n",
    "\n",
    "A representation is a formal description of how something appears in a given representation space, but it doesn't have to be visualised.<br>\n",
    "A visualisation takes care of the rendering of a representation with a given support (image, screen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation should also be made a bit more abstract.<br>\n",
    "1. There is a variety of object that can be rendered in a representation space (typically, different kinds of a dataset components)\n",
    "2. Several kinds of representation spaces could be envisionned (e.g., spatial 1D,2D,3D, or temporal, or just an abstract text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "from scipy import linalg\n",
    "\n",
    "class RepresentationSpace(object):\n",
    "    \"\"\"A general framework for Representating geological objects\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_framework = None):\n",
    "        \"\"\"Initialise base class attributes\"\"\"\n",
    "        self.knowledge_framework = knowledge_framework if knowledge_framework is not None else GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "        self._datasets= []\n",
    "        self._interpreted_objects = []\n",
    "        \n",
    "    def attach_dataset(self, dataset, use_extension= True, padding= None):\n",
    "        \"\"\"Attach a dataset to the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - dataset: a Dataset object to be attached\n",
    "        - use_extension: if True, uses the extension of the dataset, else keeps the current ones\n",
    "        - padding: if use_extension is True, the given paddign will be used to keep a space around the dataset\"\"\"\n",
    "        if dataset == None: return\n",
    "        if dataset in self._datasets: return\n",
    "        self._datasets += [dataset]\n",
    "        \n",
    "        if use_extension:\n",
    "            self.set_extension_from_data(padding= padding)\n",
    "        \n",
    "        dataset.setup_representation_space(physical_space= self)\n",
    "    \n",
    "    def get_datasets(self):\n",
    "        \"\"\"dataset getter\"\"\"\n",
    "        return self._datasets\n",
    "        \n",
    "    def attach_interpreted_object(self, interpreted_object, update_extension= True, padding= None):\n",
    "        \"\"\"Attach an interpreted object to the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - interpreted_object: an interpreted object  to be attached\n",
    "        - update_extension: if True, uses the extension of the interpreted object, else keeps the current ones\n",
    "        - padding: if use_extension is True, the given padding will be used to keep a space around the interpreted object\"\"\"\n",
    "        if interpreted_object == None: return\n",
    "        if interpreted_object in self._interpreted_objects: return\n",
    "        self._interpreted_objects += [interpreted_object]\n",
    "        \n",
    "        if update_extension:\n",
    "            # Todo self.update_extension_from_interpreted_object(interpreted_object, padding= padding)\n",
    "            print(\"Warning: extensionupdate from interpreted object is not implemented yet\")\n",
    "    \n",
    "    def get_interpreted_objects(self):\n",
    "        \"\"\"interpreted objects getter\"\"\"\n",
    "        return self._interpreted_objects\n",
    "        \n",
    "class TemporalRepresentationSpace(RepresentationSpace):\n",
    "    \"\"\"A `RepresentationSpace` representing temporal apsects of represented objects.\"\"\"\n",
    "    \n",
    "class PhysicalRepresentationSpace(RepresentationSpace):\n",
    "    \"\"\"A type of `RepresentationSpace` representing physical aspects of the represented objects.\"\"\"\n",
    "    \n",
    "    __default_coordinate_labels = [\"X\",\"Y\",\"Z\"]\n",
    "    \n",
    "    def __init__(self, dimension: int=None, coordinate_labels: str|list= None, dataset= None, **kargs):\n",
    "        \"\"\"Initialisation of the representation space.\n",
    "        \n",
    "        Parameters:\n",
    "        - dimension (int): specify the number of dimensions of the representation space, typically 1D, 2D, or 3D (i.e., 1, 2, or 3),\n",
    "        NB: larger dimension spaces are not supported. At least either the `dimension` parameter or `coordinate_label` parameter should be given.\n",
    "        - coordinate_labels(str|list(str)): gives the label(s) of the coordinates. If given, the number of dimensions is deduced from the size of the list\n",
    "        and `dimensions`is ignored, otherwise, the labels are taken from the `__default_coordinate_labels` based on the number of `dimension`s. \n",
    "        At least either the `dimension` parameter or `coordinate_label` parameter should be given.\n",
    "        - dataset: a Dataset object containing the data to be attached to this representation space.\n",
    "        Note that the RepresentationSpace can be created first and then updated automatically when creating the dataset attached to this space.\n",
    "        - **kargs:\n",
    "            - use_extension: if True, uses the extension of the dataset, else keeps the current ones\n",
    "            - padding: if use_extension is True, the given paddign will be used to keep a space around the dataset\n",
    "        \"\"\"       \n",
    "        super().__init__() \n",
    "        \n",
    "        assert not (coordinate_labels is None and dimension is None), \"At least one of the parameters should be specified\"\n",
    "        if coordinate_labels is None:\n",
    "            assert isinstance(dimension, int),\"dimension parameter must be an integer\"\n",
    "            assert dimension in [1,2,3], \"The specified number of dimensions ({:d}) is not supported, should be 1, 2 or 3.\".format(dimension)\n",
    "            self.dimension= dimension\n",
    "            self.coordinate_labels= PhysicalRepresentationSpace.__default_coordinate_labels[:self.dimension]\n",
    "        elif isinstance(coordinate_labels,str):\n",
    "            self.dimension= 1\n",
    "            self.coordinate_labels=  [coordinate_labels]\n",
    "        elif isinstance(coordinate_labels, list):\n",
    "            self.dimension= len(coordinate_labels)\n",
    "            self.coordinate_labels= coordinate_labels\n",
    "        else:\n",
    "            raise(\"Unsupported initialisation of representation space: dimension({}) and coordinate_label ({}).\\n At least one of the parameters shoudl be specified.\".format(dimension, coordinate_labels))\n",
    "\n",
    "        self.__default_padding= 0.1\n",
    "        self.set_extension()\n",
    "        self.attach_dataset(dataset,**kargs)\n",
    "    \n",
    "    def get_extension(self):\n",
    "        \"\"\"returns the extensions of the space\"\"\"\n",
    "        return self.extension\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"returns the size of the space in each dimension\"\"\"\n",
    "        ext = np.array(self.extension)\n",
    "        return ext[:,1] - ext[:,0]\n",
    "    \n",
    "    def get_center(self):\n",
    "        \"\"\"returns the center of the space\"\"\"\n",
    "        return np.mean(self.extension, axis= 1)\n",
    "    \n",
    "    def generate_random_location(self, n= 1):\n",
    "        \"\"\"Generates n random location uniformly distributed within the space extensions\"\"\"\n",
    "        return np.random.default_rng().uniform(low= self.extensions[:0], high= self.extensions[:1], size= n)\n",
    "    \n",
    "    def generate_random_dip(self, n= 1):\n",
    "        \"\"\"Generates n random dip values that are compatible with this space\"\"\"\n",
    "        return np.random.default_rng().uniform(0, 90, n)\n",
    "\n",
    "    def generate_random_dip_dir(self, n= 1):\n",
    "        \"\"\"Generates n random dip_dir values that are compatible with this space\n",
    "        \n",
    "        Note: The values are hardcoded to be in XZ cross section to simplify the example\"\"\"\n",
    "        return np.random.default_rng().choice([90,270], n)#np.random.default_rng().uniform(0, 360, n)\n",
    "    \n",
    "    def generate_random_polarity(self, n= 1):\n",
    "        \"\"\"Generates n random polarity values (-1 or 1)\"\"\"\n",
    "        return np.random.default_rng().choice([-1,1], n)\n",
    "    \n",
    "    def generate_random_size(self, n=1, min_perc= 0.05, max_perc= 0.99):\n",
    "        \"\"\"Generates n random size values\"\"\"\n",
    "        sizes = self.get_size()\n",
    "        max_sizes = max(sizes)\n",
    "        return np.random.default_rng().uniform(min_perc*max_sizes, max_perc*max_sizes, n)\n",
    "        \n",
    "    def set_extension(self, extension:list= None):\n",
    "        \"\"\"Setter for the extension (min,max) of the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - extension: a list containing a pair of min and max value for each dimension of the space.\n",
    "        If None, the default will be set, i.e., [[0,1]] * dimension\"\"\"\n",
    "        if extension is None: \n",
    "            self.extension = [[0,1]] * self.dimension\n",
    "            return\n",
    "        extension= np.array(extension)\n",
    "        assert extension.shape[0] == self.dimension, \"The specified extension ({}) do not match the space dimensions ({})\".format(extension, self.dimension)\n",
    "        assert extension.shape[1] == 2, \"The specified extension should provide both lower andupper bounds for each dimension, given: {}\".format(extension)\n",
    "        self.extension= extension\n",
    "        \n",
    "    def set_extension_from_data(self, padding= None):\n",
    "        \"\"\"Sets the extension of the space from the attached dataset\n",
    "        \n",
    "        If no dataset is attached yet, then default extension are used instead (min:0,max:1).\n",
    "        \n",
    "        Parameters:\n",
    "        - padding: a space that is left around the dataset, either a value compatible with the coordinates, or a list of values of same dimensions.\n",
    "        If None, by default the padding is 5% of the dataset range.\n",
    "        \"\"\"\n",
    "        non_empty_dataset = [data_i for data_i in self._datasets if data_i.extension is not None]\n",
    "        if len(non_empty_dataset) == 0:\n",
    "            self.set_extension()\n",
    "            return\n",
    "        \n",
    "        if padding is None:\n",
    "            padding= self.__default_padding\n",
    "        else:\n",
    "            try: # check if padding as a dimension\n",
    "                len(padding)\n",
    "            # if not, then use it a a scaling \n",
    "            except TypeError: #just checking it is a number\n",
    "                assert type(padding) == int or type(padding) == float, \"padding should be given as a number (int or float), here: \"+type(padding)\n",
    "                # keep the padding as is in this case\n",
    "            else:# else check its dimensions are ok\n",
    "                assert len(padding) == self.dimension, \"the dimensions of the specified padding (len({})->) should match the space dimension ({})\".format(padding, len(padding), self.dimension)\n",
    "                padding= np.array(padding)\n",
    "                \n",
    "        extension = non_empty_dataset[0].extension\n",
    "        for data_i in non_empty_dataset[1:]:\n",
    "            for dim_i in self.dimension:\n",
    "                extension[dim_i,0] = min(extension[dim_i,0], data_i.extension[dim_i])\n",
    "                extension[dim_i,1] = max(extension[dim_i,1], data_i.extension[dim_i])\n",
    "        \"\"\":todo: use projected coordinates instead of source coordinates, might fail if 3D data projected on a map\"\"\"\n",
    "                \n",
    "        # in any cases, except when padding and data is None\n",
    "        self.center= np.mean(extension, axis= 1)\n",
    "        diff= extension.T - self.center\n",
    "        diff= diff.T\n",
    "        \n",
    "        # padding is multiplied by the width along each axis and added to the extension\n",
    "        # check if each dimension is shrinked (ie. min == max -> diff == 0), takes the average extension of the other dimensions,\n",
    "        # unless all are shrinked, in which case [[-1,1]] * self.dimension is set\n",
    "        if np.all(diff == 0):\n",
    "            diff = np.array([[-1,1]] * self.dimension)\n",
    "        else:\n",
    "            zero_diff = np.where(~diff.any(axis=1))\n",
    "            non_zero_diff = np.where(diff.any(axis=1))\n",
    "            mean_diff = np.mean(diff[non_zero_diff], axis=0)\n",
    "            diff[zero_diff] = mean_diff\n",
    "        \n",
    "        extension= np.repeat([self.center],2,axis=0).T + (1+2*padding)*diff \n",
    "        \n",
    "        self.set_extension(extension)\n",
    "        \n",
    "    def filter_qualities(self,**qualities):\n",
    "        \"\"\"removes the named arguments corresponding to this space coordinates from qualities.\n",
    "        \n",
    "        Return:\n",
    "        - a copy with the passed parameters except for the ones correpsonding to the space coordinates\"\"\"\n",
    "        qualities =  {key:val for key, val in qualities.items() if key not in self.coordinate_labels}\n",
    "        return qualities\n",
    "    \n",
    "    def prepare_coordinate_qualities(self, **qualities):\n",
    "        \"\"\"transforms the coordinates passed in qualities into an appropriate format\n",
    "        \n",
    "        Return:\n",
    "        - a dict with quality keys and values for setting coordinates\"\"\"\n",
    "        coordinates = {key:val for key, val in qualities.items() if key in self.coordinate_labels}\n",
    "        coord_qualities = {}\n",
    "        for i,key in enumerate(self.coordinate_labels):\n",
    "            if key in coordinates:\n",
    "                val = coordinates[key]\n",
    "                coord_qualities[\"coord{}\".format(i+1)] = val if isinstance(val,list) else [val]\n",
    "                coord_qualities[\"coord{}_label\".format(i+1)] = [key]\n",
    "        return coord_qualities\n",
    "    \n",
    "    def label_map(self, object):\n",
    "        \"\"\"creates of mapping of coordinate labels\"\"\"\n",
    "        coord_label_params = [\"coord{}_label\".format(i) for i in range(1,4) if hasattr(object, \"coord{}_label\".format(i))]\n",
    "        non_empty_coord_label_param = [param for param in coord_label_params if len(getattr(object,param)) > 0]\n",
    "        label_map = {getattr(object,param)[0]:param for param in non_empty_coord_label_param if getattr(object,param)[0] in self.coordinate_labels}\n",
    "        return label_map\n",
    "        \n",
    "    def set_object_coordinates(self, object, **kargs):\n",
    "        \"\"\"Sets the coordinates corresponding to this space into the given object\n",
    "        \n",
    "        Parameters:\n",
    "        - object: the object whose coordinates needs to be set\n",
    "        - kargs: keyword argugments corresponding to the name and values of the coordinates.\n",
    "        They must match this space coordinate names, extra names will be ignored\"\"\"\n",
    "        \n",
    "        # find the object representation and check it is a point\n",
    "        #otherwise another setter must be used\n",
    "        if (object.has_Representation is None) or (len(object.has_Representation) < 1):\n",
    "            raise MalissiaBaseError(\"can't set the object coordinates because it doesn't have a geometrical representation\")\n",
    "        rep = object.has_Representation[0]\n",
    "        if not self.knowledge_framework.isinstance(rep, self.knowledge_framework().Point):\n",
    "            raise MalissiaBaseError(\"can't set the object coordinates because its geometrical representation is not a Point\")            \n",
    "        \n",
    "        # check if the object has coord{i}_label set, else set it\n",
    "        if mogi.has_qualities(rep, [\"coord{}_label\".format(i) for i in range(1,len(self.coordinate_labels)+1)]):\n",
    "            label_map = self.label_map(rep)\n",
    "            coordinate_values = {label_map[key].split(\"_\")[0]:(kargs[key] if isinstance([kargs[key]],list) else [kargs[key]])\n",
    "                                  for key in self.coordinate_labels if key in kargs}\n",
    "        else:\n",
    "            coordinate_values = self.prepare_coordinate_qualities(**kargs)\n",
    "        for param_name, val in coordinate_values.items():\n",
    "            setattr(rep, param_name, val)\n",
    "            \n",
    "    def get_object_coordinates(self, object):\n",
    "        \"\"\"Gets the coordinates corresponding to this space from the given object\"\"\"\n",
    "        \n",
    "        # find the object representation and check it is a point\n",
    "        #otherwise another setter must be used\n",
    "        if (object.has_Representation is None) or (len(object.has_Representation) < 1):\n",
    "            raise MalissiaBaseError(\"can't get the object coordinates because it doesn't have a geometrical representation\")\n",
    "        rep = object.has_Representation[0]\n",
    "        if not self.knowledge_framework.isinstance(rep, self.knowledge_framework().Point):\n",
    "            raise MalissiaBaseError(\"can't get the object coordinates because its geometrical representation is not a Point\") \n",
    "        \n",
    "        label_map = self.label_map(rep)\n",
    "        coord = np.full_like(self.coordinate_labels, np.nan, dtype= float)\n",
    "        for i, key in enumerate(self.coordinate_labels):\n",
    "            if key in label_map:\n",
    "                coord_param = label_map[key].split(\"_\")[0]\n",
    "                coord[i] = getattr(rep,coord_param)[0]\n",
    "        return coord\n",
    "    \n",
    "    def coordinates_to_dict(self, coords):\n",
    "        \"\"\" transforms a coordinate array into an appropriate dict with labels\"\"\"\n",
    "        return {label:value for label,value in zip(self.coordinate_labels, coords)}\n",
    "    \n",
    "    def get_normal_vector(self, feature):\n",
    "        dip = feature.dip\n",
    "        dip_dir = feature.dip_dir\n",
    "        return self.compute_normal_from_dip_dir(dip, dip_dir)\n",
    "        \n",
    "    def compute_normal_from_dip_dir(self, dip, dip_dir, polarity= 1):\n",
    "        dip_rad = np.deg2rad(dip)\n",
    "        dip_dir_rad = np.deg2rad(dip_dir)\n",
    "        z = np.cos(dip_rad)\n",
    "        h = np.sin(dip_rad)\n",
    "        y = h * np.cos(dip_dir_rad)\n",
    "        x = h * np.sin(dip_dir_rad)\n",
    "        return polarity * np.array([x,y,z])\n",
    "    \n",
    "    def compute_dip_dir_from_normal(self, normal):\n",
    "        polarity = 1 if normal[2] == 0 else np.sign(normal[2]) \n",
    "        x,y,z = polarity * np.array(normal) / np.linalg.norm(normal)\n",
    "        if z == 1:\n",
    "            return 0,0\n",
    "        dip = np.rad2deg(np.arccos(z))\n",
    "        dip_dir = np.rad2deg(np.arctan2(x,y)) % 360\n",
    "        return dip, dip_dir, polarity\n",
    "    \n",
    "    def compute_line_attitude_from_two_points(self, p0, p1, center= False):\n",
    "        \"\"\"Computes the attitude of a line going through two points\n",
    "        \n",
    "        Parameters:\n",
    "        - p0: coordinates of the first point\n",
    "        - p1: coordinates of the second point\n",
    "        \n",
    "        Returns:\n",
    "        - attitude: a dictionnary holding\n",
    "          \"dip_dir\" : the dip direction of the line. +or- 1 if dimension is  <3, value otherwise\n",
    "          \"dip\" : the dip of the line (ie., downward). None if dimension is <2, value otherwise\n",
    "        \"\"\"\n",
    "        v = np.array(p1) - np.array(p0)\n",
    "        center = np.mean(v,axis=0) if center else np.zeros(v.shape[-1])\n",
    "        if len(v) == 1:\n",
    "            dip_dir = np.sign(v)[0]\n",
    "            dip = None\n",
    "        else:\n",
    "            # make v downward\n",
    "            if v[-1] > 0:\n",
    "                v *= -1 \n",
    "            if len(v) == 2:\n",
    "                dip_dir = np.sign(v[0])\n",
    "                v_abs = np.abs(v)\n",
    "                dip = np.rad2deg(np.arctan2(v_abs[1], v_abs[0]))\n",
    "            else:\n",
    "                dip_dir = np.rad2deg(np.arctan2(v[0],v[1]))\n",
    "                dip_dir = 360 - np.abs(dip_dir) if dip_dir < 0 else dip_dir\n",
    "                dip = -np.rad2deg(np.arctan2(v[2], np.linalg.norm(v[:2]) ))\n",
    "        size = np.linalg.norm(v)\n",
    "                \n",
    "        return {\"dip_dir\":dip_dir, \"dip\":dip, \"center\":center, \"size\":size}\n",
    "            \n",
    "    def compute_principal_directions(self, p, center= False):\n",
    "        \"\"\"Computes the principal directions in a set of vectors\n",
    "        \n",
    "        This can be used to compute the principal vectors in a set of vectors\n",
    "        or medium plan in a group of points (needs to set center to True).\n",
    "        \n",
    "        Parameters:\n",
    "        - p: points/vectors in the shape (# points, # dimensions)\n",
    "        - center: tells if points should be centered first, important for computing medium plane, default is False\n",
    "        \n",
    "        Returns: a dictionnary with:\n",
    "        - \"values\": the singular values of the principal axes\n",
    "        - \"vectors\": the vectors of the principal axes\n",
    "        - \"center\": the center of points if center is set to True, zeros otherwise\n",
    "        \"\"\"\n",
    "        p = np.array(p)\n",
    "        center = np.mean(p,axis=0) if center else np.zeros(p.shape[-1])\n",
    "        p = p - center\n",
    "        mat = np.dot(p.T, p)\n",
    "        _,s,Vh = linalg.svd(mat)\n",
    "        result = {\n",
    "            \"values\": s,\n",
    "            \"vectors\": Vh,\n",
    "            \"center\": center\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "    def compute_average_vector(self, vectors):\n",
    "        \"\"\"Compute the average vector\"\"\"\n",
    "        principal_directions = self.compute_principal_directions(vectors)\n",
    "        principal_vectors = principal_directions[\"vectors\"]\n",
    "        return principal_vectors[0]\n",
    "    \n",
    "    def compute_attitude_from_points(self, p):\n",
    "        \"\"\"Computes the attitude dip and dip_dir for a medium plane going through points\n",
    "        \n",
    "        Parameters:\n",
    "        - p: the points coordinates given in shape ()\n",
    "        \"\"\"\n",
    "        if len(p) < self.dimension:\n",
    "            raise MalissiaBaseError(\"Underdetermined attitude computation.\")\n",
    "            \n",
    "        if self.dimension == 2:\n",
    "            return self.compute_line_attitude_from_two_points(p, center= True)\n",
    "        elif self.dimension == 3:\n",
    "            return self.compute_plane_from_points(p)\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"unsupported dimension for attitude computation.\")\n",
    "        \n",
    "    def compute_plane_from_points(self, p):\n",
    "        \"\"\"Computes the attitude dip and dip_dir for a medium plane going through points\n",
    "        \n",
    "        Parameters:\n",
    "        - p: the points coordinates given in shape ()\n",
    "        \n",
    "        Returns:\n",
    "        - \"dip_dir\": azimuth from North towards the East, None if dip is zero\n",
    "        \"\"\"\n",
    "        principal_direction = self.compute_principal_directions(p, center= True)\n",
    "        \n",
    "        result = {}\n",
    "        result[\"center\"] = principal_direction[\"center\"]\n",
    "        result[\"major_axis\"] = principal_direction[\"vectors\"][0]\n",
    "        result[\"minor_axis\"] = principal_direction[\"vectors\"][1]\n",
    "        result[\"size\"] = np.sqrt(principal_direction[\"values\"][0])\n",
    "        \n",
    "        result[\"normal\"] = principal_direction[\"vectors\"][-1]\n",
    "        n = -result[\"normal\"] if result[\"normal\"][2]<0 else result[\"normal\"]\n",
    "        h = np.linalg.norm(n[:2])\n",
    "        result[\"dip\"] = np.rad2deg(np.arctan2(h, n[2]))\n",
    "        if h == 0:\n",
    "            result[\"dip_dir\"] = None\n",
    "        else:\n",
    "            dip_dir = np.rad2deg(np.arctan2(n[0], n[1]))\n",
    "            result[\"dip_dir\"] = 360 - np.abs(dip_dir) if dip_dir < 0 else dip_dir\n",
    "        result[\"azimuth\"] = result[\"dip_dir\"] - 90 if result[\"dip_dir\"] > 90 else result[\"dip_dir\"] + 270\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Description of the physical space parameters and data\n",
    "        \"\"\"\n",
    "        desc= [\"Representation space of type: {:s}\".format(type(self).__name__)]\n",
    "        desc+= [\"- Number of dimension(s): {}\".format(self.dimension)]\n",
    "        desc+= [\"- Coordinate label(s): {}\".format(self.coordinate_labels)]\n",
    "        desc+= [\"- Space extension:\"]\n",
    "        for dim_i, lim_i in zip(self.coordinate_labels,self.extension):\n",
    "            desc+= [\" |- Coord {}: {}\".format(dim_i, lim_i)]  \n",
    "        return \"\\n\".join(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class GeologicalDataset(object):\n",
    "    \"\"\"A GeologicalDataset gathers information about geological data to be interpreted.\n",
    "    \n",
    "    This class is a hybrid ontology&python class. It is providing pythonic algorithm and high level interface,\n",
    "    while the data is actually stored in an ontology.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, physical_space= None, time_space= None, representation_spaces= None, knowledge_framework= None):\n",
    "        \"\"\"Initialises a `GeologicalDataset`\n",
    "        \n",
    "        Parameters:\n",
    "        - physical_space: a `PhysicalRepresentationSpace`,  which defines the spatial coordinates of this dataset\n",
    "        - time_space: a `TemporalRepresentationSpace`,  which defines the time coordinates of this dataset\n",
    "        - representation_spaces: list of `Representationspace`s to which the dataset must be attached\n",
    "        Note: datasets can be created without representation space and attached later on by using `RepresentationSpace.attach_dataset`\n",
    "        or `GeologicalDataset.setup_representation_space`.\n",
    "        Alternativelly, a single `PhysicalRepresentationspace` and or `TemporalRepresentationspace` can be given here if `physical_space` and `time_space` are None.\n",
    "        - default_representation_space: the main RepresentationSpace to which this dataset is attached.\n",
    "        If None and representation_spaces are provided, then the first one will be taken.\n",
    "        If the default one is not initially in the full list, then it is added to it.\n",
    "        - ontology: the name of the ontology to be used for storing the data.\n",
    "        If None, the default will be taken from `the GeologicalKnowledgeManager`.\n",
    "        \n",
    "        Internals: this method initialises several internal attributes:\n",
    "        - extension: represents the extension of the dataset in the attached representation space\n",
    "        (i.e., the default on if this dataset is represented in several representation spaces\n",
    "        - representation_spaces: the dataset can be attached to and represented into several representation spaces,\n",
    "        `physical_space` and `time_space` are included into this list.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.knowledge_framework= knowledge_framework if knowledge_framework is not None else GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "        \n",
    "        self.extension = None\n",
    "        \n",
    "        # setup representation spaces\n",
    "        self.representation_spaces= set()\n",
    "        if physical_space is None:\n",
    "            # try to initialise the physical space with first existing data\n",
    "            observations = self.get_observations()\n",
    "            if len(observations) > 0:\n",
    "                coord_labels = [getattr(observations[0],\"coord{}_label\".format(i)) for i in range(1,4)]\n",
    "                coord_labels = [label[0] for label in coord_labels if len(label)>0]\n",
    "                physical_space = PhysicalRepresentationSpace(coordinate_labels=coord_labels)\n",
    "        self.setup_representation_space(physical_space, time_space, representation_spaces)\n",
    "        \n",
    "        # register the datast in the listed representation spaces\n",
    "        for space_i in self.representation_spaces:\n",
    "            space_i.attach_dataset(self)\n",
    "        \n",
    "        # initialize extension of the dataset\n",
    "        self.update_extension()\n",
    "            \n",
    "    def __dell__(self):\n",
    "        self.remove_all_observations()\n",
    "\n",
    "    def __setup_space(self, space, space_type):\n",
    "        \"\"\"Check if space of given type is in list or parameter and return the appropriate value.\n",
    "        \n",
    "        Take the given physical/time space, or if None use the first one in the list, and if none just leave None.\n",
    "        Adds the space to the `self.representation_spaces` set.\"\"\"\n",
    "        if space is not None: \n",
    "            self.representation_spaces.add(space)\n",
    "            return space\n",
    "        if len(self.representation_spaces) == 0: return None\n",
    "        space_list= [space_i for space_i in self.representation_spaces if isinstance(space_i, space_type)]\n",
    "        return space_list[0] if len(space_list) > 0 else None\n",
    "    \n",
    "    def update_extension(self, update_representation_spaces= True):\n",
    "        \"\"\"Sets the extension of the dataset in the physical space from existing observations\n",
    "        \n",
    "        If there is no observation, `self.extension` is set to None\"\"\"\n",
    "        observations = self.get_observations()\n",
    "        if (len(observations) == 0) or (self.physical_space is None) or (self.physical_space.dimension == 0):\n",
    "            self.extension = None \n",
    "            return\n",
    "            \n",
    "        di = observations[0]\n",
    "        coord = self.physical_space.get_object_coordinates(di)\n",
    "        self.extension = np.repeat([coord],2,axis=0).T\n",
    "        for di in observations[1:]:\n",
    "            coord = self.physical_space.get_object_coordinates(di)\n",
    "            for j, val in enumerate(coord):\n",
    "                self.extension[j,0] = min(self.extension[j,0], val)\n",
    "                self.extension[j,1] = max(self.extension[j,1], val)\n",
    "                \n",
    "        if update_representation_spaces:\n",
    "            if self.physical_space is not None:\n",
    "                self.physical_space.set_extension_from_data()\n",
    "        \n",
    "    def setup_representation_space(self, physical_space= None, time_space= None, representation_spaces= None):\n",
    "        \"\"\"Setup the representation space list and default\n",
    "        \n",
    "        Parameters:\n",
    "        - physical_space: a `PhysicalRepresentationSpace`,  which defines the spatial coordinates of this dataset\n",
    "        - time_space: a `TemporalRepresentationSpace`,  which defines the time coordinates of this dataset\n",
    "        - representation_spaces: list of `Representationspace`s to which the dataset must be attached\n",
    "        Note: datasets can be created without representation space and attached later on by using `RepresentationSpace.attach_dataset`\n",
    "        or `GeologicalDataset.setup_representation_space`.\n",
    "        Alternativelly, a single `PhysicalRepresentationspace` and or `TemporalRepresentationspace` can be given here if `physical_space` and `time_space` are None.\n",
    "        \"\"\"\n",
    "        if representation_spaces is not None: self.representation_spaces = self.representation_spaces.union(representation_spaces)\n",
    "        self.physical_space = self.__setup_space(physical_space, PhysicalRepresentationSpace)\n",
    "        self.time_space = self.__setup_space(time_space, TemporalRepresentationSpace)\n",
    "        \n",
    "        for space in self.representation_spaces:\n",
    "            space.attach_dataset(self)\n",
    "        \n",
    "    def get_observations(self, observation_type= None, qualities= None, name= None):\n",
    "        \"\"\"Accessor to the observations stored in the internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - observation_type: the type of the searched observations as defined by the internal ontology\n",
    "        - qualities: qualities to filter the observations (c.f. `KnowledgeFramework.search`)\n",
    "        - name: name of the serached observation (c.f. `KnowledgeFramework.search`)\"\"\"\n",
    "        observation_type = observation_type if observation_type is not None else self.knowledge_framework().PointBased_Observation\n",
    "        return self.knowledge_framework.search(type= observation_type, qualities= qualities, name= name)\n",
    "    \n",
    "    def get_unexplained_observations(self, observation_type= None, qualities= None, name= None):\n",
    "        \"\"\"Accessor filtering out explained observation\n",
    "        \n",
    "        Parameters:\n",
    "        - observation_type: the type of the searched observations as defined by the internal ontology\n",
    "        - qualities: qualities to filter the observations (c.f. `KnowledgeFramework.search`)\n",
    "        - name: name of the serached observation (c.f. `KnowledgeFramework.search`)\"\"\"\n",
    "        observations = self.get_observations(observation_type= observation_type, qualities= qualities, name= name)\n",
    "        observations = [obs_i for obs_i in observations if len(obs_i.is_Explained_by) == 0]\n",
    "        return observations\n",
    "    \n",
    "    def get_occurrence_observations(self):\n",
    "        \"\"\"helper method to access occurrence data, i.e., those having a occurrence quality\n",
    "        \n",
    "        :todo: for now the occurrence quality doesn't exist so all the observations are occurrence by default\"\"\"\n",
    "        return self.get_observations(qualities= \"occurrence\")\n",
    "    \n",
    "    def get_orientation_observations(self):\n",
    "        return self.get_observations(qualities= \"dip\")\n",
    "    \n",
    "    def remove_observation(self, observations, update_extension= True):\n",
    "        \"\"\"Removes the given observations stored in this dataset and internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - observations: an iterable containing objects of the internal ontology.\n",
    "        Note that you can use the `search`method to get such a list\n",
    "        - update_extension: if True (default) the extension will be updated\"\"\"\n",
    "        self.knowledge_framework.remove_all_instances(observations)\n",
    "        if update_extension: self.update_extension()\n",
    "            \n",
    "    def remove_observation_by_name(self, name:str, update_extension= True):\n",
    "        \"\"\"Removes the given observations stored in this dataset and internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation to be removed\n",
    "        - update_extension: if True (default) the extension will be updated\"\"\"\n",
    "        self.remove_observation(self.get_observations(name= name), update_extension)\n",
    "        \n",
    "    def remove_all_observations(self, update_extension= True):\n",
    "        \"\"\"Removes all the observations stored in this dataset and internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        Note: the update is performed only once at the end.\"\"\"\n",
    "        self.remove_observation(self.get_observations(), update_extension= False)\n",
    "        if update_extension: self.update_extension()\n",
    "        \n",
    "    def add_observation(self, name: str= None, update_extension= True, **kargs):\n",
    "        \"\"\"creates a new observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\n",
    "          |   Also Note: the arguments with None value are removed from the dict\"\"\"\n",
    "        if self.physical_space is  None:\n",
    "            raise MalissiaBaseError(\"Trying to add observation while physical space is not set. Please setup_representation_space first\")\n",
    "        \n",
    "        constructor = self.knowledge_framework.select_object_constructor(\"Observation\", dataset=self, **kargs)\n",
    "        new_observation = constructor(knowledge_framework= self.knowledge_framework, dataset= self, name= name, **kargs)\n",
    "        if update_extension: self.update_extension()\n",
    "        \n",
    "    def add_occurrence_observation(self, name: str, observed_object:str, occurrence= True, update_extension= True, **kargs):\n",
    "        \"\"\"creates a new occurrence observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - observed_object: the name of the observed object\n",
    "        - occurrence: True (default) if the object was observed here, False if it was observed that it is not there.\n",
    "        Note that this is different from not having observed that it is here, in which case there should not be an observation.\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\"\"\"\n",
    "        if observed_object is not None: kargs[\"geology\"]= observed_object\n",
    "        if occurrence is not None: kargs[\"occurrence\"]= occurrence\n",
    "        self.add_observation(name, update_extension= update_extension, **kargs)\n",
    "    \n",
    "    def add_orientation_observation(self, name: str, observed_object:str, dip, dip_dir, occurrence= True, update_extension= True, **kargs):\n",
    "        \"\"\"creates a new orientation observation and adds it to te internal ontology\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name of the observation (similar to an observation id)\n",
    "        - observed_object: the name of the observed object\n",
    "        - dip: the value of the measured dip (in degrees, 0-90)\n",
    "        - dip_dir: the value of the dip direction (in degrees, 0-360, from North towards the East)\n",
    "        - occurrence: True (default) if the object was observed here.\n",
    "        This is the default behaviour because if the measurement was made here, we assume that the object actually existed\n",
    "        so this is in itself a proof ox occurrence. However, one might want to record the orientation without specifically attaching any\n",
    "        observation of occurrence, in which case None should be given for occurrence and the quality won't be set.\n",
    "        False, would not make much sense as it would imply that the orientation was measured but we observed that the object wasn't there.\n",
    "        - update_extension: if True (default) the extension will be updated.\n",
    "        - **kargs:\n",
    "          |- any argument whose name corresponds to the `physical_space`coordinate labels or other properties.\n",
    "          |   Note: all the coordinates must be specified\"\"\"\n",
    "        if occurrence is not None: kargs[\"occurrence\"]= occurrence\n",
    "        if occurrence == False: logging.warning(\"occurrence parameter was set to False while adding an orientation observation.\"\\\n",
    "            \"This is weird because it would imply the measure was taken but the rock couldn't be observed.\"\\\n",
    "            \"Did you intend to avoid recording the occurrence, in which case you should prefer None isntead of False.\")\n",
    "        \n",
    "        if observed_object is not None: kargs[\"geology\"]= observed_object\n",
    "        if (dip is not None) and (dip_dir is not None):\n",
    "            kargs[\"dip\"]= dip\n",
    "            kargs[\"dip_dir\"]= dip_dir\n",
    "        self.add_observation(name, update_extension= update_extension, **kargs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.get_observations())\n",
    "         \n",
    "    def __str__(self):\n",
    "        \"\"\"Description of the dataset\n",
    "        \"\"\"\n",
    "        desc= [\"A dataset of type: {:s}\".format(type(self).__name__)]\n",
    "        n = len(self)\n",
    "        if n == 0:\n",
    "            desc+= [\"- The dataset is empty\"]\n",
    "        else:\n",
    "            desc+= [\"- Size of dataset: {:d}\".format(n)]\n",
    "            desc+= [\"- Types of data:\"]\n",
    "            desc+= [\" |- occurrence:\\t{} entries\".format(len(self.get_occurrence_observations()))]\n",
    "            desc+= [\" |- orientation:\\t{} entries\".format(len(self.get_orientation_observations()))]\n",
    "        if self.extension is None:\n",
    "            desc+= [\"- Extension: \"+str(self.extension)]\n",
    "        else:\n",
    "            desc+= [\"- Extension:\"]\n",
    "            labels = self.physical_space.coordinate_labels if self.physical_space is not None else [\"Coord_{}\".format(i) for i in range(3)][:len(self.extension)]\n",
    "            for dim_i, lim_i in zip(labels,self.extension):\n",
    "                desc+= [\" |- Coord {}: [{}, {}]\".format(dim_i, *lim_i)]  \n",
    "        desc+= [\"- Number of spaces this dataset is attached to: {:d}\".format(len(self.representation_spaces))]\n",
    "        if self.physical_space is None:\n",
    "            desc+= [\" |- Physical space: None\"]\n",
    "        else:\n",
    "            desc+= [\" |- Physical space:\"+\"\\n | |\".join(self.physical_space.__str__().split(\"\\n\"))]\n",
    "        if self.time_space is None:\n",
    "            desc+= [\" |- Temporal space: None\"]\n",
    "        else:\n",
    "            desc+= [\" |- Temporal space:\"+\"\\n | |\".join(self.time_space.__str__().split(\"\\n\"))]\n",
    "        return \"\\n\".join(desc) \n",
    "    \n",
    "    def head(self, n:int= 5):\n",
    "        \"\"\"Returns the `n`first data in the dataset\"\"\"\n",
    "        return self.to_dataframe(max_rows=n)\n",
    "    \n",
    "    def info(self):\n",
    "        \"\"\"Returns a description of the dataset\"\"\"\n",
    "        return self.__str__()\n",
    "    \n",
    "    def to_dataframe(self, max_rows:int= None):\n",
    "        \"\"\"Creates a `pandas.DataFrame` showing the data in this dataset\n",
    "        \n",
    "        Parameters:\n",
    "        - max_rows: limits the number of rows in the output, unless None is given (default).\n",
    "        Note: interanlly, all the observations are still recovered from the internal ontology, \n",
    "        but only the `max_rows`first ones are show for consision.\n",
    "        \"\"\"\n",
    "        columns = [\"name\"]\n",
    "        if self.physical_space is not None:\n",
    "            columns += self.physical_space.coordinate_labels\n",
    "        if self.time_space is not None:\n",
    "            columns += self.time_space.coordinate_labels\n",
    "        columns += [\"dip_dir\",\"dip\",'geology', 'occurrence']\n",
    "        output_frame = pd.DataFrame(columns= columns)\n",
    "        output_frame.set_index(\"name\",inplace=True)\n",
    "        \n",
    "        observations = self.get_observations() \n",
    "        observations = observations if max_rows is None else observations[:max_rows]\n",
    "        for di in observations:\n",
    "            for prop in di.get_properties():\n",
    "                if prop.name == '': continue\n",
    "                if \"coord\" in prop.name: continue\n",
    "                output_frame.loc[di.name,prop.name] = prop[di][0]\n",
    "                \n",
    "            if self.physical_space is not None:\n",
    "                for label, val in zip(self.physical_space.coordinate_labels, self.physical_space.get_object_coordinates(di)):\n",
    "                    output_frame.loc[di.name,label] = val\n",
    "            if self.time_space is not None:\n",
    "                for label, val in zip(self.time_space.coordinate_labels, self.time_space.get_object_times(di)):\n",
    "                    output_frame.loc[di.name,label] = val\n",
    "        coordinate_types = {label:float for label in self.physical_space.coordinate_labels} if self.physical_space is not None else {}\n",
    "        time_types = {label:float for label in self.physical_space.coordinate_labels} if self.time_space is not None else {}\n",
    "        other_types = {'dip_dir':float, 'dip':float, 'geology':str, \"occurrence\": bool}\n",
    "        output_frame = output_frame.astype({**coordinate_types, **other_types})\n",
    "        return output_frame\n",
    "    \n",
    "def load_dataset_from_csv(source:str, dataset:GeologicalDataset = None, coordinate_labels = [\"X\",\"Y\",\"Z\"], **kargs) ->GeologicalDataset:\n",
    "    \"\"\"Loads a dataset from a csv file\n",
    "    \n",
    "    Parameters:\n",
    "    - source(str): the source file from which the data should be loaded\n",
    "    - dataset: the `GeologicalDataset` in which the data will be loaded. If None, the dataset will be created.\n",
    "    - **arkgs: passed to pandas.read_csv\n",
    "    \n",
    "    Return:\n",
    "    - the `GeologicalDataset` with the newly loaded data (a new `GeologicalDataset` is created if needed).\"\"\"\n",
    "    try:\n",
    "        dataframe = pd.read_csv(source, **filter_kargs(pd.read_csv,**kargs))\n",
    "    except Exception as e:\n",
    "        raise( Exception(\"This error occurred while loading a dataset from: {}\\nAdditional arguments were given: {}\".format(source,\n",
    "                            \",\".join([\"{}:{}\".format(key,val) for key, val in kargs.items()]))))\n",
    "        \n",
    "    coordinate_labels = [label for label in coordinate_labels if label in dataframe.columns]\n",
    "    if len(coordinate_labels) == 0:\n",
    "        logging.warning(\"There isn't any coordinate column in the loaded dataset.\\nCheck the output and consider changing the separator with sep keyword or cahnge 'coordinate_label' parameter.\")\n",
    "    return load_dataset_from_dataframe(dataframe, dataset, coordinate_labels= coordinate_labels, **filter_kargs(load_dataset_from_dataframe,**kargs) )\n",
    "\n",
    "def load_dataset_from_dataframe(dataframe, dataset:GeologicalDataset = None, coordinate_labels = [\"X\",\"Y\",\"Z\"], labels= None, index= None, dtypes= None, clear_existing_data= True):\n",
    "    \"\"\"Loads a dataset from a `pandas.DataFrame`\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe(`pandas.DataFrame`): the source dataframe from which the data should be loaded\n",
    "    - dataset: the `GeologicalDataset` in which the data will be loaded. If None, the dataset will be created.\n",
    "    - coordinate_labels: the labels to be used as coordinates of the physical space\n",
    "    - labels: a dict to relabel the dataframe columns prior to loading in the dataset.\n",
    "    This is usefull for example when the coordinates in the source aren't labelled the same as in the internal ontology.\n",
    "    The format is {\"old_label\":\"new_label\", ...}.\n",
    "    - index: the label of the column (in original DataFrame, i.e., before renaming), which is to be used as index\n",
    "    - dtypes: a dict containing a mapping between column name and type\n",
    "    - clear_existing_data: if set (default), any oservation stored in the internal ontology is removed prior to loading the new dataset \n",
    "    \n",
    "    Return:\n",
    "    - the `GeologicalDataset` with the newly loaded data (a new `GeologicalDataset` is created if needed).\"\"\"\n",
    "    if dataset is None:\n",
    "        physical_space = PhysicalRepresentationSpace(coordinate_labels= coordinate_labels)\n",
    "        dataset = GeologicalDataset(physical_space = physical_space)\n",
    "    else:\n",
    "        if dataset.physical_space is None or clear_existing_data:\n",
    "            physical_space = PhysicalRepresentationSpace(coordinate_labels= coordinate_labels)\n",
    "            dataset.setup_representation_space(physical_space = physical_space)\n",
    "        else:\n",
    "            if dataset.physical_space.coordinate_labels != coordinate_labels:\n",
    "                raise MalissiaBaseError(\"Trying to add data into an existing dataset with different coordiante labels\")\n",
    "                \n",
    "    if clear_existing_data:\n",
    "        dataset.remove_all_observations()\n",
    "\n",
    "    # declare default dtypes here, in case they should be relabelled\n",
    "    default_coord_types = {key:float for key in dataset.physical_space.coordinate_labels}\n",
    "    other_types = {'dip_dir':float, 'dip':float, 'geology':str, 'observed_object':str, \"occurrence\":bool}\n",
    "    dtypes= dtypes if dtypes is not None else {**default_coord_types, **other_types}\n",
    "    assert isinstance(dtypes, dict), \"dtypes for type management should be given as a dict\"\n",
    "    \n",
    "    # relabelling\n",
    "    if labels is not None:\n",
    "        dataframe = dataframe.rename(columns= labels)\n",
    "        index= labels[index] if index in labels else index\n",
    "        dtypes= {labels[key] if key in labels else key: value for key, value in dtypes.items()}\n",
    "    \n",
    "    # setting the index\n",
    "    if index is not None:\n",
    "        # if already set, reset it\n",
    "        if type(dataframe.index) != pd.core.indexes.base.Index:\n",
    "            dataframe = dataframe.reset_index()\n",
    "        # then set the index\n",
    "        try:\n",
    "            dataframe = dataframe.set_index(index)\n",
    "        except Exception as e:\n",
    "            raise( Exception(\"Error while setting the index of the loaded dataset:\\nThis is how the dataframe looks like:\\n\"+dataframe.head().to_string()))\n",
    "    \n",
    "    for extra_type_i in dtypes.keys() - set(dataframe.columns):\n",
    "        del dtypes[extra_type_i]\n",
    "    dataframe = dataframe.astype(dtypes)\n",
    "        \n",
    "    for name_i, values_i in dataframe.iterrows():\n",
    "        # drop nan values and filter None objects\n",
    "        values_i = {key:val for key, val in values_i.dropna().items() if val is not None}\n",
    "        dataset.add_observation(name_i, **values_i, update_extension= False)\n",
    "    dataset.update_extension()\n",
    "    \n",
    "    # resetting the default size when all data are loaded if not specified\n",
    "    if \"size\" not in dataframe:\n",
    "        size = max(physical_space.get_size())\n",
    "        for di in dataset.get_observations():\n",
    "            di.size = [float(size/10)]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def filter_kargs(target_function,**kargs):\n",
    "    \"\"\"Helper function to filter keyword arguments and only pass the needed ones in a function signature\"\"\"\n",
    "    sig = inspect.signature(target_function)\n",
    "    # check if there is a **kargs in the signature of the function, if yes it is ok as it will take care of the passed extra kargs\n",
    "    if not any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values()):\n",
    "        extra_args = kargs.keys() - sig.parameters.keys()\n",
    "        for args in extra_args:\n",
    "            del kargs[args]\n",
    "    return kargs\n",
    "\n",
    "def debuf_karg_filter(target_function, **kargs):\n",
    "    print(\"Kargs filtering:\")\n",
    "    print(\"before: {\",*[\"{}:{}\".format(key,val) for key, val in kargs.items()],\"}\")\n",
    "    print(\"after: {\",\",\".join([\"{}:{}\".format(key,val) for key, val in filter_kargs(target_function,**kargs).items()]),\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filter_kargs(pd.read_csv, **{\"sep\":\";\", \"truc\":\"test\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset\n",
    "\n",
    "Data are actually described within the ontology, here thanks to the *Data* class.<br>\n",
    "Adding new data points calls for creating new *Data* individuals (i.e., instances in the ontology). (see demos below, section datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(center, dip, dir, length= 1, ax= None, color = \"black\", **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "\n",
    "    center = np.array(center)\n",
    "    dip_rad = np.deg2rad(dip)\n",
    "    vec_x =  np.cos(dip_rad)\n",
    "    if dir == \"left\": vec_x *= -1\n",
    "    vec_z = -np.sin(dip_rad)\n",
    "    vect = 0.5 * length * np.array([vec_x,vec_z])\n",
    "    start = center - vect\n",
    "    end = center + vect\n",
    "    ax_plt.plot([start[0],end[0]],[start[1],end[1]], color = color, **kargs)\n",
    "    \n",
    "    return vect\n",
    "    \n",
    "def draw_dip_symbol(center, dip, dir, length= 1, polarity= None, ax= None, color = \"black\", polarity_ratio= 0.4, **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "    \n",
    "    vect = draw_line(center= center, dip= dip, dir= dir, length= length, ax= ax_plt, color = color, **kargs)\n",
    "    \n",
    "    if polarity is not None:\n",
    "        vect_pol = polarity_ratio * np.array([-vect[1],vect[0]])\n",
    "        if (dir == \"left\" and polarity == \"up\") or (dir == \"right\" and polarity == \"down\") : vect_pol *= -1\n",
    "        ax_plt.arrow(*center,*vect_pol, width=length/100, color = color, **kargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class RenderingObject(object):\n",
    "    \"\"\"Class dedicated to rendering a `RepresentationSpace`\"\"\"\n",
    "    registered_drawing_methods = {}\n",
    "    \n",
    "    def __init__(self, space:RepresentationSpace):\n",
    "        \"\"\"Creating a rendering for the given `RepresentationSpace`\"\"\"\n",
    "        self.space = space\n",
    "\n",
    "    @classmethod\n",
    "    def register_drawing_method(cls, object_class_name, drawing_method):\n",
    "        \"\"\"Registers a drawing function for the given  `object_class_name`\n",
    "        \"\"\"\n",
    "        if (object_class_name is None):\n",
    "            raise MalissiaBaseError(\"Registering drawing method for an undefined object class.\")\n",
    "        if drawing_method is None: raise MalissiaBaseError(\"Registering  an undefined drawing method for an object class.\")\n",
    "        \n",
    "        cls.registered_drawing_methods[object_class_name] = drawing_method\n",
    "        \n",
    "    @classmethod\n",
    "    def get_drawing_method(cls, object_class):\n",
    "        \"\"\"Accessor to the drawing method for a given object\n",
    "        \n",
    "        Parameters:\n",
    "        - object_class_name (str): is the name of the class of the object to be drawned\n",
    "        \"\"\"\n",
    "        if (object_class is None):\n",
    "            raise MalissiaBaseError(\"Drawing methods are registered by object type. None was given, please give in a valid class.\")\n",
    "        if object_class not in cls.registered_drawing_methods:\n",
    "            raise MalissiaBaseError(\"No drawing method registered for this class: \"+ object_class)\n",
    "        return cls.registered_drawing_methods[object_class]\n",
    "        \n",
    "    def setup_ax(self, ax= None):\n",
    "            if ax is not None:\n",
    "                self.plt_ax = ax\n",
    "            elif self.plt_ax is None:\n",
    "                self.plt_ax = plt\n",
    "                \n",
    "    def draw_interpreted_objects(self, ax= None, setup_drawing= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        for object_i in self.space.get_interpreted_objects():\n",
    "            object_class_name = type(object_i).name\n",
    "            drawing_method = self.get_drawing_method(object_class_name)\n",
    "            drawing_method(object_i, representation_space= self, ax= ax, setup_drawing= False, **kargs)\n",
    "                \n",
    "class AxisAlignedCrossSection(RenderingObject):\n",
    "    \"\"\"A specialised `Rendering` that procudes a cross-section.\n",
    "    \n",
    "    The cross-section is defined by two coordinates of the rendered `PhysicalRepresentationSpace`\"\"\"\n",
    "    registered_drawing_methods = {}\n",
    "    \n",
    "    def __init__(self, space:PhysicalRepresentationSpace, u= None, v= None, ax= None):\n",
    "        \"\"\"Creates a cross section through the given physical space\n",
    "        \n",
    "        Parameters:\n",
    "        - space: the space through which the cross section is going.\n",
    "        Note: the given space must be a `PhysicalRepresentationSpace` and have two or more coordinates\n",
    "        - u: the label of the abscissa axis among the physical space coordinates.\n",
    "        By default, if None is given, the first axis of the space is used.\n",
    "        - v: the label of the ordinate axis among the physical space coordinates.\n",
    "        By default, if None is given, the last axis of the space is used,\n",
    "        effectively using both coordinates if the space is 2D, but a vertical cross-section if it is 3D.\n",
    "        - ax: the matplotlib axis in which the space is to be rendered.\n",
    "        If None (default), then a new axis will be created.\"\"\"\n",
    "        assert isinstance(space,PhysicalRepresentationSpace), \"The given representation space must be a PhysicalRepresentationSpace, here: \"+str(type(space))\n",
    "        assert space.dimension > 1, \"Cross sections are only possible through spaces of dimensions >=2, here: \"+str(space.dimension)\n",
    "        self.space = space\n",
    "        \n",
    "        if u is None:\n",
    "            self.u = self.space.coordinate_labels[0]\n",
    "            self.u_index = 0\n",
    "        else:\n",
    "            assert u in self.space.coordinate_labels, \"The first given coordinate (u={}) is not in the represented space ({})\".format(u, \",\".join(space.coordinate_labels))\n",
    "            self.u = u\n",
    "            self.u_index = np.argwhere(self.space.coordinate_labels == self.u)[0,0]\n",
    "        \n",
    "        if v is None:\n",
    "            self.v = space.coordinate_labels[-1]\n",
    "            self.v_index = len(space.coordinate_labels) - 1\n",
    "        else:\n",
    "            assert v in space.coordinate_labels, \"The second given coordinate (v={}) is not in the represented space ({})\".format(v, \",\".join(space.coordinate_labels))\n",
    "            self.v = v\n",
    "            self.v_index = np.argwhere(self.space.coordinate_labels == self.v)[0,0]\n",
    "            \n",
    "        self.plt_ax = None\n",
    "                \n",
    "    def setup_drawing(self, ax= None):\n",
    "            self.setup_ax(ax)\n",
    "            \n",
    "            ax = self.plt_ax.gca() if self.plt_ax == plt else self.plt_ax\n",
    "            ax.set_aspect(\"equal\")\n",
    "            ax.set_xlim( *self.space.extension[self.u_index])\n",
    "            ax.set_xlabel(self.u)\n",
    "            ax.set_ylim( *self.space.extension[self.v_index])\n",
    "            ax.set_ylabel(self.v)\n",
    "    \n",
    "    def draw_line(self, center, dip, dir, length= 1, color = \"black\", setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        center = np.array(center)\n",
    "        dip_rad = np.deg2rad(dip)\n",
    "        vec_x =  np.cos(dip_rad)\n",
    "        if dir == \"left\": vec_x *= -1\n",
    "        vec_z = -np.sin(dip_rad)\n",
    "        vect = 0.5 * length * np.array([vec_x,vec_z])\n",
    "        start = center - vect\n",
    "        end = center + vect\n",
    "        self.plt_ax.plot([start[0],end[0]],[start[1],end[1]], color = color, **kargs)\n",
    "        \n",
    "        return vect        \n",
    "    \n",
    "    def draw_dip_symbol(self, di, length= None, polarity= None, color = \"black\", polarity_ratio= 0.4, setup_drawing= True, ax= None, zorder= 20, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "\n",
    "        dip = di.dip[0]\n",
    "        dir = \"right\" if di.dip_dir[0] < 180 else \"left\"\n",
    "        center = self.get_center_coordinates(di)\n",
    "        \n",
    "        length = 0.5 * di.size[0] if di.size is not None else length\n",
    "        if length is None or length is np.nan:\n",
    "            length = np.abs(np.max(self.space.extension[:,1] - self.space.extension[:,0])) / 20\n",
    "        \n",
    "        vect = self.draw_line(center= center, dip= dip, dir= dir, length= length, color = color, setup_drawing=False, ax=ax, zorder= zorder, **kargs)\n",
    "        \n",
    "        if di.polarity is not None:\n",
    "            polarity = \"up\" if di.polarity[0] else \"down\"\n",
    "        if polarity is not None:\n",
    "            vect_pol = polarity_ratio * np.array([-vect[1],vect[0]])\n",
    "            if (dir == \"left\" and polarity == \"up\") or (dir == \"right\" and polarity == \"down\") : vect_pol *= -1\n",
    "            self.plt_ax.arrow(*center,*vect_pol, width=length/100, color = color,  zorder= zorder, **kargs)\n",
    "        \n",
    "    def filter_section_coordinates(self, coord):\n",
    "        return coord[[self.u_index,self.v_index]]\n",
    "        \n",
    "    def get_center_coordinates(self,di):\n",
    "        center = di.has_Center[0]\n",
    "        coord = self.space.get_object_coordinates(center)\n",
    "        return self.filter_section_coordinates(coord)\n",
    "    \n",
    "    def get_corner_coordinates(self, surf):\n",
    "        corners = surf.has_Representation\n",
    "        coord = np.array([self.space.get_object_coordinates(corner_i) for corner_i in corners]).T\n",
    "        return self.filter_section_coordinates(coord)\n",
    "    \n",
    "    def draw_dip_data(self, ax= None, polarity= \"up\", setup_drawing= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        for dataset_i in self.space.get_datasets():\n",
    "            for di in dataset_i.get_orientation_observations():\n",
    "                self.draw_dip_symbol(di, setup_drawing= False, ax=ax, **kargs) \n",
    "    \n",
    "    def draw_point(self, center, color = \"black\", marker=\"*\", edge_color= \"black\", zorder= 10, setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        center = self.get_center_coordinates(center)\n",
    "        self.plt_ax.scatter(*center, color = color, marker= marker, edgecolors= edge_color, zorder=zorder, **kargs)\n",
    "        \n",
    "    def draw_plane(self, plane, color = \"limegreen\", marker=\".\", edge_color= \"black\", zorder= 2, setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        corner_coords = self.get_corner_coordinates(plane)\n",
    "        corner_coords = np.append(corner_coords,corner_coords[:,0].reshape((2,1)),axis=1)\n",
    "        self.plt_ax.scatter(*corner_coords, color = color, marker= marker, edgecolors= edge_color, zorder=zorder, **kargs)\n",
    "        self.plt_ax.plot(*corner_coords, color = color, marker= None, zorder=zorder-1, **kargs)\n",
    "    \n",
    "    def draw_occurrence_symbol(self, d, color = \"lightblue\", marker=\"*\", edge_color= \"black\", zorder= 10, setup_drawing= True, ax= None, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        self.draw_point( d, color, marker, edge_color= edge_color, zorder=zorder, setup_drawing= False, ax=ax, **kargs) \n",
    "        \n",
    "    def draw_occurrence_data(self, ax= None, setup_drawing= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        \n",
    "        for dataset_i in self.space.get_datasets():\n",
    "            for di in dataset_i.get_occurrence_observations():\n",
    "                self.draw_occurrence_symbol(di, setup_drawing= False, ax=ax, **kargs) \n",
    "        \n",
    "    def show(self, ax= None, setup_drawing= True, **kargs):\n",
    "        if setup_drawing: self.setup_drawing(ax)\n",
    "        self.draw_occurrence_data(ax=ax, setup_drawing= False, **kargs)\n",
    "        self.draw_dip_data(ax=ax, setup_drawing= False, **kargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_observation_in_AxisAlignedCrossSection(\n",
    "        observation,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "        \n",
    "    if observation.occurrence is not None:\n",
    "        representation_space.draw_occurrence_symbol(observation, ax= ax, setup_drawing= False, **kargs)\n",
    "    if observation.dip is not None:\n",
    "        representation_space.draw_dip_symbol(observation, ax= ax, setup_drawing= False, **kargs)\n",
    "        \n",
    "if mogi().PointBased_Observation in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().PointBased_Observation]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().PointBased_Observation, draw_observation_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_surface_part_in_AxisAlignedCrossSection(\n",
    "        strati_part,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "        \n",
    "    plane = strati_part.has_Representation[0]\n",
    "    representation_space.draw_plane(plane, **kargs)\n",
    "    \n",
    "        \n",
    "if mogi().Stratigraphic_Part in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[mogi().Stratigraphic_Part]\n",
    "AxisAlignedCrossSection.register_drawing_method(mogi().Stratigraphic_Part, draw_surface_part_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_situation_in_AxisAlignedCrossSection(\n",
    "        situation,\n",
    "        representation_space,\n",
    "        ax= None,\n",
    "        setup_drawing= True,\n",
    "        **kargs\n",
    "    ):\n",
    "    if setup_drawing:\n",
    "        representation_space.setup_drawing(ax)\n",
    "    else:\n",
    "        representation_space.setup_ax(ax)\n",
    "        \n",
    "    for feature_i in situation.features:\n",
    "        drawing_function = representation_space.get_drawing_method(feature_i.is_instance_of[0])\n",
    "        drawing_function(feature_i, representation_space, color= \"deepskyblue\", ax = ax, setup_drawing= False, **kargs)\n",
    "        \n",
    "if InterpretationSituation in AxisAlignedCrossSection.registered_drawing_methods:\n",
    "    del AxisAlignedCrossSection.registered_drawing_methods[InterpretationSituation]\n",
    "AxisAlignedCrossSection.register_drawing_method(InterpretationSituation, draw_situation_in_AxisAlignedCrossSection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tasks in the algorithm of interpretation are not directly specified in the interpretation process but are made abstract to make it easier to implement alternative ways to proceed.\n",
    "\n",
    "Our implementation separates two aspects of the problem:\n",
    "1. **Task**: providing an interface for describing the task to be achieved\n",
    "2. **Strategy**: providing the implementation for achieving the task.\n",
    "\n",
    "Both are represented by a separate interface; only derived classes of Task that have a defined strategy are actually applicable.\n",
    "\n",
    "There is a weak distinction between two kind of tasks:\n",
    "- Strategies: for implementing parts of the overall algorithm that are fully dependent on the user's choice. In other terms, \n",
    "there is no obvious or ultimately best strategy for achieving a task, just actions that can be carried out in different ways.\n",
    "- Heuristics: for providing quick and/or easy approximate solutions to a given problem. Here, there is in theory a correct answer\n",
    "but it might be to difficult or expansive to infer it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from enum import Enum, Flag, auto\n",
    "import random, string\n",
    "    \n",
    "class MissingTaskImplementationError(MalissiaBaseError):\n",
    "    \"\"\"Exception generated when trying to run a task for which no implementation was selected.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__ (\"This error occurred because a task has been executed without a proper implementation defined by a strategy.\\n\"\\\n",
    "            \"Make sure to use fully implemented Task class (ie. not the base ones) and that self.strategy is set\")\n",
    "        \n",
    "\n",
    "class StrategyType(Flag):\n",
    "    \"\"\"Defines flags for specifying behaviour of the Strategy.\n",
    "    \n",
    "    DEFAULT: no specific behaviour\n",
    "    USER: it will ask user for some input\n",
    "    RANDOM: it sill set some results in a random way\n",
    "    BRUTE_FORCE: it will try all options and select the best result\"\"\"\n",
    "    DEFAULT = 0\n",
    "    USER = auto()\n",
    "    RANDOM = auto()\n",
    "    BRUTE_FORCE = auto()\n",
    "\n",
    "class Strategy(object):\n",
    "    \"\"\"Strategies are pieces of algorithm that are implementing a specific task.\n",
    "    \n",
    "    Strategy provides the generic interface of algorithms implementing a `Task`\n",
    "    and determines which tasks are actually concrete. In other words, a `Task` that does not inherit from `Strategy`\n",
    "    can not be executed.\n",
    "    \n",
    "    `Strategy` provides the following interface (which has to be implemented in inheriting classes):\n",
    "    - class attributes:\n",
    "      - name: the human-readable name of the strategy\n",
    "      - short_desc: a one-line description of the strategy\n",
    "      - full_desc: a complete description of the strategy\n",
    "    - attributes initialised by __init__:\n",
    "      - strategy_type: the type of strategy (cf. `StrategyType`)\n",
    "    - methods:\n",
    "      - check_applicability(): which checks if the algorithm can be applied in the context defined by the task\n",
    "      - execute(): which effectively runs the algorithm\n",
    "    \n",
    "    Strategies differ from Heuristics, in that the implemented task does not have an obvious/natural/true result or way to proceed,\n",
    "    but it rather a matter of choice, hence a strategy.\"\"\"\n",
    "    \n",
    "    name = \"Strategy\"\n",
    "    short_desc = \"Generic Abstract Strategy\"\n",
    "    full_desc = \"This is not supposed to be implemented.\"\n",
    "    strategy_type= StrategyType.DEFAULT\n",
    "    \n",
    "    def __init__(self, type= None, **kargs):\n",
    "        \"\"\"Initialises common attributes for the `Strategy` class\n",
    "        \n",
    "        Parameters:\n",
    "        - type (`StrategyType`): the type of strategy implement, mostly to specify if user defined or random\"\"\"\n",
    "        if type is not None:\n",
    "            self.strategy_type = type\n",
    "        \n",
    "    def check_applicability(task = None):\n",
    "        \"\"\"Checks if this algorithm can be applied in the current context\n",
    "        \n",
    "        This base class method actually triggers an Exception to make sure the methods is implemented in inheriting class\"\"\"\n",
    "        raise MalissiaBaseError(\"Trying to check applicability of the {} strategy but this class doesn't implement \"\\\n",
    "            \"the check_applicability method or run Strategy.check_applicability() instead.\".format(__class__.__name__))\n",
    "        \n",
    "    def execute(self, task):\n",
    "        \"\"\"runs the algorithm\n",
    "        \n",
    "        This base class method actually triggers an Exception to make sure the methods is implemented in inheriting class.\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        raise MalissiaBaseError(\"Trying to execute the {} strategy but this class doesn't implement the execute method \"\\\n",
    "            \"or run Strategy.execute() instead.\".format(self.__class__.__name__))\n",
    "\n",
    "class UserInputStrategy(Strategy):\n",
    "    \"\"\"Strategy based on interaction with the user.\"\"\"\n",
    "    name = \"UserInputStrategy\"\n",
    "    short_desc = \"Generic User Strategy\"\n",
    "    full_desc = \"This is implementing generic user interaction.\"\n",
    "    strategy_type= StrategyType.USER\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        \"\"\"Initialises a User based strategy\"\"\"\n",
    "        super().__init__(**kargs)\n",
    "        \n",
    "    def check_applicability(task):\n",
    "        \"\"\"Checks if user interaction is possible (based on sys.ps1)\n",
    "        \n",
    "        This kind of algorithm is also restricted to result tasks because actions would be too complex to ask to users.\n",
    "        \"\"\"\n",
    "        applicable = isinstance(task, ResultTask) \n",
    "        applicable &= UserInputStrategy.check_user_interface()\n",
    "        return applicable\n",
    "    \n",
    "    def check_user_interface():\n",
    "        return hasattr(sys, \"ps1\")\n",
    "    \n",
    "    def execute(self, task):\n",
    "        \"\"\"Asks user for the result to give.\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        task.result = task.result_type(input(\"Please give in an input that can be used as a '{}':\".format(task.result_type.__name__)))\n",
    "    \n",
    "class RandomStrategy(Strategy):\n",
    "    \"\"\"Strategy based on random generator.\"\"\"\n",
    "    name = \"RandomStrategy\"\n",
    "    short_desc = \"Generic Random Strategy\"\n",
    "    full_desc = \"This is implementing generic random generator.\"\n",
    "    strategy_type= StrategyType.RANDOM\n",
    "    \n",
    "    supported_types = (str, float, int)\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        \"\"\"Initialises a Random based strategy\"\"\"\n",
    "        super().__init__(**kargs)\n",
    "        \n",
    "    def check_applicability(task):\n",
    "        \"\"\"Checks if applicable\n",
    "        \n",
    "        i.e., for a ResultTask and result_type being in supported_types\n",
    "        \"\"\"\n",
    "        applicable = isinstance(task, ResultTask)\n",
    "        applicable &= task.result_type in __class__.supported_types\n",
    "        return applicable\n",
    "    \n",
    "    def execute(self, task):\n",
    "        \"\"\"apply the strategy.\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        if task.result_type == str:\n",
    "            task.result = \"\".join(random.choices(string.ascii_lowercase, k=10))\n",
    "        elif task.result_type == float:\n",
    "            task.result = random.random()\n",
    "        elif task.result_type == int:\n",
    "            task.result = random.randint(0,10)\n",
    "        else:\n",
    "            raise MalissiaBaseError(\"Unsupported type for RandomStrategy. Request: {}, Available: {}.\".format(task.result_type,\", \".join(__class__.supported_types)))\n",
    "        \n",
    "class Task(object):\n",
    "    \"\"\"Task is an abstract base class to provide the generic interface for any tasks of the `GeologicalInterpretationProcess`\n",
    "    \n",
    "    Task provides the following interface (which has to be implemented in inheriting classes):\n",
    "    - class attributes:\n",
    "      - name: the human-readable name of the task\n",
    "      - short_desc: a one-line description of the task\n",
    "      - full_desc: a complete description of the task\n",
    "      - available_strategies: a list of available strategy types for implementing the `Task`.\n",
    "        NB: strategies are related to `Task` by composition to make it possible to derive the tasks and still inherit from the base class strategies.\n",
    "    - attributes initialised by __init__:\n",
    "      - context: a context in which the task must be executed, typically the `GeologicalInterpretationProcess` instance\n",
    "      - strategy: the selected strategy instance for implementing the task if any.\n",
    "    \"\"\"\n",
    "    \n",
    "    name = \"Task\"\n",
    "    short_desc = \"Generic Abstract Task\"\n",
    "    full_desc = \"This is not supposed to be implemented.\"\n",
    "    available_strategies = set()\n",
    "    \n",
    "    def __init__(self, context, **kargs):\n",
    "        \"\"\"Initialises common attributes for the `Task` class\n",
    "        \n",
    "        Parameters:\n",
    "        - task_name: the human-readable name of the task\n",
    "        - task_short_desc: a one-line description of the task\n",
    "        - task_full_desc: a complete description of the task\n",
    "        - context: the context of the task, this should provide the adapted interface for giving the required information\n",
    "        \"\"\"\n",
    "        self.context = context\n",
    "\n",
    "    def set_strategy(self, strategy):\n",
    "        \"\"\"Setter for the strategy, this ensures everything is set properly\"\"\"\n",
    "        assert not ( (strategy is not None) and (not isinstance(strategy, Strategy))), \"strategy must be either None or an instance of a Strategy (not a class)\"\n",
    "        self.strategy = strategy\n",
    "        \n",
    "    def check_applicability(self):\n",
    "        \"\"\"Checks that the task can be executed (ie. a strategy is set and applicable)\"\"\"\n",
    "        if self.strategy is None: return False\n",
    "        return self.strategy.__class__.check_applicability(self)\n",
    "    \n",
    "    def execute(self, bypass_applicability_check = False):\n",
    "        \"\"\"Method to run the selected implementation of the task.\n",
    "        \n",
    "        If no strategy is implemented/selected, this triggers an exception (`MissingTaskImplementationError`).\n",
    "        \n",
    "        parameters:\n",
    "        - bypass_applicability_check (bool): if True, the applicability of the task won't be checked beforehand. Default: False\"\"\"\n",
    "        if (not hasattr(self, \"strategy\")) or (self.strategy is None):\n",
    "            raise MissingTaskImplementationError()\n",
    "        elif (not bypass_applicability_check) and (not self.check_applicability()):\n",
    "            raise MalissiaBaseError(\"executed a Task with a strategy that was not applicable in the current context.\")\n",
    "        else: self.strategy.execute(self)\n",
    "        \n",
    "class ActionTask(Task): \n",
    "    \"\"\"Task in charge of applying some actions.\n",
    "    \n",
    "    Such tasks do not typically store a result but instead interact and possibly modify the context.\n",
    "    \"\"\"\n",
    "    name = \"ActionTask\"\n",
    "    short_desc = \"Generic Abstract Task for actions\"\n",
    "    full_desc = \"This is a generic action, meaning it doesn't hold a result but instead affects the context directly. This is not supposed to be implemented directly.\"\n",
    "    available_strategies = Task.available_strategies\n",
    "    \n",
    "    def __init__(self, context, **kargs):\n",
    "        \"\"\"Initialises an ActionTask\n",
    "        \n",
    "        Parameter:\n",
    "        - context: provides a context with the appropriate (task specific) interface. This parameter is compulsory.\"\"\"\n",
    "        super().__init__(context= context, **kargs)\n",
    "        \n",
    "class ResultTask(Task):\n",
    "    \"\"\"`Task` in charge of creating a result.\n",
    "    \n",
    "    Such tasks should not modify the context but only generate a result stored the `self.result` attribute.\n",
    "    \n",
    "    `UserInputStrategy` and `RandomStrategy` are automaticall added as available strategies\n",
    "    \"\"\"\n",
    "    name = \"ResultTask\"\n",
    "    short_desc = \"Generic Task for generating results\"\n",
    "    full_desc = \"This is a generic result generator, meaning it doesn't affect the context but holds a result instead.\"\n",
    "    available_strategies = Task.available_strategies | set((UserInputStrategy, RandomStrategy))\n",
    "    \n",
    "    def __init__(self, result_type, context = None, **kargs):\n",
    "        \"\"\"Initialises a ResultTask\n",
    "                \n",
    "        Additional parameters (for others see `Task`):\n",
    "        - result_type: the type of expeted result\n",
    "        - context: provides a context with the appropriate (task specific) interface. This parameter is compulsory.\"\"\"\n",
    "        super().__init__(context= context, **kargs)\n",
    "        self.result_type = result_type\n",
    "        self.result = None\n",
    "\n",
    "    def execute(self, bypass_applicability_check = False, return_result= True):\n",
    "        \"\"\"Additional behaviour for `ResultTask`\n",
    "        \n",
    "        Runs the `Task.execute()` then return result if the option is activated (default)\"\"\"\n",
    "        super().execute(bypass_applicability_check= bypass_applicability_check)\n",
    "        if return_result: return self.result\n",
    "        \n",
    "class StrategyFactory(object):\n",
    "    \"\"\"This class gathers all the strategies available for the `GeologicalInterpretationProcess`.\n",
    "    \n",
    "    The strategies are automatically recovered from the subclasses of Task and Strategy.\n",
    "    Three dictionaries are registering the task and corresponding strategies:\n",
    "    - self.tasks: are the Defined Tasks having defined possible Strategies\n",
    "    - self.specified_tasks: are the tasks with a specific available strategy (i.e., only one)\n",
    "    - self.implemented_tasks: tasks that have an implemented strategy\n",
    "    NB: they aren't used for generating the tasks, but just for reporting the existing tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialisation of the strategy storage.\"\"\"\n",
    "        self.tasks = {}\n",
    "        self.specified_tasks = {}\n",
    "        self.implemented_tasks = {}\n",
    "        self.tasks_without_strategy = {}\n",
    "        self.list_available_tasks()\n",
    "        self.list_specified_tasks()\n",
    "        self.list_implemented_tasks()\n",
    "        self.list_tasks_without_strategy()\n",
    "        \n",
    "    def generate_task(self, task_type, strategy_type= None, strategy_class= None, context= None, **kargs):\n",
    "        \"\"\"Creates a task of a given type and instanciate appropriate strategy\n",
    "        \n",
    "        Parameters:\n",
    "        - task_type: the class of the task to be created\n",
    "        - strategy_type: the type of strategy to be used, see `StrategyType`\n",
    "        - strategy_class: to force the strategy selection, its class can be directly given here\n",
    "        - context: a context that can be passed to the task to define it (typically the `GeologicalInterpretationProcess`)\n",
    "        - kargs: keyword arguments that can be passed to the task initialisation\"\"\"\n",
    "        \n",
    "        # Create the task\n",
    "        task = task_type(context = context, **kargs)\n",
    "        \n",
    "        if strategy_class is None:\n",
    "            # list strategies\n",
    "            strategies = np.array(list(task_type.available_strategies))\n",
    "            \n",
    "            # check applicability to the task and context\n",
    "            applicable_strategy = [candidate_strategy for candidate_strategy in strategies if candidate_strategy.check_applicability(task)]\n",
    "            \n",
    "            # filter strategy types\n",
    "            if strategy_type is not None:\n",
    "                applicable_strategy = [candidate_strategy for candidate_strategy in applicable_strategy if candidate_strategy.strategy_type == strategy_type]\n",
    "                \n",
    "            if len(applicable_strategy) == 0:\n",
    "                message = [\"No implementation of the requested task ({}) found in this context.\".format(task_type)]\n",
    "                message += [\"Note that the following strategies were available:\\n\"+\"\\n\".join(strategies)]\n",
    "                message += [\"Note that the following strategies were applicable:\\n\"+\"\\n\".join(applicable_strategy)]\n",
    "                message += [\"Note that the following strategies were filtered out:\\n\"+\"\\n\".join([candidate_strategy for candidate_strategy in applicable_strategy if candidate_strategy.strategy_type != strategy_type])]\n",
    "                raise MalissiaBaseError(\"\\n\".join(message))\n",
    "             \n",
    "            # this could be defined by a strategy as well, e.g. pick first of random\n",
    "            strategy_class = random.choice(applicable_strategy)\n",
    "            \n",
    "        else:\n",
    "            if strategy_class.check_applicability(task) == False:\n",
    "                raise MalissiaBaseError(\"The proposed implementation ({}) of the requested task ({}) is not applicable in this context.\".format(strategy_class,task_type))\n",
    "                \n",
    "        # instanciate the strategy\n",
    "        strategy_instance = strategy_class(**kargs)\n",
    "        task.set_strategy(strategy_instance)\n",
    "            \n",
    "        return task\n",
    "    \n",
    "    def list_available_tasks(self):\n",
    "        \"\"\"List the Tasks that are fully defined (ie. having attached strategies)\"\"\"\n",
    "        task_pile = list(Task.__subclasses__())\n",
    "        while len(task_pile) > 0:\n",
    "            task_i = task_pile.pop()\n",
    "            \n",
    "            strategies = task_i.available_strategies\n",
    "            if len(strategies) > 0:\n",
    "                self.tasks[task_i] = strategies \n",
    "            \n",
    "            sub_tasks = list(task_i.__subclasses__())\n",
    "            task_pile += sub_tasks\n",
    "    \n",
    "    def list_specified_tasks(self):\n",
    "        \"\"\"List the Tasks that have only one possible strategy\"\"\"\n",
    "        task_pile = list(Task.__subclasses__())\n",
    "        while len(task_pile) > 0:\n",
    "            task_i = task_pile.pop()\n",
    "            \n",
    "            strategies = task_i.available_strategies\n",
    "            if len(strategies) == 1:\n",
    "                self.specified_tasks[task_i] = list(strategies)[0]\n",
    "            \n",
    "            sub_tasks = list(task_i.__subclasses__())\n",
    "            task_pile += sub_tasks\n",
    "    \n",
    "    def list_implemented_tasks(self):\n",
    "        \"\"\"List the Tasks that are implemented (ie. having an attached strategy instance)\"\"\"\n",
    "        task_pile = list(Task.__subclasses__())\n",
    "        while len(task_pile) > 0:\n",
    "            task_i = task_pile.pop()\n",
    "            \n",
    "            if hasattr(task_i,\"strategy\") and task_i.strategy is not None:\n",
    "                self.implemented_tasks[task_i] = task_i.strategy \n",
    "            \n",
    "            sub_tasks = list(task_i.__subclasses__())\n",
    "            task_pile += sub_tasks\n",
    "            \n",
    "    def list_tasks_without_strategy(self):\n",
    "        \"\"\"Lists the tasks that have no strategies nor subclass\"\"\"\n",
    "        task_pile = list(Task.__subclasses__())\n",
    "        while len(task_pile) > 0:\n",
    "            task_i = task_pile.pop()\n",
    "            \n",
    "            strategies = task_i.available_strategies\n",
    "            if (len(strategies) == 0) and (len(task_i.__subclasses__()) == 0):\n",
    "                self.tasks_without_strategy[task_i] = ()\n",
    "            \n",
    "            sub_tasks = list(task_i.__subclasses__())\n",
    "            task_pile += sub_tasks\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Describe the stored strategies\"\"\"\n",
    "        self.list_available_tasks()\n",
    "        self.list_specified_tasks()\n",
    "        self.list_implemented_tasks()\n",
    "        self.list_tasks_without_strategy()\n",
    "        \n",
    "        desc = [\"Strategy Factory:\"]\n",
    "        desc+= [\"- Available Tasks:\"]\n",
    "        for task, strat_list in self.tasks.items():\n",
    "            desc += [\"  |- {}:\".format(task.name)]\n",
    "            if len(strat_list) == 0:\n",
    "                desc += [\"  |[Warning]: no strategy provided for this task\"] \n",
    "            for strat_i in strat_list:\n",
    "                desc += [\"  |  |- {}{}: {}\".format(strat_i.__class__.__name__, \"\" if strat_i.strategy_type is StrategyType.DEFAULT else \"[\"+strat_i.strategy_type.name+\"]\", strat_i.short_desc)]\n",
    "        \n",
    "        if len(self.specified_tasks) > 0:\n",
    "            desc+= [\"- Specified Tasks:\"]\n",
    "            for task, strat in self.specified_tasks.items():\n",
    "                desc += [\"  |- {}: {}{}({})\".format(task.name, strat.name, \"\" if strat.strategy_type is StrategyType.DEFAULT else \"[\"+strat.strategy_type.name+\"]\", strat.short_desc)]\n",
    "        if len(self.implemented_tasks) > 0:\n",
    "            desc+= [\"- Implemented Tasks:\"]\n",
    "            for task, strat in self.implemented_tasks.items():\n",
    "                desc += [\"  |- {}: {}{}({})\".format(task.name, strat.name, \"\" if strat.strategy_type is StrategyType.DEFAULT else \"[\"+strat.strategy_type.name+\"]\", strat.short_desc)]\n",
    "        if len(self.tasks_without_strategy) > 0:\n",
    "            desc+= [\"- Tasks without strategy:\"]\n",
    "            for task, strat in self.tasks_without_strategy.items():\n",
    "                desc += [\"  |- {}: [Warning]: no strategy provided for this task\".format(task.name)]\n",
    "        return \"\\n\".join(desc)\n",
    "        \n",
    "    \n",
    "__default_strategy_factory__ = StrategyFactory()\n",
    "    \n",
    "class HeuristicsFactory(object):\n",
    "    \"\"\"This class provides Heuristics for the `GeologicalInterpretationProcess`.\n",
    "    \n",
    "    Heuristics are pieces of algorithm that are implementing a specific computational or decisionnal task.\n",
    "    They differ from strategies, in that there exists a true result even if it is not know or two complex to infer accurately.\n",
    "    Heuristics try to provide acceptable approximate results with a simple algorithms to save either computation time, memory, or complexity.\n",
    "\n",
    "    The heuristics are stored in a dictionnary (`self.__heuristics`) that associates tasks with a list of possible implementation. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises the Heuristics storage.\"\"\"\n",
    "        self.__heuristics = {}\n",
    "        \n",
    "# print(__default_strategy_factory__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleTask(ResultTask):\n",
    "    \"\"\"class in charge of showing how to implement new tasks\n",
    "    \n",
    "    Such task is independent of a context and ignores it if passed.\"\"\"\n",
    "    \n",
    "    name = \"ExampleTask\"\n",
    "    short_desc = \"gives an example\"\n",
    "    full_desc = \"Demonstrates the implementation of new tasks.\"\n",
    "    def __init__(self, result_type= None, **kargs):\n",
    "        \"\"\"example of initialisation of a result task\n",
    "        \n",
    "        Parameters:\n",
    "        - result_type: the type of expeted result. If None, one of the RandomStrategy supported type is used.\"\"\"\n",
    "        result_type = result_type if result_type is not None else random.choice(RandomStrategy.supported_types)\n",
    "        super().__init__(result_type= result_type)\n",
    "        \n",
    "class UserExampleTask(ExampleTask):\n",
    "\n",
    "    name = \"UserExampleTask\"\n",
    "    short_desc = \"example user strategy\"\n",
    "    full_desc = \"Gives an example of implementation of user defined algorithm.\"\n",
    "    available_strategies = {UserInputStrategy}\n",
    "    \n",
    "    def __init__(self, result_type= str, **kargs):\n",
    "        super().__init__(result_type= result_type)\n",
    "        strategy = UserInputStrategy()\n",
    "        super().set_strategy(strategy)\n",
    "    \n",
    "class RandomExampleTask(ExampleTask):\n",
    "\n",
    "    name = \"RandomExampleTask\"\n",
    "    short_desc = \"example random strategy\"\n",
    "    full_desc = \"Gives an example of implementation of random defined algorithm.\"\n",
    "    available_strategies = {RandomStrategy}\n",
    "    \n",
    "    def __init__(self, result_type= str, **kargs):\n",
    "        super().__init__(result_type= result_type)\n",
    "        strategy = RandomStrategy()\n",
    "        super().set_strategy(strategy)\n",
    "        \n",
    "# print(__default_strategy_factory__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Tasks implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserSelectStrategy(UserInputStrategy):\n",
    "    \"\"\"Strategy based on interaction with the user.\"\"\"\n",
    "    name = \"UserSelectStrategy\"\n",
    "    short_desc = \"Selection User Strategy\"\n",
    "    full_desc = \"This is implementing selection based on user interaction.\"\n",
    "    type= StrategyType.USER\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        \"\"\"Initialises a User based strategy\"\"\"\n",
    "        super().__init__(**kargs)\n",
    "        \n",
    "    def check_applicability(task, context = None):\n",
    "        \"\"\"Checks if applicable\n",
    "        \n",
    "        uses UserInputStrategy.check_applicability and SelectionTask type\n",
    "        \"\"\"\n",
    "        applicable = isinstance(task, SelectionTask) \n",
    "        applicable &= UserInputStrategy.check_user_interface()\n",
    "        return applicable\n",
    "    \n",
    "    def execute(self, task):\n",
    "        \"\"\"Asks user for the result to give.\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        message = \"Please select an option amongst the following (give its index):\\n \"+\"\\n\".join([\"{}: {}\".format(i,val) for i, val in enumerate(task.choices)])\n",
    "        try:\n",
    "            selected = int(input(message))\n",
    "            task.result = task.result_type(task.choices[selected])\n",
    "        except ValueError:\n",
    "            raise MalissiaBaseError(\"User input must be in the form of an integer corresponding to the selected index.\")\n",
    "        except IndexError:\n",
    "            raise MalissiaBaseError(\"The selected index must be within [{}, {}].\".format(0,len(task.choices)-1))\n",
    "        return task.result\n",
    "    \n",
    "class RandomSelectStrategy(RandomStrategy):\n",
    "    \"\"\"Strategy based on random generator.\"\"\"\n",
    "    name = \"RandomSelectStrategy\"\n",
    "    short_desc = \"Random Selection Strategy\"\n",
    "    full_desc = \"This is implementing a random selection.\"\n",
    "    type= StrategyType.RANDOM\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        \"\"\"Initialises a Random based strategy\"\"\"\n",
    "        super().__init__(**kargs)\n",
    "        \n",
    "    def check_applicability(task, context = None):\n",
    "        \"\"\"Checks if applicable\n",
    "        \n",
    "        Needs to be set for a SelectionTask with at least one choice\n",
    "        \"\"\"\n",
    "        applicable = isinstance(task, SelectionTask)\n",
    "        applicable &= len(task.choices) >0\n",
    "        return applicable\n",
    "    \n",
    "    def execute(self, task):\n",
    "        \"\"\"perform random selection\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        \n",
    "        if len(task.n) == 1:\n",
    "            if task.n[0] == 1:\n",
    "                task.result = random.choice(task.choices)\n",
    "                return task.result_type(task.result)\n",
    "            else:\n",
    "                k = task.n\n",
    "        else:\n",
    "            k = np.cumprod(task.n)[-1]\n",
    "            \n",
    "        task.result = random.choices(task.choices, k = k)\n",
    "        if len(task.n) > 1:\n",
    "            task.result.reshape(task.n)\n",
    "            \n",
    "        return task.result.astype(task.result_type)\n",
    "\n",
    "class SelectionTask(ResultTask):\n",
    "    \"\"\"SelectionTask performs a selection among a series of possible results.\"\"\"\n",
    "    \n",
    "    name = \"SelectionTask\"\n",
    "    short_desc = \"makes a selection\"\n",
    "    full_desc = \"Performs a selection among a series of possible results.\"\n",
    "    available_strategies = set((UserSelectStrategy, RandomSelectStrategy))\n",
    "    \n",
    "    def __init__(self, choices, n= 1, result_type= None, context= None, **kargs):\n",
    "        \"\"\"Initialisation of a selection task\n",
    "        \n",
    "        Parameters:\n",
    "        - choices: a series of possible values for the result. Will be transformed into a numpy array and flatten (np.array(choices).ravel())\n",
    "        - n (int): number of elements to be picked, alternatively if a shape is given, the shape of the expected result array\n",
    "        - result_type: the type of expeted result. If None, the type of the choices will be used by default.\"\"\"\n",
    "        self.choices = np.array(choices).ravel()\n",
    "        self.n = np.array(n).ravel()\n",
    "        result_type = result_type if result_type is not None else self.choices.dtype.type\n",
    "        super().__init__(result_type= result_type)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectObservation(SelectionTask):\n",
    "    \"\"\"Selection of an observation\"\"\"\n",
    "    name = \"SelectObservation\"\n",
    "    short_desc = \"selects observations\"\n",
    "    full_desc = \"Performs a selection among possible observations.\"\n",
    "    \n",
    "    def __init__(self, n= 1, choices = None, dataset= None, context= None, **kargs):\n",
    "        \"\"\"Initialisation of a selection task\n",
    "        \n",
    "        Parameters:\n",
    "        - choices: a series of possible observations. If None (default), the dataset in the context will be used (and therefore can't be None)\n",
    "        - dataset: a GeologicalDataset to bring the available observations unless already defined by choices \n",
    "        - context: if given a `GeologicalInterpretationProcess`, will be used to define the possible selection unless, choices or dataset are given.\n",
    "        - n (int): number of elements to be picked, alternatively if a shape is given, the shape of the expected result array\"\"\"\n",
    "        choices = choices if choices is not None else self.get_choices(dataset= dataset, context= context, **kargs)\n",
    "        super().__init__(choices = choices, n= n, context = context, **kargs)\n",
    "\n",
    "    def get_choices(self, choices = None, dataset:GeologicalDataset= None, context= None, **kargs):\n",
    "        \"\"\"generates the choices list from different possible sources\n",
    "        \n",
    "        Parameters:\n",
    "        - choices: a series of possible observations. If None (default), the dataset in the context will be used (and therefore can't be None)\n",
    "        - dataset: a GeologicalDataset to bring the available observations unless already defined by choices \n",
    "        - context: if given a `GeologicalInterpretationProcess`, will be used to define the possible selection unless, choices or dataset are given.\n",
    "        \"\"\"\n",
    "        if (context is None) and (dataset is None):\n",
    "            raise MalissiaBaseError(\"Either a series of choices, a dataset, or a context must be given.\")\n",
    "        if dataset is None:\n",
    "            dataset = context.dataset\n",
    "        choices = dataset.get_observations()\n",
    "        return choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectObservation.available_strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation process in itself is run in a **GeologicalInterpretationProcess** and follow a very simple and generic algorithm.<br>\n",
    "This algorithm implements a Deming wheel process of continual improvement:\n",
    "1. Plan:\n",
    "    1. Select a situation\n",
    "    2. Select an action\n",
    "2. Do: Implement the action (e.g., CreateInterpretationElement)\n",
    "    1. List features\n",
    "    2. Identify possible explanations\n",
    "    3. Rank/chose explanations\n",
    "    4. Instanciate individuals\n",
    "    5. Infer and set parameters\n",
    "3. Check: Evaluate consistency\n",
    "    1. Evaluate internal consistency\n",
    "    2. Evaluate relational likelihood\n",
    "    3. Evaluate feature explanation\n",
    "4. Act: Generate anomalies and report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from enum import Enum, Flag, auto\n",
    "class TerminationFlag(Flag):\n",
    "    \"\"\"Defines flags for specifying why the `GeologicalInterpretationProcess` stoped\n",
    "    \n",
    "    DEFAULT: it has not been terminated\n",
    "    USER: was terminated by user\n",
    "    MAX_ITER: maximum iteration number reached\n",
    "    \"\"\"\n",
    "    DEFAULT = 0\n",
    "    USER = auto()\n",
    "    MAX_ITER = auto()\n",
    "   \n",
    "\n",
    "def select_user_unexplained_feature(process):\n",
    "   \"\"\"user selects a single feature to be explained\"\"\"\n",
    "   observations = process.dataset.get_unexplained_observations()\n",
    "   message = \"; \".join([\"{}: {}\".format(i,obs_i.name) for i, obs_i in enumerate(observations)])\n",
    "   selected_feature_index = int(input(message))\n",
    "   selected_feature = observations[selected_feature_index]\n",
    "   process.situation = InterpretationSituation( features= [selected_feature], process= process)\n",
    "   \n",
    "def select_single_random_feature(process):\n",
    "   \"\"\"randomly selects a single feature to be explained\"\"\"\n",
    "   observations = process.dataset.get_observations()\n",
    "   selected_feature = random.choice(observations) if len(observations) > 0 else []\n",
    "   process.situation = InterpretationSituation( features= [selected_feature], process= process)\n",
    "   \n",
    "def select_single_random_unexplained_feature(process):\n",
    "   \"\"\"randomly selects a single feature to be explained\"\"\"\n",
    "   observations = process.dataset.get_unexplained_observations()\n",
    "   selected_feature = random.choice(observations) if len(observations) > 0 else []\n",
    "   process.situation = InterpretationSituation( features= [selected_feature], process= process)\n",
    "\n",
    "def create_interpretation(process):\n",
    "   pass\n",
    "\n",
    "def select_interpretation_move():\n",
    "   pass\n",
    "\n",
    "class GeologicalInterpretationProcess(object):\n",
    "   \"\"\"GeologicalInterpretationProcess implements the core process of a geological intepretation.\n",
    "    \n",
    "    It connects all the required elements and resulting artefacts relatively to a given interpretation sequence:\n",
    "     - knowledge_framework: a GeologicalKnowledgeFramework\n",
    "     - dataset: a GeologicalDataSet that interfaces all the available data\n",
    "     - representation_space: a `RepresentationSpace` defining the study zone\n",
    "     - strategies: a dict that associates algorithm stages with preferred options\"\"\"\n",
    "   \n",
    "   default_strategies = {\n",
    "      \"SituationSelection\": select_single_random_unexplained_feature,\n",
    "      \"InterpretationMoveSelection\": select_interpretation_move\n",
    "   }\n",
    "     \n",
    "   def __init__(self,\n",
    "                dataset: GeologicalDataset,\n",
    "                representation_space: RepresentationSpace = None,\n",
    "                knowledge_framework= None,\n",
    "                strategies = {}):\n",
    "      \"\"\"Creates a GeologicalInterpretationProcess\n",
    "        \n",
    "      ---------------------------\n",
    "      Parameters:\n",
    "       - dataset (GeologicalDataset): a dataset to be explained by this interpretor\n",
    "       - representation_space (RepresentationSpace): the representation space that must be explainined by the interpretation.\n",
    "       If None (default), the physical space of the dataset is used instead.\n",
    "       - knowledge_framework: a GeologicalKnowledgeFramework that defines the concepts used for this interpretation.\n",
    "         If None is given, the the default knowledge framework is used (`GeologicalKnowledgeManager().get_knowledge_framework()`)\n",
    "       - strategies: a dict that associates algorithm stages with preferred options\n",
    "      \"\"\"\n",
    "      self.dataset = dataset\n",
    "      self._interpreted_objects = set()\n",
    "      self.representation_space = representation_space if representation_space is not None else self.dataset.physical_space\n",
    "      self.knowledge_framework= GeologicalKnowledgeManager().get_knowledge_framework() if knowledge_framework is None else knowledge_framework\n",
    "      \n",
    "      self.strategies = {**GeologicalInterpretationProcess.default_strategies, **strategies}\n",
    "      \n",
    "      self.update_status()\n",
    "         \n",
    "      self.interpretation_moves ={\n",
    "         \"NewInterpretation\": self.apply_new_interpretation,\n",
    "         \"UpdateInterpretation\": self.apply_update_interpretation,\n",
    "         \"RemoveInterpretation\": self.apply_remove_interpretation\n",
    "      }\n",
    "      \n",
    "   def __init_status(self, init):\n",
    "      \"\"\"creates the status on first run or reset\"\"\"\n",
    "      if not hasattr(self, \"status\"):\n",
    "         self.status = {}\n",
    "         init = True\n",
    "      if init:\n",
    "         self.status[\"epoch\"] = None # index of the current iteration\n",
    "      \n",
    "   def update_status(self, init= False):\n",
    "      \"\"\"Performs status update operations\n",
    "      \n",
    "      If the status is not set yet, this method will also create it and initialise it.\n",
    "      \n",
    "      Parameters:\n",
    "      - init (Bool): if True, the status will be reset to initial values. Default is False.\"\"\"\n",
    "      self.__init_status(init)\n",
    "            \n",
    "      self.status[\"coverage\"] = self.evaluate_coverage() # ratio of representation space covered by an explanation\n",
    "      self.status[\"data_explanation_ratio\"] = self.evaluate_data() # ratio of explained observations\n",
    "      self.status[\"anomaly_explanation_ratio\"] =  self.evaluate_anomalies() # ratio of explained observations\n",
    "      self.status[\"termination_criterion\"] = TerminationFlag.DEFAULT # gives explanation about why it terminated\n",
    "      \n",
    "   def register_interpreted_object(self, object, **kargs):\n",
    "      \"\"\"Registers an interpreted object\"\"\"\n",
    "      self.registered_interpreted_objects.add(object)\n",
    "      self.physical_space.attach_interpreted_object(object, **kargs)\n",
    "      \n",
    "   def evaluate_coverage(self):\n",
    "      \"\"\"Evaluates the amount of physical space that is explained\n",
    "      \n",
    "      Returns:\n",
    "      - a float between 0 and 1, representing the ratio of covered space\"\"\"\n",
    "      return 0\n",
    "      \n",
    "   def evaluate_data(self):\n",
    "      \"\"\"Evaluates the amount of data that is explained\n",
    "      \n",
    "      Returns:\n",
    "      - a float between 0 and 1, representing the ratio of explained data\"\"\"\n",
    "      return 0\n",
    "   \n",
    "   def evaluate_anomalies(self):\n",
    "      \"\"\"Evaluates the amount of raised anomalies that are now explained\n",
    "      \n",
    "      Returns:\n",
    "      - a float between 0 and 1, representing the ratio of explained anomalies\"\"\"\n",
    "      return 0\n",
    "   \n",
    "   def __str__(self):\n",
    "      \"\"\"Return a report describing the interpretation process\"\"\"\n",
    "      desc = [\"Geological Interpretation Process:\"]\n",
    "      if self.dataset is None:\n",
    "         desc+= [\"|- Dataset: None\"]\n",
    "      else:\n",
    "         desc+= [\"|- Dataset: \"+\"\\n| |\".join(self.dataset.__str__().split(\"\\n\"))]\n",
    "         \n",
    "      if self.knowledge_framework is None:\n",
    "         desc+= [\"|- Geological Knowledge Framework: None\"]\n",
    "      else:\n",
    "         desc+= [\"|- \"+\"\\n| |\".join(self.knowledge_framework.__str__().split(\"\\n\"))]\n",
    "         \n",
    "      if self.representation_space is None:\n",
    "         desc+= [\"|- No representation space defined, this is an abstract interpretation only.\"]\n",
    "      else:\n",
    "         desc+= [\"|- \"+\"\\n| |\".join(self.representation_space.__str__().split(\"\\n\"))]\n",
    "         \n",
    "      return \"\\n\".join(desc)\n",
    "   \n",
    "   def run(self, max_iter= None):\n",
    "      \"\"\"Runs the iterative interpretation process\n",
    "      \n",
    "      Parameters:\n",
    "      - max_iter (int): if set, it specifies the maximum number of iterations to run before terminating.\n",
    "      Iteration (epoch) are numbered starting at 0, this way the counter also represents the number of passed iterations.\n",
    "      \"\"\"\n",
    "      \n",
    "      # run the iterative process as long as a termination criterion is not reached\n",
    "      \n",
    "      #first update the status to make sure it is up to date, and reset the termination criteria\n",
    "      self.update_status()\n",
    "      \n",
    "      # new epoch increments the iteration count and check termination\n",
    "      try:\n",
    "         while self.new_epoch(max_iter = max_iter):\n",
    "            \n",
    "            # 1. Plan\n",
    "            self.plan()\n",
    "            print(self.situation.features)\n",
    "            \n",
    "            if len(self.situation.features) == 0:\n",
    "               raise MalissiaBaseError(\"Empty feature selection\")\n",
    "            self.situation.features[0].is_Explained_by = [self.situation.features[0]]\n",
    "            print(self.dataset.get_unexplained_observations())\n",
    "            \n",
    "            # 2. Do\n",
    "            # -----------------------------\n",
    "            # 2.1 execute the specified action\n",
    "            \n",
    "            # 3. Check\n",
    "            # -----------------------------\n",
    "            #  3.1 Evaluate internal consistency\n",
    "            #  3.2 Evaluate relational likelihood\n",
    "            #  3.3 Evaluate feature explanation\n",
    "            \n",
    "            # 4. Act\n",
    "            # -----------------------------\n",
    "            # 4.1 Generate anomalies and report\n",
    "            # 4.2 update status\n",
    "            \n",
    "      except KeyboardInterrupt:\n",
    "         self.status[\"termination_criterion\"] = TerminationFlag.USER\n",
    "         \n",
    "      return self.status[\"termination_criterion\"]\n",
    "         \n",
    "   def new_epoch(self, max_iter= None):\n",
    "      \"\"\"Starts a new epoch (iteration) unless termination criteria were reached\n",
    "      \n",
    "      Returns:\n",
    "      - True if the new epoch should start, False if iterations should stop\n",
    "      - max_iter (int): if set, it specifies the maximum number of iterations to run before terminating\"\"\"\n",
    "      keep_going = True\n",
    "      \n",
    "      self.increment_epoch()\n",
    "      if (max_iter is not None) and (self.status[\"epoch\"] >= max_iter):\n",
    "         self.status[\"termination_criterion\"] = TerminationFlag.MAX_ITER\n",
    "         return False\n",
    "      \n",
    "      return keep_going\n",
    "      \n",
    "   def increment_epoch(self):\n",
    "      \"\"\"Initialize or increment the epoch count\"\"\"\n",
    "      if self.status[\"epoch\"] is None:\n",
    "         self.status[\"epoch\"] = 0\n",
    "      else:\n",
    "         self.status[\"epoch\"] += 1\n",
    "      \n",
    "   def select_strategy(self, key):\n",
    "      \"\"\"Select a the implementation to be used based on strategy\"\"\"\n",
    "      if key not in self.strategies:\n",
    "         raise MalissiaBaseError(\"The task has no defined strategy: \" + key)\n",
    "      strat = self.strategies[key]\n",
    "      if isinstance(strat,list):\n",
    "         strat = random.choice(strat)\n",
    "      return strat\n",
    "   \n",
    "   def select_interpretation_move(self, key = None, random = False):\n",
    "      \"\"\"Select the type of interpretation action to be taken.\n",
    "      \n",
    "      Parameters:\n",
    "      - key: the key corresponding to the chosen action, see `interpretation_moves`.\n",
    "      This is to be used only for forcing the choice. Default, None.\n",
    "      - random: if True, will be chosen among the available options (default: False).\n",
    "      \"\"\"\n",
    "      \n",
    "      # random of user choice\n",
    "      if key is None and random:\n",
    "         key = random.choice( self.interpretation_moves.keys() )\n",
    "      if key is not None:\n",
    "         return self.interpretation_moves[key]\n",
    "      \n",
    "      # default move is creating an new interpretation\n",
    "      move = self.interpretation_moves[\"NewInterpretation\"]\n",
    "      # when applicable do others\n",
    "      # if ...:\n",
    "      #   move = self.interpretation_moves[\"UpdateInterpretation\"]\n",
    "      # elif ...:\n",
    "      #   move = self.interpretation_moves[\"RemoveInterpretation\"]\n",
    "      return move\n",
    "      \n",
    "   def apply_new_interpretation(self, random_choice= False, debug= False):\n",
    "      \"\"\"Implements the creation of a new interpretation object\"\"\"\n",
    "      if self.situation is None:\n",
    "         raise MalissiaBaseError(\"Something needs to be selected for interpretation. Here the situtation is None.\")\n",
    "      \n",
    "      # get a list of possibly explaining concepts for the features to be explained\n",
    "      possible_explanations = [concept\n",
    "                             for feature_i in self.situation.features\n",
    "                             for concept in self.knowledge_framework.get_possible_interpretations_of(feature_i)]\n",
    "      \n",
    "      ## sort existing concepts by preference\n",
    "      #possible_explanations = possible_explanations\n",
    "      \n",
    "      # pick an explaining concept\n",
    "      candidate_explanation_class = random.choice(possible_explanations)\n",
    "      \n",
    "      # check for existing instances of this concept in the selected context\n",
    "      existing_instance_mask = [self.knowledge_framework.isinstance(context_object, candidate_explanation_class)\n",
    "                            for context_object in self.situation.context]\n",
    "      # if applicable use it else create a new one\n",
    "      if np.any(existing_instance_mask):\n",
    "         existing_instances = self.situation.context[existing_instance_mask]\n",
    "         self.situation.candidate_explaining_object = random.choice(existing_instances)\n",
    "      else:\n",
    "         constructor = self.knowledge_framework.generate_object_constructor(\n",
    "                              candidate_explanation_class,\n",
    "                              interpretation_situation = self.situation,\n",
    "                              interpretation_status = self.status,\n",
    "                              random_choice = random_choice,\n",
    "                              debug= debug\n",
    "                              )\n",
    "         # Todo: catch the error when missing constructor and generate a report for next iteration\n",
    "         self.situation.candidate_explaining_object = constructor(\n",
    "                                                         interpretation_situation = self.situation,\n",
    "                                                         interpretation_status = self.status\n",
    "                                                         )\n",
    "         self.register_interpreted_object(self.situation.candidate_explaining_object)\n",
    "   \n",
    "   def apply_update_interpretation(self):\n",
    "      \"\"\"Implements the update of an existing interpretation object\"\"\"\n",
    "      pass\n",
    "   \n",
    "   def apply_remove_interpretation(self):\n",
    "      \"\"\"Implements the removal of an existing interpretation object\"\"\"\n",
    "      self.knowledge_framework.remove_all_instances(self.situation.features)\n",
    "   \n",
    "      \n",
    "   def plan(self):\n",
    "      \"\"\"Perform the Plan part of the algorithm\n",
    "      \n",
    "      1. Select a situation to be explained\n",
    "      2. Select a Type of move to be performed in the interpretation process\"\"\"\n",
    "      self.select_strategy(\"SituationSelection\")(self)\n",
    "      self.select_interpretation_move()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation Process Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RandomSituationStrategy(Strategy):\n",
    "    \"\"\"Strategy for randomly selecting a situation\"\"\"\n",
    "    name = \"RandomSituationStrategy\"\n",
    "    short_desc = \"Random Situation Selection\"\n",
    "    full_desc = \"This is implementing random selection of situation to be explained.\"\n",
    "    strategy_type= StrategyType.RANDOM\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        \"\"\"Initialisation of radom situation selection\"\"\"\n",
    "        super().__init__(**kargs)\n",
    "        \n",
    "    \n",
    "    def check_applicability(task):\n",
    "        \"\"\"Checks if applicable\n",
    "        \n",
    "        i.e., for a SelectSituation\n",
    "        \"\"\"\n",
    "        applicable = isinstance(task, SelectSituation)\n",
    "        applicable &= task.context is not None\n",
    "        return applicable\n",
    "    \n",
    "    def execute(self, task):\n",
    "        \"\"\"apply the strategy.\n",
    "        \n",
    "        Parameters:\n",
    "        - task: the task for which this strategy must be applied\"\"\"\n",
    "        \n",
    "        feature_selection_task = task.context.strategies.generate_task(SelectObservation, strategy_type= StrategyType.RANDOM, context= task.context)\n",
    "        selected_feature = feature_selection_task.execute(return_result= True)\n",
    "        \n",
    "        task.result = InterpretationSituation( selected_feature= selected_feature,  process= task.context)\n",
    "  \n",
    "class SelectSituation(ResultTask):\n",
    "    \"\"\"Task for selecting a situation to be explained (feature to be explained + interpretation context)\"\"\"\n",
    "    \n",
    "    name = \"SelectSituation\"\n",
    "    short_desc = \"selects a situation to ne explained\"\n",
    "    full_desc = \"Selects features to be explained and an interpretation context.\"\n",
    "    result_type = InterpretationSituation\n",
    "    available_strategies = set((RandomSituationStrategy,))\n",
    "    \n",
    "    def __init__(self, context:GeologicalInterpretationProcess, feature_choices = None, n= None, **kargs):\n",
    "        \"\"\"Initialisation of a situation selection task\n",
    "        \n",
    "        Parameters:\n",
    "        - feature_choices: a preselected list of features to be explained. If None (default), the dataset in the context will be used \n",
    "        - context: a `GeologicalInterpretationProcess`, as this is an action task, this context will be modified by the task.\n",
    "        It will also be used to define the possible selection unless feature_choices is given.\n",
    "        - n (int): number of features to be selected, if None, this is inferred from the strategies in the context.\"\"\"\n",
    "        self.feature_choices = feature_choices\n",
    "        self.n = n\n",
    "        super().__init__(context = context, result_type= SelectSituation.result_type, **kargs)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of a square surface using Imd method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_orientation(normal_vector):\n",
    "    # Extract components of the normal vector and center point\n",
    "    Nx, Ny, Nz = normal_vector\n",
    "\n",
    "    # Calculate yaw angle (azimuth)\n",
    "    yaw = math.atan2(Ny, Nx)\n",
    "\n",
    "    # Calculate roll angle\n",
    "    roll = math.atan2(math.sqrt(Nx**2 + Ny**2), Nz)\n",
    "\n",
    "    # Calculate pitch angle\n",
    "    pitch = math.atan2(-Nx, Ny)\n",
    "\n",
    "    # Convert angles from radians to degrees if needed\n",
    "    yaw_degrees = math.degrees(yaw)\n",
    "    roll_degrees = math.degrees(roll)\n",
    "    pitch_degrees = math.degrees(pitch)\n",
    "\n",
    "    return yaw_degrees, roll_degrees, pitch_degrees\n",
    "\n",
    "def construct_square_from_sad(normal_vector, center, \n",
    "                              side_length = None, area = None, diagonal=None):\n",
    "    \"\"\"function for constructing a square based on its normal vector, center point and \n",
    "    SAD (either his side length,diagonal or its area) \"\"\"\n",
    "\n",
    "    # Calculate orientation angles\n",
    "    roll_degrees, pitch_degrees, yaw_degrees = calculate_orientation(normal_vector)\n",
    "\n",
    "    # Calculate half-length of the square's side\n",
    "    if side_length == None and area !=None:\n",
    "        side_length = np.sqrt(area)\n",
    "    elif side_length == None and diagonal !=None:\n",
    "        side_length = diagonal/np.sqrt(2)\n",
    "    elif side_length == None and area == None and diagonal == None :\n",
    "        raise ValueError(\"please introduce value of a paramter to calculate geometery of the square\")\n",
    "\n",
    "    half_length = side_length / 2\n",
    "\n",
    "    # Convert orientation angles from degrees to radians\n",
    "    roll_rad = np.radians(roll_degrees)\n",
    "    pitch_rad = np.radians(pitch_degrees)\n",
    "    yaw_rad = np.radians(yaw_degrees)\n",
    "\n",
    "    # Rotation matrix for the specified angles\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(roll_rad) * np.cos(yaw_rad) - np.sin(roll_rad) * np.sin(pitch_rad) * np.sin(yaw_rad), -np.cos(roll_rad) * np.sin(yaw_rad) - np.sin(roll_rad) * np.sin(pitch_rad) * np.cos(yaw_rad), np.cos(pitch_rad) * np.sin(roll_rad)],\n",
    "        [np.cos(pitch_rad) * np.sin(yaw_rad), np.cos(pitch_rad) * np.cos(yaw_rad), -np.sin(pitch_rad)],\n",
    "        [np.sin(roll_rad) * np.cos(yaw_rad) + np.cos(roll_rad) * np.sin(pitch_rad) * np.sin(yaw_rad), np.cos(roll_rad) * np.sin(pitch_rad) * np.cos(yaw_rad) - np.sin(roll_rad) * np.sin(yaw_rad), np.cos(roll_rad) * np.cos(pitch_rad)]\n",
    "    ])\n",
    "\n",
    "    # Calculate the coordinates of the four vertices\n",
    "    vertices = [\n",
    "        center + np.dot(rotation_matrix, np.array([-half_length, -half_length, 0])),\n",
    "        center + np.dot(rotation_matrix, np.array([half_length, -half_length, 0])),\n",
    "        center + np.dot(rotation_matrix, np.array([half_length, half_length, 0])),\n",
    "        center + np.dot(rotation_matrix, np.array([-half_length, half_length, 0]))\n",
    "    ]\n",
    "\n",
    "    # Define the edges (pairs of vertex indices)\n",
    "    edges = [\n",
    "        (0, 1),\n",
    "        (1, 2),\n",
    "        (2, 3),\n",
    "        (3, 0)\n",
    "    ]\n",
    "\n",
    "    # Create a square as a collection of polygons\n",
    "    square = [vertices[i] for i in [0, 1, 2, 3, 0]]\n",
    "\n",
    "    return vertices, edges, square\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing constructors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ontology manipulation\n",
    "\n",
    "The knowledge manipulated in this package is formalised in an ontology,<br>\n",
    "which is store in a *.owl* file.\n",
    "\n",
    "It is named **MOGI** for **M**inimal **O**ntology for **G**eological **I**nterpretation\n",
    "\n",
    "To manipulated this ontology, we use the package **owlready2** available from here: https://owlready2.readthedocs.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ontology provides access to its components, e.g.:\n",
    "* classes\n",
    "* properties\n",
    "* individuals\n",
    "* rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(mogi().classes()))\n",
    "print(list(mogi().properties()))\n",
    "print(list(mogi().individuals()))\n",
    "print(list(mogi().rules()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specific elements can be searched through simple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(iri = \"*Surface*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(type= mogi().PointBased_Observation, qualities= [\"dip\",\"occurrence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(type= mogi().PointBased_Observation, qualities= {\"dip\":45})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(qualities= {\"dip\":45})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(name=\"o?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(name=\"o1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi._ontology_backend.Thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoner\n",
    "\n",
    "Ontologies are even more powerful thansk to their capabilities to use reasoning for infering types, properties, and relationships that were not explicitly stated.\n",
    "This is usefull for obtaining results implied by the already stated information.\n",
    "\n",
    "This is achieved by running a *reasoner* on the ontology as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing GeologicalKnowledgeManager and ontology manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### when the ontoology is attached to the knowledge manager, it is now referred to as mogi() and the roginal backend (owl or others) is now referred to as _ontology_backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "print(GeologicalKnowledgeManager().get_knowledge_framework())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our approach, geological datasets will be progressively interpreted in terms of structural objects,<br>\n",
    "based on a formal definition of concepts own by a **GeologicalKnowledgeManager**.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.sync_reasoner()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().Stratigraphic_Part.is_Possible_Explanation_Of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().Stratigraphic_Part.has_Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.get_objects_potentially_explained_by(mogi().Stratigraphic_Surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().search(type= mogi().PointBased_Observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Representation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(2)\n",
    "\n",
    "space.set_extension_from_data(padding= None)\n",
    "\n",
    "print(space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(3)\n",
    "space.compute_line_attitude_from_two_points([0,0,0],[1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [[0.,0,0],[-1,0,-1],[0,-1,0],[-1,-1,-1.3]]\n",
    "space= PhysicalRepresentationSpace(3)\n",
    "plane = space.compute_attitude_from_points(p)\n",
    "plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dip = 90\n",
    "dip_dir = 270\n",
    "space= PhysicalRepresentationSpace(3)\n",
    "space.compute_normal_from_dip_dir(dip, dip_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(3)\n",
    "space.compute_dip_dir_from_normal([-1,1,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(pv.PolyData(p))\n",
    "plotter.add_mesh(pv.PolyData(plane[\"center\"]), color=\"red\")\n",
    "\n",
    "plotter.add_arrows(plane[\"center\"],plane[\"major_axis\"], color= \"red\")\n",
    "plotter.add_arrows(plane[\"center\"],plane[\"minor_axis\"], color= \"green\")\n",
    "plotter.add_arrows(plane[\"center\"],plane[\"normal\"], color= \"blue\")\n",
    "plotter.show_axes()\n",
    "plotter.show(cpos='xz', window_size = (600,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = pv.Plotter()\n",
    "center = np.array([0,0,0])\n",
    "plotter.add_arrows(np.repeat([center],4,axis=0), np.array(p))\n",
    "plane = space.compute_principal_directions(p)\n",
    "average = space.compute_average_vector(p)\n",
    "plotter.add_arrows(center, average, color=\"Black\")\n",
    "\n",
    "#plotter.add_arrows(plane[\"center\"],plane[\"vectors\"][0], color= \"red\")\n",
    "#plotter.add_arrows(plane[\"center\"],plane[\"vectors\"][1], color= \"green\")\n",
    "#plotter.add_arrows(plane[\"center\"],plane[\"vectors\"][2], color= \"blue\")\n",
    "plotter.show_axes()\n",
    "plotter.show(cpos='xz', window_size = (600,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = PhysicalRepresentationSpace(coordinate_labels= \"depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space.compute_line_attitude_from_two_points([0,0,0],[-10,0,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: PhysicalRepresentationSpace()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(coordinate_labels=[\"X\",\"Y\"])\n",
    "print(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(coordinate_labels=[\"X\",\"Z\"])\n",
    "print(space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing DataSet & Manual entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = mogi().search(type=mogi().PointBased_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(obs) > 0 : \n",
    "    mogi.show_instance_qualities(obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= GeologicalDataset()\n",
    "dataset.remove_all_observations()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_all_instance_qualities(dataset.get_observations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_observation_by_name(\"D8\")\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_all_observations()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "physical_space = PhysicalRepresentationSpace(coordinate_labels= [\"x\",\"y\",\"z\"])\n",
    "dataset.setup_representation_space(physical_space= physical_space)\n",
    "dataset.add_occurrence_observation(name=\"DD\", observed_object= \"Keuper\", x= 1, z= 2, y= 0, occurrence= True)\n",
    "dataset.add_occurrence_observation(name=\"DN\", observed_object= \"Keuper\",  x= 3, z= 1, y= 0)\n",
    "dataset.update_extension()\n",
    "print(dataset)\n",
    "\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_orientation_observation(name=\"DO\", observed_object= \"Trias\", dip= 30, dip_dir= 270, x= 2, z= 2, y= 0)\n",
    "print(dataset)\n",
    "\n",
    "dataset.get_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_from_csv(\"../inputs/data_for_paper.csv\", sep=\";\", index= \"Id\", coordinate_labels=[\"x\",\"y\",\"z\"], labels={\"ID\":\"Id\", \"strike\":\"dip_dir\",\"name\":\"observed_object\"})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrates errors because of wrong separator\n",
    "try:\n",
    "    dataset2= load_dataset_from_csv(\"../inputs/data_for_paper.csv\", labels={\"ID\":\"Id\", \"strike\":\"dip_dir\",\"name\":\"observed_object\"})\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    gkf = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    observations = gkf.search(type=gkf().PointBased_Observation)\n",
    "    gkf.remove_all_instances(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrates errors when leaving wrong labels\n",
    "try:\n",
    "    dataset3 = load_dataset_from_csv(\"../inputs/data_for_paper.csv\", sep=\";\", index= \"ID\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    gkf = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "    observations = gkf.search(type=gkf().PointBased_Observation)\n",
    "    gkf.remove_all_instances(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_head = np.array(['name', 'x', 'y', 'z', 'dip_dir', 'dip', 'observed_object','occurrence'])\n",
    "data_array = np.array([['D1', 15, 20, 35, 270, 45, 'Trias_Base',True],\n",
    "                       ['D2', 30, 25, 50, 270, 45, 'Trias_Base',True],\n",
    "                       ['D3', 60, 30, 40, 90, 45, 'Trias_Base',True],\n",
    "                       ['D4', 75, 15, 25, 90, 45, 'Trias_Base',True],\n",
    "                       ['D5', 110, 20, 40, 270, 63, 'Trias_Base',True],\n",
    "                       ['D6', 120, 20, 60, 270, 64, 'Trias_Base',True],\n",
    "                       ['D7', 155, 20, 60, 89, 39, 'Trias_Base',True],\n",
    "                       ['D8', 190, 20, 30, 91, 40, 'Trias_Base',True],\n",
    "                       ['D11', 25, 22, 45, np.nan, np.nan, None ,True],\n",
    "                       ['D22', 50, 22, 50, np.nan, np.nan, None,True],\n",
    "                       ['D44', 100, 30, 20, np.nan, np.nan, None,True],\n",
    "                       ['D77', 168, 30, 47, np.nan, np.nan, None,True]]\n",
    ")\n",
    "data_test = pd.DataFrame(data = data_array, columns = data_head)\n",
    "data_test = data_test.astype({'name':str, 'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'observed_object':str, 'occurrence':bool})\n",
    "data_test.set_index(\"name\", inplace = True)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_from_dataframe(data_test, coordinate_labels= [\"x\",\"y\",\"z\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "chart = HTML(dataset.info().replace(\"\\n\",\"<br>\"))\n",
    "display(chart)\n",
    "\n",
    "chart = HTML(dataset.to_dataframe().to_html())\n",
    "display(chart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing visualizations & drawings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_line([0,0],30, \"left\")\n",
    "draw_dip_symbol([0,1],60, \"right\", polarity= \"up\", color= \"red\" )\n",
    "plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(coordinate_labels=[\"x\",\"y\",\"z\"])\n",
    "dataset= GeologicalDataset(physical_space=physical_space)\n",
    "print(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = AxisAlignedCrossSection(physical_space)\n",
    "section.draw_dip_symbol([1,30],30, \"right\", polarity= \"up\", length=10 )\n",
    "section.draw_occurrence_symbol([100,40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = AxisAlignedCrossSection(physical_space)\n",
    "section.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing objets constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation\n",
    "dip = 60\n",
    "dip_dir = 45\n",
    "Rotation.from_euler(\"XZY\",[dip,dip_dir,0], degrees= True).as_matrix()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rotation.from_euler(\"XZY\",[[60,0,0],[60,180,0]], degrees= True).mean().as_euler(\"XZY\",degrees= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rotation.from_euler(\"XZY\",[[60,0,0],[60,180,0]], degrees= True).mean().as_matrix()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(ExampleTask, strategy_type= StrategyType.RANDOM)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(ExampleTask)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    strat = Strategy()\n",
    "    strat.check_applicability()\n",
    "except MalissiaBaseError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = RandomExampleTask()\n",
    "if task.check_applicability():\n",
    "    task.execute()\n",
    "task.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ActionTask(None)\n",
    "task.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    task.execute()\n",
    "except MalissiaBaseError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExampleTask.available_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserExampleTask.available_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActionTask.available_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = UserExampleTask()\n",
    "if task.check_applicability():\n",
    "    task.execute()\n",
    "task.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(SelectionTask, choices = [1,2,27,42])\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = dataset.get_observations()\n",
    "task = __default_strategy_factory__.generate_task(SelectionTask, choices = choices, strategy_type= StrategyType.RANDOM)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)\n",
    "print(\"Result type:\",type(task.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(SelectObservation, dataset = dataset)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)\n",
    "print(\"Result type:\",type(task.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = __default_strategy_factory__.generate_task(SelectSituation, context = gip)\n",
    "task.execute()\n",
    "if isinstance(task,ResultTask):\n",
    "    print(task.result)\n",
    "print(\"Result type:\",type(task.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the creation of the different possible instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "# clean all individuals\n",
    "mogi.remove_all_instances()\n",
    "mogi.sync_reasoner()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_all_instance_qualities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = constructor_point(mogi, [\"X\",\"Y\",\"Z\"], [1,2,3], name= \"point_test\")\n",
    "mogi.show_instance_qualities(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.sync_reasoner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = constructor_vector(mogi, [\"X\",\"Y\",\"Z\"], [1,2,3], name= \"vector_test\")\n",
    "mogi.show_instance_qualities(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.sync_reasoner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs_occur = constructor_observation(mogi, name= \"obs_occurrence\", coord_labels= [\"X\",\"Y\",\"Z\"], X= 1, Y= 2, Z= 3, occurrence= True)\n",
    "mogi.show_instance_qualities(test_obs_occur)\n",
    "mogi.show_instance_qualities(test_obs_occur.has_Center[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs_dip = constructor_observation(mogi, name= \"obs_dip\", coord_labels= [\"X\",\"Y\",\"Z\"], X= 1, Y= 2, Z= 3,\n",
    "                                       dip= 30, dip_dir = 270, polarity= True, size= 0.2)\n",
    "mogi.show_instance_qualities(test_obs_dip)\n",
    "mogi.show_instance_qualities(test_obs_dip.has_Center[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.sync_reasoner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planar_Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0,0,0], name=\"N0\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[1,0,1], name=\"N1\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[1,1,1], name=\"N2\"),\n",
    "    constructor_point(mogi, [\"X\",\"Y\",\"Z\"],[0,1,0], name=\"N3\")\n",
    "    ]\n",
    "plan = constructor_planar_surface_from_nodes(mogi,nodes, name=\"plan_from_nodes\")\n",
    "mogi.show_instance_qualities(plan)\n",
    "mogi.show_instance_qualities(plan.has_Center[0])\n",
    "mogi.show_instance_qualities(plan.has_Normal[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_labels = [\"X\",\"Y\",\"Z\"]\n",
    "coords = [\n",
    "    [0,0,0],\n",
    "    [1,0,1],\n",
    "    [1,1,1],\n",
    "    [0,1,0]\n",
    "    ]\n",
    "plan = constructor_planar_surface_from_coords(mogi, coord_labels= coord_labels, coords= coords, name= \"plan_from_coord\")\n",
    "mogi.show_instance_qualities(plan)\n",
    "mogi.show_instance_qualities(plan.has_Center[0])\n",
    "mogi.show_instance_qualities(plan.has_Normal[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_labels = [\"X\",\"Y\",\"Z\"]\n",
    "center = [0.,0.,0.]\n",
    "plan = constructor_planar_surface_from_center_attitude(mogi, coord_labels= coord_labels, center= center,\n",
    "                                                       size= 2, dip= 30, dip_dir= 90, polarity= 1,\n",
    "                                                       name= \"plan_from_attitude\"\n",
    "                                                       )\n",
    "mogi.show_instance_qualities(plan)\n",
    "mogi.show_instance_qualities(plan.has_Center[0])\n",
    "mogi.show_instance_qualities(plan.has_Normal[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.sync_reasoner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratigraphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "points = [mogi().N0, mogi().N1, mogi().N2, mogi().N3]\n",
    "#mogi.show_all_instance_qualities(points)\n",
    "#print(np.array([physical_space.get_object_coordinates(feature_i) for feature_i in points]))\n",
    "location_constructor([mogi().N0, mogi().N1, mogi().N2, mogi().N3], physical_space= physical_space, method= \"random\", return_as_dict= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "points = [mogi().N0, mogi().N1, mogi().N2, mogi().N3]\n",
    "#mogi.show_all_instance_qualities(points)\n",
    "#print(np.array([physical_space.get_object_coordinates(feature_i) for feature_i in points]))\n",
    "attitude_constructor(points, physical_space= physical_space, knowledge_framework= mogi, method= \"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(3)\n",
    "points = [mogi().N0, mogi().N2]\n",
    "#mogi.show_all_instance_qualities(points)\n",
    "#print(np.array([physical_space.get_object_coordinates(feature_i) for feature_i in points]))\n",
    "attitude_constructor(points, physical_space= physical_space, knowledge_framework= mogi, method= \"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(coordinate_labels= [\"X\",\"Y\",\"Z\"])\n",
    "obs = []\n",
    "obs += [constructor_observation(mogi, name= \"O1\", coord_labels= [\"X\",\"Y\",\"Z\"], X= 1, Y= 2, Z= 3,\n",
    "                                       dip= 30, dip_dir = 270, polarity= True, size= 0.2)]\n",
    "#mogi.show_all_instance_qualities(points)\n",
    "#print(np.array([physical_space.get_object_coordinates(feature_i) for feature_i in points]))\n",
    "attitude_constructor(obs, physical_space= physical_space, knowledge_framework= mogi, method= \"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_space = PhysicalRepresentationSpace(coordinate_labels= [\"X\",\"Y\",\"Z\"])\n",
    "obs = []\n",
    "obs += [constructor_observation(mogi, name= \"O1\", coord_labels= [\"X\",\"Y\",\"Z\"], X= 1, Y= 2, Z= 3,\n",
    "                                       dip= 30, dip_dir = 270, polarity= True, size= 0.2)]\n",
    "obs += [constructor_observation(mogi, name= \"O2\", coord_labels= [\"X\",\"Y\",\"Z\"], X= 2, Y= 2, Z= 3,\n",
    "                                       dip= 31, dip_dir = 90, polarity= True, size= 0.2)]\n",
    "#mogi.show_all_instance_qualities(points)\n",
    "#print(np.array([physical_space.get_object_coordinates(feature_i) for feature_i in points]))\n",
    "attitude_constructor(obs, physical_space= physical_space, knowledge_framework= mogi, method= \"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.show_all_instance_qualities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.sync_reasoner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratigraphicpart1 = mogi.Stratigraphic_Part()\n",
    "stratigraphicpart2 = mogi.Stratigraphic_Part()\n",
    "anom3 = discontinuousStratigaphyAnomaly_constructor(knowledge_framework= mogi, stratigraphicpart2 = stratigraphicpart2, stratigraphicpart1 = stratigraphicpart1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom3.is_Related_To"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = mogi().search(type= mogi().PointBased_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(obs) > 0:\n",
    "    interpretations = mogi.get_possible_interpretations_of(obs[0])\n",
    "interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretations[0].has_Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.get_objects_potentially_explained_by(mogi().Stratigraphic_Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(obs) > 1:\n",
    "    interpretations = mogi.get_possible_interpretations_of(obs[1])\n",
    "interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().Stratigraphic_Surface.is_Possible_Explanation_Of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip = GeologicalInterpretationProcess(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.run(max_iter=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip = GeologicalInterpretationProcess(dataset, strategies= {\"SituationSelection\":select_user_unexplained_feature} )\n",
    "gip.run(max_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gip.strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing interpretation from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.remove_all_instances()\n",
    "mogi.sync_reasoner()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_head = np.array(['name', 'x', 'y', 'z', 'dip_dir', 'dip', 'observed_object','occurrence'])\n",
    "data_array = np.array([['D1', 15, 20, 35, 270, 45, 'Trias_Base',True],\n",
    "                       ['D2', 30, 25, 50, 270, 45, 'Trias_Base',True],\n",
    "                       ['D3', 60, 30, 40, 90, 45, 'Trias_Base',True],\n",
    "                       ['D4', 75, 15, 25, 90, 45, 'Trias_Base',True],\n",
    "                       ['D5', 110, 20, 40, 270, 63, 'Trias_Base',True],\n",
    "                       ['D6', 120, 20, 60, 270, 64, 'Trias_Base',True],\n",
    "                       ['D7', 155, 20, 60, 89, 39, 'Trias_Base',True],\n",
    "                       ['D8', 190, 20, 30, 91, 40, 'Trias_Base',True],\n",
    "                       ['D11', 25, 22, 45, np.nan, np.nan, None ,True],\n",
    "                       ['D22', 50, 22, 50, np.nan, np.nan, None,True],\n",
    "                       ['D44', 100, 30, 20, np.nan, np.nan, None,True],\n",
    "                       ['D77', 168, 30, 47, np.nan, np.nan, None,True]]\n",
    ")\n",
    "data_test = pd.DataFrame(data = data_array, columns = data_head)\n",
    "data_test = data_test.astype({'name':str, 'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'observed_object':str, 'occurrence':bool})\n",
    "data_test.set_index(\"name\", inplace = True)\n",
    "dataset = load_dataset_from_dataframe(data_test, coordinate_labels= [\"x\",\"y\",\"z\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = AxisAlignedCrossSection(dataset.physical_space)\n",
    "section.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation = InterpretationSituation(\n",
    "    [mogi().D1, mogi().D2, mogi().D4,mogi().D5, mogi().D3]\n",
    ")\n",
    "print(situation.features)\n",
    "mogi.show_all_instance_qualities(situation.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = constructor_surface_part_from_interpretation(knowledge_framework= mogi, interpretation_situation= situation,\n",
    "                                             physical_space= dataset.physical_space, name= \"interp_surf\")\n",
    "mogi.show_instance_qualities(interp)\n",
    "mogi.show_instance_qualities(interp.has_Representation[0])\n",
    "mogi.show_all_instance_qualities(interp.explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = AxisAlignedCrossSection(dataset.physical_space)\n",
    "section.show()\n",
    "\n",
    "drawing_function = section.get_drawing_method(interp.is_instance_of[0])\n",
    "drawing_function(interp, section, setup_drawing= False)\n",
    "section.get_drawing_method(InterpretationSituation)(situation, section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section.get_drawing_method(InterpretationSituation)(situation, section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
