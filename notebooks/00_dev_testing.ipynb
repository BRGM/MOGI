{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geological Interpretor Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for testing and developping some of the basic code in this package."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ontology manipulation\n",
    "\n",
    "The knowledge manipulated in this package is formalised in an ontology,<br>\n",
    "which is store in a *.owl* file.\n",
    "\n",
    "It is named **MOGI** for **M**inimal **O**ntology for **G**eological **I**nterpretation\n",
    "\n",
    "To manipulated this ontology, we use the package **owlready2** available from here: https://owlready2.readthedocs.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import owlready2 as owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owl.onto_path.append(\"../ontologies/\")\n",
    "mogi = owl.get_ontology(\"mogi.owl\").load()\n",
    "mogi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ontology provides access to its components, e.g.:\n",
    "* classes\n",
    "* properties\n",
    "* individuals\n",
    "* rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(mogi.classes()))\n",
    "print(list(mogi.properties()))\n",
    "print(list(mogi.individuals()))\n",
    "print(list(mogi.rules()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specific elements can be searched through simple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi.search(iri = \"*Surface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mogi.Geologic_Context('Data_properties')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.INDIRECT_get_properties()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoner\n",
    "\n",
    "Ontologies are even more powerful thansk to their capabilities to use reasoning for infering types, properties, and relationships that were not explicitly stated.\n",
    "This is usefull for obtaining results implied by the already stated information.\n",
    "\n",
    "This is achieved by running a *reasoner* on the ontology as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owl.sync_reasoner(infer_property_values=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geological Knowledge Manager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GeologicalKnowledgeManager** may know different instances of **GeologicalKnowledgeFramework**,<br>\n",
    "for example to allow differenciating scenarios or for allowing customisation of knowledge and its formalisation.\n",
    "\n",
    "**GeologicalKnowledgeFramework** provides access to concept definitions for providing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class GeologicalKnowledgeManager(object):\n",
    "    \"\"\"GeologicalKnowledgeManager is managing one or several GeologicalKnowledgeFramework.\n",
    "    \n",
    "    The GeologicalKnowledgeManager is typically a singleton, so there is always one and only one instance of it.\n",
    "    \n",
    "    The GeologicalKnowledgeManager may know different instances of GeologicalKnowledgeFramework,\n",
    "    for example to allow different interpretation scenarios or for allowing user-specific customisation\n",
    "    of knowledge and its formalisation.\n",
    "    \n",
    "    GeologicalKnowledgeFramework are typically ontologies and extensions defined in this package or elsewhere.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __new__(cls):\n",
    "        \"\"\"Method to access (and create if needed) the only allowed instance of this class.\n",
    "        \n",
    "        Returns:\n",
    "        - an instance of GeologicalKnowledgeManager\"\"\"\n",
    "        if not hasattr(cls, 'instance'):\n",
    "            cls.instance = super(GeologicalKnowledgeManager, cls).__new__(cls)\n",
    "            cls.initialised= False\n",
    "            print(\"DEBUG::creates new manager\")\n",
    "        return cls.instance\n",
    "        \n",
    "    def __init__(self, default= \"mogi\", default_source_directory= \"../ontologies/\", default_source_file= \"mogi.owl\", default_ontology_backend= \"owlready2\"):\n",
    "        \"\"\"Initializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        print(\"DEBUG::__init__\")\n",
    "        if not self.initialised:\n",
    "            self._initialise(default= default, default_source_directory= default_source_directory, default_source_file= default_source_file, default_ontology_backend= default_ontology_backend)\n",
    "            \n",
    "    def _initialise(self, default, default_source_directory, default_source_file, default_ontology_backend):\n",
    "        \"\"\"Initializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        print(\"DEBUG::initialize manager\")\n",
    "        self.default= default\n",
    "        self.default_source_directory= default_source_directory\n",
    "        self.default_source_file= default_source_file\n",
    "        self.default_ontology_backend= default_ontology_backend\n",
    "        \n",
    "        self.knowledge_framework_dict = {}\n",
    "        \n",
    "        self.initialised= True\n",
    "        \n",
    "    def reset(self, default= \"mogi\", default_source_directory= \"../ontologies/\", default_source_file= \"mogi.owl\", default_ontology_backend= \"owlready2\"):\n",
    "        \"\"\"Reinitializes the GeologicalKnowledgeManager with some default values from configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - default: specifies the name of the default knowledge framework\n",
    "        - default_source_directory: specifies the default folder containing of the knowledge framework definitions\n",
    "        - default_source_file: file contained in the source_directory defining the knowledge framework (e.g., .owl file)\n",
    "        - default_ontology_backend: specifies the default ontology backend to be used\n",
    "        \"\"\"\n",
    "        print(\"DEBUG::reset manager\")\n",
    "        self._initialise(default= default, default_source_directory= default_source_directory, default_source_file= default_source_file, default_ontology_backend= default_ontology_backend)\n",
    "             \n",
    "    def load_knowledge_framework(self, name=None, source= None, source_directory= None, backend= None):\n",
    "        \"\"\"Gets and initilises the ontology from the specified source.\n",
    "        \n",
    "        Parameters:\n",
    "        - name: the name to be given to the knowledge framework. If None (default) the file name will be used.\n",
    "        - source: filename to the ontology source. If None(default) the default ontology is used.\n",
    "        - source_directory: where the system should look for ontology definition files. If None, the `GeologicalKnowledgeFramework` will decide.\n",
    "        - backend: the ontology backend to be used. If None, the `GeologicalKnowledgeFramework` will decide.\"\"\"\n",
    "        source = source if source is not None else self.default_source_file\n",
    "        name = name if name is not None else os.path.basename(source).split(os.path.extsep)[0]\n",
    "        self.knowledge_framework_dict[name] = GeologicalKnowledgeFramework(name= name, source= source, source_directory= source_directory, backend= backend)\n",
    "    \n",
    "    def get_knowledge_framework(self,name= \"default\"):\n",
    "        \"\"\"Accessor to knowledge frameworks.\"\"\"\n",
    "        name = self.default if name == \"default\" else name\n",
    "        assert len(self.knowledge_framework_dict) > 0, \"No ontology has been loaded yet. Please use GeologicalKnowledgeManager().load_knowledge_framework() first\"\n",
    "        assert name in self.knowledge_framework_dict.keys(), \"The specified ontology hasn't been loaded: \"+name+\\\n",
    "            \"\\navailable ontology names are: \"+\"\\n\".join(self.knowledge_framework_dict.keys())\n",
    "        return self.knowledge_framework_dict[name]\n",
    "    \n",
    "class GeologicalKnowledgeFramework(object):\n",
    "    \"\"\"A GeologicalKnowledgeFramework holds the definition of concepts and relationships describing knowledge.\n",
    "    \n",
    "    This is typically an overlay around a formal ontology definition, which also brings additional capabilities,\n",
    "    such as algorithms and factories to achieve specific tasks and create objects.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, source, source_directory= None, backend= None):\n",
    "        \"\"\"Initialise a KnowledgeFramework form a given ontology file (source).\n",
    "        \n",
    "        Parameters:\n",
    "        - name: should be the name under which this KnowledgeFramework is known in the manager\n",
    "        - source: the source file for the ontology definition\n",
    "        - source_directory: the directory where the source files for the ontology definition are looked for.\n",
    "        If None (default) the default path provided by the `KnowledgeManager` is used.\n",
    "        - backend: the ontology backend to be used for this knwoledge framework.\n",
    "        If None (default) the default ontology backend provided by the `KnowledgeManager` is used.\"\"\"\n",
    "        self.name= name\n",
    "        print(source)\n",
    "        self.__source_directory= None\n",
    "        self.init_source_directory(source_directory)\n",
    "        self.initialise_ontology_backend(backend)\n",
    "        print(source)\n",
    "        self.load_ontology(source)\n",
    "    \n",
    "    def init_source_directory(self, source_directory):\n",
    "        \"\"\"Initialises the folder where source files are searched.\n",
    "        \n",
    "        Parameters:\n",
    "        - source_directory: if None, the previous value is used if it wasn't None, else the `GeologicalKnowledgeManager`default is used.\"\"\"\n",
    "        if source_directory is not None:\n",
    "            self.__source_directory= source_directory\n",
    "        elif self.__source_directory is None:\n",
    "            self.__source_directory= GeologicalKnowledgeManager().default_source_directory\n",
    "    \n",
    "    def initialise_ontology_backend(self, backend_name:str= None):\n",
    "        \"\"\"Initializes the ontology package used as a backend to access ontologies.\n",
    "        \n",
    "        This will:\n",
    "        - try to import the backend as onto\n",
    "        - set the default path for ontologies\"\"\"\n",
    "                \n",
    "        self.__ontology_backend = None\n",
    "        backend_name= GeologicalKnowledgeManager().default_ontology_backend if backend_name is None else backend_name\n",
    "        if backend_name == \"owlready2\":\n",
    "            try:\n",
    "                import owlready2 as owl2 \n",
    "                self.__ontology_backend = owl2\n",
    "                if self.__source_directory not in self.__ontology_backend.onto_path:\n",
    "                    self.__ontology_backend.onto_path.append(self.__source_directory)\n",
    "            except ImportError:\n",
    "                raise ImportError(\"Your are trying to use Owlready2 as a backend for ontology management, but it doesn't appear to be installed.\"\\\n",
    "                \"This is either because OwlReady2 is given as default option or because you asked for it.\"\\\n",
    "                \"Please install the OwlReady2 package from https://owlready2.readthedocs.io\"\\\n",
    "                \"or give another backend through GeologicalKnowledgeManager().initialise_ontology_backend()\")\n",
    "                \n",
    "            # also test if java is correctly installed & accessible, as it is used by owlready2 for reasoning\n",
    "            try:\n",
    "                os.system(\"java -version\")\n",
    "            except:\n",
    "                raise ImportError(\"Java doesn't appear to be installed properly as the command `java -version` returned an error.\"\\\n",
    "                    \"This error occured while loading owlready2 package as an ontology backend, because java is used for the reasoning engine.\")\n",
    "        else:\n",
    "            raise Exception(\"The specified backed for ontology is not supported: \"+backend_name)\n",
    "          \n",
    "        \n",
    "    def load_ontology(self, source):\n",
    "        \"\"\"Loads the ontology specified by source.\n",
    "        \n",
    "        Parameters:\n",
    "        - source: the source file for the ontology definition\n",
    "        - source_directory: the directory where the source files for the ontology definition are looked for.\n",
    "        If None (default) the default path provided by the `KnowledgeManager` is used.\"\"\"\n",
    "        self.__source= source\n",
    "        print(source)\n",
    "        try:\n",
    "            self.__onto = self.__ontology_backend.get_ontology(self.__source).load()\n",
    "        except Exception as err:\n",
    "            raise Exception(\"Unexpected exception received while loading ontology:\\n - source: {}\\n - onto_path: {}\".format(self.__source, self.__ontology_backend.onto_path))\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.__onto\n",
    "        \n",
    "    def get_ontology_backend(self):\n",
    "        \"\"\"Gets the ontology backend\"\"\"\n",
    "        assert self.__ontology_backend is not None, \"Trying to access the ontology backend without initialising it.\"\n",
    "        return self.__ontology_backend\n",
    "    \n",
    "    def sync_reasoner(self, **kargs):\n",
    "        \"\"\"Synchronise the reasoner.\n",
    "        \n",
    "        Parameters:\n",
    "        - **kargs:\n",
    "        |-infer_property_values\"\"\"\n",
    "        self.__ontology_backend.sync_reasoner(**kargs)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our approach, geological datasets will be progressively interpreted in terms of structural objects,<br>\n",
    "based on a formal definition of concepts own by a **GeologicalKnowledgeManager**.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().load_knowledge_framework()\n",
    "GeologicalKnowledgeManager().get_knowledge_framework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeFramework(\"mogi\",\"mogi.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeologicalKnowledgeManager().knowledge_framework_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi = GeologicalKnowledgeManager().get_knowledge_framework()\n",
    "mogi.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mogi().classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset\n",
    "\n",
    "Data are actually described within the ontology, here thanks to the *Data* class.<br>\n",
    "Adding new data points calls for creating new *Data* individuals (i.e., instances in the ontology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_head = np.array(['name', 'x', 'y', 'z', 'dip_dir', 'dip', 'geology'])\n",
    "data_array = np.array([['D1', 15, 20, 35, 270, 45, 'Trias_Base'],\n",
    "                       ['D2', 30, 25, 50, 270, 45, 'Trias_Base'],\n",
    "                       ['D3', 60, 30, 40, 90, 45, 'Trias_Base'],\n",
    "                       ['D4', 75, 15, 25, 90, 45, 'Trias_Base'],\n",
    "                       ['D5', 110, 20, 40, 270, 63, 'Trias_Base'],\n",
    "                       ['D6', 120, 20, 60, 270, 64, 'Trias_Base'],\n",
    "                       ['D7', 155, 20, 60, 89, 39, 'Trias_Base'],\n",
    "                       ['D8', 190, 20, 30, 91, 40, 'Trias_Base'],\n",
    "                       ['D11', 25, 22, 45, np.nan, np.nan, np.nan],\n",
    "                       ['D22', 50, 22, 50, np.nan, np.nan, np.nan],\n",
    "                       ['D44', 100, 30, 20, np.nan, np.nan, np.nan],\n",
    "                       ['D77', 168, 30, 47, np.nan, np.nan, np.nan]]\n",
    ")\n",
    "dataset = pd.DataFrame(data = data_array, columns = data_head)\n",
    "dataset = dataset.astype({'name':str, 'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'geology':str})\n",
    "dataset.set_index(\"name\", inplace = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing any data already stored in the ontology\n",
    "for data_i in mogi.search(type = mogi.Ponctual_Observation):\n",
    "    owl.destroy_entity(data_i)\n",
    "mogi.search(type = mogi.Ponctual_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the dataset in the ontology by creating individuals\n",
    "for name_i, values_i in dataset.iterrows():\n",
    "    mogi.Ponctual_Observation(name_i, **{key:[val] for key, val in values_i.items()})\n",
    "mogi.search(type = mogi.Ponctual_Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading dataset from the ontology\n",
    "dataset = pd.DataFrame(columns=[\"name\",\"x\",\"y\",\"z\",\"dip_dir\",\"dip\",'geology'])\n",
    "dataset.set_index(\"name\",inplace=True)\n",
    "for di in mogi.search(type = mogi.Ponctual_Observation):\n",
    "    for prop in di.get_properties():\n",
    "        for value in prop[di]:\n",
    "            dataset.loc[di.name,prop.name] = value\n",
    "dataset = dataset.astype({'x':float, 'y':float, 'z':float, 'dip_dir':float, 'dip':float, 'geology':str})\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Space(object):\n",
    "    \"\"\"A `Space` represents an abstract place where things exist and can be observed or rendered.\n",
    "    \n",
    "    It is typically dereived into:\n",
    "    - `PhysicalSpace` for spaces with physical coordinates (typically X, Y, Z)\n",
    "    - `TemporalSpace` for spaces with a time coordinate\"\"\"\n",
    "    \n",
    "class PhysicalSpace(Space):\n",
    "    \"\"\"A `PhysicalSpace` represents a physical place where things exist and can be observed or rendered.\"\"\"\n",
    "    \n",
    "class TemporalSpace(Space):\n",
    "    \"\"\"A `TemporalSpace` represents a time span where things exist and can be observed or rendered.\"\"\"\n",
    "\n",
    "class DataSet(object):\n",
    "    \"\"\"A `DataSet` gathers several kinds of data / observations / informations\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(center, dip, dir, length= 1, ax= None, color = \"black\", **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "\n",
    "    center = np.array(center)\n",
    "    dip_rad = np.deg2rad(dip)\n",
    "    vec_x =  np.cos(dip_rad)\n",
    "    if dir == \"left\": vec_x *= -1\n",
    "    vec_z = -np.sin(dip_rad)\n",
    "    vect = 0.5 * length * np.array([vec_x,vec_z])\n",
    "    start = center - vect\n",
    "    end = center + vect\n",
    "    ax_plt.plot([start[0],end[0]],[start[1],end[1]], color = color, **kargs)\n",
    "    \n",
    "    return vect\n",
    "    \n",
    "def draw_dip_symbol(center, dip, dir, length= 1, polarity= None, ax= None, color = \"black\", polarity_ratio= 0.4, **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "    \n",
    "    vect = draw_line(center= center, dip= dip, dir= dir, length= length, ax= ax_plt, color = color, **kargs)\n",
    "    \n",
    "    if polarity is not None:\n",
    "        vect_pol = polarity_ratio * np.array([-vect[1],vect[0]])\n",
    "        if (dir == \"left\" and polarity == \"up\") or (dir == \"right\" and polarity == \"down\") : vect_pol *= -1\n",
    "        ax_plt.arrow(*center,*vect_pol, width=length/100, color = color, **kargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_line([0,0],30, \"left\")\n",
    "draw_dip_symbol([0,1],60, \"right\", polarity= \"up\", color= \"red\" )\n",
    "plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dataset( dataset, ax= None, **kargs):\n",
    "    ax_plt = plt if ax is None else ax\n",
    "    \n",
    "    for data_i in dataset.itertuples():\n",
    "        if (data_i.dip != np.nan) and (data_i.dip_dir != np.nan):\n",
    "            dir = \"right\" if data_i.dip_dir < 180 else \"left\"\n",
    "            draw_dip_symbol( center= [data_i.x,data_i.z], dip= data_i.dip, dir= dir, **kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dataset(dataset, length=10, polarity=\"up\")\n",
    "plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(dataset.itertuples()).dip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distinguish two kind of operations here:\n",
    "* representation\n",
    "* visualisation\n",
    "\n",
    "A representation is a formal description of how something appears in a given representation space, but it doesn't have to be visualised.<br>\n",
    "A visualisation takes care of the rendering of a representation with a given support (image, screen)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation should also be made a bit more abstract.<br>\n",
    "1. There is a variety of object that can be rendered in a representation space (typically, different kinds of a dataset components)\n",
    "2. Several kinds of representation spaces could be envisionned (e.g., spatial 1D,2D,3D, or temporal, or just an abstract text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RepresentationSpace(object):\n",
    "    \"\"\"A general framework for Representating geological objects\"\"\"\n",
    "    \n",
    "class TemporalRepresentationSpace(RepresentationSpace):\n",
    "    \"\"\"A `RepresentationSpace` representing temporal apsects of represented objects.\"\"\"\n",
    "    \n",
    "class PhysicalRepresentationSpace(RepresentationSpace):\n",
    "    \"\"\"A type of `RepresentationSpace` representing physical aspects of the represented objects.\"\"\"\n",
    "    \n",
    "    __default_coordinate_labels = [\"X\",\"Y\",\"Z\"]\n",
    "    \n",
    "    def __init__(self, dimension: int=None, coordinate_label: str|list= None, dataset= None, **kargs):\n",
    "        \"\"\"Initialisation of the representation space.\n",
    "        \n",
    "        Parameters:\n",
    "        - dimension (int): specify the number of dimensions of the representation space, typically 1D, 2D, or 3D (i.e., 1, 2, or 3),\n",
    "        NB: larger dimension spaces are not supported. At least either the `dimension` parameter or `coordinate_label` parameter should be given.\n",
    "        - coordinate_label(str|list(str)): gives the label(s) of the coordinates. If given, the number of dimensions is deduced from the size of the list\n",
    "        and `dimensions`is ignored, otherwise, the labels are taken from the `__default_coordinate_labels` based on the number of `dimension`s. \n",
    "        At least either the `dimension` parameter or `coordinate_label` parameter should be given.\n",
    "        - dataset: a Dataset object containing the data to be attached to this representation space.\n",
    "        Note that the RepresentationSpace can be created first and then updated automatically when creating the dataset attached to this space.\n",
    "        - **kargs:\n",
    "            - use_limits: if True, uses the limits of the dataset, else keeps the current ones\n",
    "            - padding: if use_limits is True, the given paddign will be used to keep a space around the dataset\n",
    "        \"\"\"\n",
    "        assert not (coordinate_label is None and dimension is None), \"At least one of the parameters should be specified\"\n",
    "        if coordinate_label is None:\n",
    "            assert isinstance(dimension, int),\"dimension parameter must be an integer\"\n",
    "            assert dimension in [1,2,3], \"The specified number of dimensions ({:d}) is not supported, should be 1, 2 or 3.\".format(dimension)\n",
    "            self.dimension= dimension\n",
    "            self.coordinate_labels= PhysicalRepresentationSpace.__default_coordinate_labels[:self.dimension]\n",
    "        elif isinstance(coordinate_label,str):\n",
    "            self.dimension= 1\n",
    "            self.coordinate_labels=  [coordinate_label]\n",
    "        elif isinstance(coordinate_label, list):\n",
    "            self.dimension= len(coordinate_label)\n",
    "            self.coordinate_labels= coordinate_label\n",
    "        else:\n",
    "            raise(\"Unsupported initialisation of representation space: dimension({}) and coordinate_label ({}).\\n At least one of the parameters shoudl be specified.\".format(dimension, coordinate_label))\n",
    "    \n",
    "        self.__default_padding= 0.05\n",
    "        self.attach_data(dataset,**kargs)\n",
    "        \n",
    "    def attach_data(self, dataset= None, use_limits= True, padding= None, **kargs):\n",
    "        \"\"\"Attach a dataset to the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - dataset: a Dataset object to be attached\n",
    "        - use_limits: if True, uses the limits of the dataset, else keeps the current ones\n",
    "        - padding: if use_limits is True, the given paddign will be used to keep a space around the dataset\"\"\"\n",
    "        self.data= dataset\n",
    "        self.set_limits_from_data(padding= padding)\n",
    "        \n",
    "    def set_limits(self, limits:list):\n",
    "        \"\"\"Setter for the limits (min,max) of the representation space\n",
    "        \n",
    "        Parameters:\n",
    "        - limits: a list containing a pair of min and max value for each dimension of the space\"\"\"\n",
    "        limits= np.array(limits)\n",
    "        assert limits.shape[0] == self.dimension, \"The specified limits ({}) do not match the space dimensions ({})\".format(limits, self.dimension)\n",
    "        assert limits.shape[1] == 2, \"The specified limits should provide both lower andupper bounds for each dimension, given: {}\".format(limits)\n",
    "        self.limits= limits\n",
    "        \n",
    "    def set_limits_from_data(self, padding= None):\n",
    "        \"\"\"Sets the limits of the space from the attached dataset\n",
    "        \n",
    "        If no dataset is attached yet, then default limits are used instead (min:0,max:1).\n",
    "        \n",
    "        Parameters:\n",
    "        - padding: a space that is left around the dataset, either a value compatible with the coordinates, or a list of values of same dimensions.\n",
    "        If None, by default the padding is 5% of the dataset range.\n",
    "        \"\"\"\n",
    "        if padding is None:\n",
    "            if self.data is None:\n",
    "                self.set_limits([[0,1],[0,1]])\n",
    "                return\n",
    "            else:\n",
    "                padding= self.__default_padding\n",
    "        else:\n",
    "            try: # check if padding as a dimension\n",
    "                len(padding)\n",
    "            # if not, then use it a a scaling \n",
    "            except TypeError: #just checking it is a number\n",
    "                assert type(padding) == int or type(padding) == float, \"padding should be given as a number (int or float), here: \"+type(padding)\n",
    "                # keep the padding as is in this case\n",
    "            else:# else check its dimensions are ok\n",
    "                assert len(padding) == self.dimension, \"the dimensions of the specified padding (len({})->) should match the space dimension ({})\".format(padding, len(padding), self.dimension)\n",
    "                padding= np.array(padding)\n",
    "        # in any cases, except when padding and data is None\n",
    "        center= np.mean(self.data.limits, axis= 1)\n",
    "        diff= self.data.limits.T - center\n",
    "        limits= center + (1+2*padding)*diff\n",
    "        self.set_limits(limits.T)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Description of the physical space parameters and data\n",
    "        \"\"\"\n",
    "        desc= [\"Representation space of type: {:s}\".format(type(self).__name__)]\n",
    "        desc+= [\"- Number of dimension(s): {}\".format(self.dimension)]\n",
    "        desc+= [\"- Coordinate label(s): {}\".format(self.coordinate_labels)]\n",
    "        desc+= [\"- Space limits:\"]\n",
    "        for dim_i, lim_i in zip(self.coordinate_labels,self.limits):\n",
    "            desc+= [\" - Coord {}: {}\".format(dim_i, lim_i)]  \n",
    "        return \"\\n\".join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(2)\n",
    "\n",
    "space.set_limits_from_data(padding= None)\n",
    "\n",
    "print(space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhysicalRepresentationSpace(\"depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhysicalRepresentationSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= PhysicalRepresentationSpace(coordinate_label=[\"X\",\"Y\"])\n",
    "print(space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeologicalDataset(object):\n",
    "    \"\"\"A GeologicalDataset gathers information about geological data to be interpreted\"\"\"\n",
    "    \n",
    "    existing_data_types= [\"unit_observation\",\"dip_measurement\"]\n",
    "    \"\"\"List of data/observations types that are supported in the code.\"\"\"\n",
    "    \n",
    "    def __init__(self, representation_spaces= None, default_representation_space= None):\n",
    "        \"\"\"Initialises a `GeologicalDataset`\n",
    "        \n",
    "        Parameters:\n",
    "        - representation_spaces: list of `Representationspace`s to which the dataset must be attached\n",
    "        Note: datasets can be created without representation space and attached later on by using `RepresentationSpace.attach_dataset`.\n",
    "        Alternativelly, a single `Representationspace` can be given here and default_representation_space is not used.\n",
    "        - default_representation_space: the main RepresentationSpace to which this dataset is attached.\n",
    "        If None and representation_spaces are provided, then the first one will be taken.\n",
    "        If the default one is not initially in the full list, then it is added to it.\n",
    "        \n",
    "        Internals: this method initialises several internal attributes:\n",
    "        - data_map: is a map that links a `str` data type (c.f. `existing_data_types`) to data information\n",
    "        - limits: represents the extension of the dataset in the attached representation space\n",
    "        (i.e., the default on if this dataset is represented in several representation spaces\n",
    "        - representation_spaces: the dataset can be attached to and represented into several representation spaces,\n",
    "        the `default_representation_space` olds a pointer towards the primarily used space if more than one are defined \n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_map= {}\n",
    "        \"\"\"This map links available type of information and the stored data.\"\"\"\n",
    "        \n",
    "        self.limits= None\n",
    "        \"\"\"This attribute stores extension of the dataset in its default representation space\"\"\"\n",
    "        \n",
    "        # setup representation spaces\n",
    "        if (representation_spaces is None):\n",
    "            if default_representation_space is None:\n",
    "                self.default_representation_space= None\n",
    "                self.representation_spaces= list()\n",
    "            else:\n",
    "                self.default_representation_space= default_representation_space\n",
    "                self.representation_spaces= list(default_representation_space)\n",
    "        elif isinstance(representation_spaces, RepresentationSpace):\n",
    "            if (default_representation_space is None) or (representation_spaces is default_representation_space):\n",
    "                self.default_representation_space= representation_spaces\n",
    "                self.representation_spaces= list(representation_spaces)\n",
    "            else:\n",
    "                self.default_representation_space= default_representation_space\n",
    "                self.representation_spaces= [default_representation_space,representation_spaces]\n",
    "        else:\n",
    "            if (default_representation_space is None):\n",
    "                self.representation_spaces= representation_spaces\n",
    "                self.default_representation_space= representation_spaces[0]\n",
    "            elif default_representation_space not in representation_spaces:\n",
    "                self.default_representation_space= default_representation_space\n",
    "                self.representation_spaces= [default_representation_space]+[representation_spaces]\n",
    "            else:\n",
    "                self.default_representation_space= default_representation_space\n",
    "                self.representation_spaces= representation_spaces\n",
    "        \n",
    "        # register the datast in the listed representation spaces\n",
    "        for space_i in self.representation_spaces:\n",
    "            space_i.att\n",
    "                \n",
    "    def __str__(self):\n",
    "        \"\"\"Description of the dataset\n",
    "        \"\"\"\n",
    "        desc= [\"A dataset of type: {:s}\".format(type(self).__name__)]\n",
    "        desc+= [\"- types of data:\"]\n",
    "        for type_i, data_i in self.data_map.items():\n",
    "            desc+= [\" - {}: nb entries: {}\".format(type_i, len(data_i.index))]  \n",
    "        return \"\\n\".join(desc) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation process in itself is run in a **GeologicalInterpretationProcess** and follow a very simple and generic algorithm.<br>\n",
    "This algorithm implements a Deming wheel process of continual improvement:\n",
    "1. Plan:\n",
    "    1. Select a situation\n",
    "    2. Select an action\n",
    "2. Do: Implement the action (e.g., CreateInterpretationElement)\n",
    "    1. List features\n",
    "    2. Identify possible explanations\n",
    "    3. Rank/chose explanations\n",
    "    4. Instanciate individuals\n",
    "    5. Infer and set parameters\n",
    "3. Check: Evaluate consistency\n",
    "    1. Evaluate internal consistency\n",
    "    2. Evaluate relational likelihood\n",
    "    3. Evaluate feature explanation\n",
    "4. Act: Generate anomalies and report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeologicalInterpretationProcess(object):\n",
    "    \"\"\"GeologicalInterpretationProcess implements the core process of a geological intepretation.\n",
    "    \n",
    "    It connects all the required elements and resulting artefacts relatively to a given interpretation sequence:\n",
    "     - a GeologicalKnowledgeFramework\"\"\"\n",
    "     \n",
    "    def __init__(self, dataset: GeologicalDataset, knowledge_framework= None):\n",
    "         \"\"\"Creates a GeologicalInterpretationProcess\n",
    "         \n",
    "         ---------------------------\n",
    "         Parameters:\n",
    "         - dataset (GeologicalDataset): a dataset to be explained by this interpretor\n",
    "         - knowledge_framework: a GeologicalKnowledgeFramework that defines the concepts used for this interpretation.\n",
    "            If None is given, the the default knowledge framework is used (`GeologicalKnowledgeManager().get_knowledge_framework()`)\n",
    "         \"\"\"\n",
    "         self.knowledge_framework= GeologicalKnowledgeManager().get_knowledge_framework() if knowledge_framework is None else knowledge_framework\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
